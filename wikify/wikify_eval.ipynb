{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DatasetsÂ¶\n",
    "Local and Global Algorithms ...\n",
    "\n",
    "AQUAINT: Milne\n",
    "\n",
    "MSNBC dataset, taken from (Cucerzan, 2007),\n",
    "\n",
    "ACE: Mechanical Turkn\n",
    "\n",
    "Wiki: choose those paragraphts that p(t|m) makes atleast 10% error\n",
    "\n",
    "For evaluation, check BOT evaluation, mentioned in Milne\n",
    "\n",
    "Downloadable from : http://cogcomp.cs.illinois.edu/page/resource_view/4\n",
    "\n",
    "Spotlight two datasets, a wiki selection 35 paragraphs from New York times There is a website, but couldn't find it\n",
    "\n",
    "http://oldwiki.dbpedia.org/Datasets/NLP\n",
    "\n",
    "Tag me: Wiki and tweet, available, but looks old! http://acube.di.unipi.it/tagme-dataset/\n",
    "\n",
    "AIDA\n",
    "\n",
    ": https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/aida/downloads/** AIDA CoNLL-YAGO Dataset: Hnad create from Conll AIDA-EE Dataset: Again hand done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove after transferring to wikification module\n",
    "    if  op_method == 'word2vec_word_context'  :\n",
    "        return word_context_disambiguate(S, M, C, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload\n",
    "\n",
    "# %aimport wsd\n",
    "# import sys\n",
    "from wikify import *\n",
    "import time\n",
    "ws=5\n",
    "S=[\"Carlos\", \"met\", \"David\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "M=[[0, \"Roberto_Carlos\"], [2, \"David_Beckham\"], [4, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "\n",
    "S=[\"David\", \"met\", \"Victoria\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "M=[[0, \"David_Beckham\"], [2, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "S=[\"Phoenix, Arizona\"] \n",
    "M=[[0, \"Phoenix,_Arizona\"]]\n",
    "\n",
    "start = time.time()\n",
    "C = generate_candidates(S, M, max_t=5, enforce=False)\n",
    "print \"Candidates: \", C, \"\\n\"\n",
    "\n",
    "ids, titles = wikify(S,M,C, ws, method='context2context')\n",
    "\n",
    "\n",
    "#print \"Key Scores_method_1: \", candslist_scores, \"\\n\"\n",
    "print \"Best IDS\", ids, \"\\n\"\n",
    "print \"Best titles\", titles, \"\\n\"\n",
    "#print \"get_tp\",get_tp(M, ids) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSD Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wsd_eval.py \n",
    "import sys\n",
    "from optparse import OptionParser\n",
    "\n",
    "#sys.path.insert(0,'..')\n",
    "#from wikisim.calcsim import *\n",
    "from wsd import *\n",
    "import time\n",
    "from random import shuffle\n",
    "np.seterr(all='raise')\n",
    "\n",
    "# parser = OptionParser()\n",
    "# parser.add_option(\"-t\", \"--max_t\", action=\"store\", type=\"int\", dest=\"max_t\", default=5)\n",
    "# parser.add_option(\"-c\", \"--max_count\", action=\"store\", type=\"int\", dest=\"max_count\", default=-1)\n",
    "# parser.add_option(\"-w\", \"--win_size\", action=\"store\", type=\"int\", dest=\"win_size\", default=5)\n",
    "# parser.add_option(\"-v\", action=\"store_true\", dest=\"verbose\", default=False)\n",
    "\n",
    "#(options, args) = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "# max_t = options.max_t\n",
    "# max_count = options.max_count\n",
    "# verbose = options.verbose\n",
    "# ws = options.win_size\n",
    "\n",
    "\n",
    "max_t = 20\n",
    "max_count = -1\n",
    "verbose = True\n",
    "\n",
    "fresh_restart=True\n",
    "\n",
    "\n",
    "\n",
    "dsnames = [os.path.join(home,'backup/datasets/ner/kore.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/wiki-mentions.5000.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/msnbc.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/aquaint.json') \n",
    "          ]\n",
    "\n",
    "methods = ['popularity','keydisamb','entitycontext','mention2entity','context2context','context2profile', 'learned']\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wikify')\n",
    "# if not os.path.exists(outdir): #Causes synchronization problem\n",
    "#     os.makedirs(outdir)\n",
    "\n",
    "tmpdir = os.path.join(outdir, 'tmp')\n",
    "# if not os.path.exists(tmpdir): #Causes synchronization problem\n",
    "#     os.makedirs(tmpdir)\n",
    "    \n",
    "resname =  os.path.join(outdir, 'reslog.csv')\n",
    "#clearlog(resname)\n",
    "\n",
    "detailedresname=  os.path.join(outdir, 'detailedreslog.txt')\n",
    "#clearlog(detailedresname)\n",
    "\n",
    "\n",
    "\n",
    "for method in methods:\n",
    "    if 'word2vec' in method:\n",
    "        gensim_loadmodel(word2vec_path)\n",
    "        print \"loaded\"\n",
    "        sys.stdout.flush()\n",
    "    for dsname in dsnames:\n",
    "        start = time.time()\n",
    "        \n",
    "        print \"dsname: %s, method: %s, max_t: %s ...\"  % (dsname,\n",
    "                method, max_t)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        tmpfilename = os.path.join(tmpdir, \n",
    "                                   '-'.join([method, str(max_t), os.path.basename(dsname)]))\n",
    "        overall=[]\n",
    "        start_count=-1\n",
    "        if os.path.isfile(tmpfilename) and not fresh_restart:\n",
    "            with open(tmpfilename,'r') as tmpf:\n",
    "                for line in tmpf:\n",
    "                    js = json.loads(line.strip())\n",
    "                    start_count = js['no']\n",
    "                    if js['tp'] is not None:\n",
    "                        overall.append(js['tp'])\n",
    "        \n",
    "        if start_count !=-1:\n",
    "            print \"Continuing from\\t\", start_count\n",
    "            \n",
    "        count=0\n",
    "        with open(dsname,'r') as ds, open(tmpfilename,'a') as tmpf:\n",
    "            for line in ds:\n",
    "                js = json.loads(line.decode('utf-8').strip());\n",
    "                S = js[\"text\"]\n",
    "                M = js[\"mentions\"]\n",
    "                count +=1\n",
    "                if count <= start_count:\n",
    "                    continue\n",
    "                if verbose:\n",
    "                    print \"%s:\\tS=%s\\n\\tM=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                C = generate_candidates(S, M, max_t=max_t, enforce=False)\n",
    "                \n",
    "                try:\n",
    "                    #ids, titles = disambiguate_driver(S,M, C, ws=0, method=method, direction=direction, op_method=op_method)\n",
    "                    ids, titles = wsd(S,M,C, method=method)\n",
    "                    tp = get_tp(ids, M) \n",
    "                except Exception as ex:\n",
    "                    tp = (None, None)\n",
    "                    print \"[Error]:\\t\", type(ex), ex\n",
    "                    raise\n",
    "                    continue\n",
    "                \n",
    "                overall.append(tp)\n",
    "                tmpf.write(json.dumps({\"no\":count, \"tp\":tp})+\"\\n\")\n",
    "                if (max_count !=-1) and (count >= max_count):\n",
    "                    break\n",
    "                    \n",
    "\n",
    "        elapsed = str(timeformat(int(time.time()-start)));\n",
    "        print \"done\"\n",
    "        detailedres ={\"dsname\":dsname, \"method\": method, \n",
    "                      \"max_t\": max_t, \"tp\":overall, \"elapsed\": elapsed, \"problem\": \"wsd\"}\n",
    "        \n",
    "        \n",
    "        logres(detailedresname, '%s',  json.dumps(detailedres))\n",
    "        \n",
    "        micro_prec, macro_prec = get_prec(overall)        \n",
    "        logres(resname, 'wsd\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s', method, max_t, \n",
    "               dsname, micro_prec, macro_prec, elapsed)\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikification Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wsd_eval.py\n",
    "\"\"\"Evaluating the wsd module. It assumes the sentences are already segmented\n",
    "\"\"\"\n",
    "import sys\n",
    "from optparse import OptionParser\n",
    "\n",
    "#sys.path.insert(0,'..')\n",
    "#from wikisim.calcsim import *\n",
    "from wsd import *\n",
    "import time\n",
    "from random import shuffle\n",
    "np.seterr(all='raise')\n",
    "\n",
    "# parser = OptionParser()\n",
    "# parser.add_option(\"-t\", \"--max_t\", action=\"store\", type=\"int\", dest=\"max_t\", default=5)\n",
    "# parser.add_option(\"-c\", \"--max_count\", action=\"store\", type=\"int\", dest=\"max_count\", default=-1)\n",
    "# parser.add_option(\"-w\", \"--win_size\", action=\"store\", type=\"int\", dest=\"win_size\", default=5)\n",
    "# parser.add_option(\"-v\", action=\"store_true\", dest=\"verbose\", default=False)\n",
    "\n",
    "#(options, args) = parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "# max_t = options.max_t\n",
    "# max_count = options.max_count\n",
    "# verbose = options.verbose\n",
    "# ws = options.win_size\n",
    "\n",
    "\n",
    "max_t = 20\n",
    "max_count = -1\n",
    "verbose = True\n",
    "\n",
    "fresh_restart=True\n",
    "\n",
    "\n",
    "dsnames = [os.path.join(home,'backup/datasets/ner/kore.json'),\n",
    "           os.path.join(home,'backup/datasets/ner/wiki-mentions.5000.json'),\n",
    "          ]\n",
    "\n",
    "mentionmethods = (CORE_NLP, LEARNED_MENTION)\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wikify')\n",
    "# if not os.path.exists(outdir): #Causes synchronization problem\n",
    "#     os.makedirs(outdir)\n",
    "\n",
    "tmpdir = os.path.join(outdir, 'tmp')\n",
    "# if not os.path.exists(tmpdir): #Causes synchronization problem\n",
    "#     os.makedirs(tmpdir)\n",
    "    \n",
    "resname =  os.path.join(outdir, 'reslog.csv')\n",
    "#clearlog(resname)\n",
    "\n",
    "detailedresname=  os.path.join(outdir, 'detailedreslog.txt')\n",
    "#clearlog(detailedresname)\n",
    "\n",
    "\n",
    "\n",
    "for mentionmethod in mentionmethods:\n",
    "    for dsname in dsnames:\n",
    "        start = time.time()\n",
    "        \n",
    "        print \"dsname: %s, method: %s, max_t: %s, ws: %s ...\"  % (dsname,\n",
    "                method, max_t, ws)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        tmpfilename = os.path.join(tmpdir, \n",
    "                                   '-'.join([method, str(max_t), str(ws), os.path.basename(dsname)]))\n",
    "        overall_tp=[]\n",
    "        overall_fp=[]\n",
    "        overall_tn=[]\n",
    "        start_count=-1\n",
    "        if os.path.isfile(tmpfilename) and not fresh_restart:\n",
    "            with open(tmpfilename,'r') as tmpf:\n",
    "                for line in tmpf:\n",
    "                    js = json.loads(line.strip())\n",
    "                    start_count = js['no']\n",
    "                    if js['tp'] is not None:\n",
    "                        overall_tp.append(js['tp'])\n",
    "                        overall_fp.append(js['fp'])\n",
    "                        overall_tn.append(js['tn'])\n",
    "        \n",
    "        if start_count !=-1:\n",
    "            print \"Continuing from\\t\", start_count\n",
    "            \n",
    "        count=0\n",
    "        with open(dsname,'r') as ds, open(tmpfilename,'a') as tmpf:\n",
    "            for line in ds:\n",
    "                js = json.loads(line.decode('utf-8').strip());\n",
    "                S = js[\"text\"]\n",
    "                M = js[\"mentions\"]\n",
    "                count +=1\n",
    "                if count <= start_count:\n",
    "                    continue\n",
    "                if verbose:\n",
    "                    print \"%s:\\tS=%s\\n\\tM=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                S2,M2 = wikify_a_line(line, mentionmethod=mentionmethod)\n",
    "                mention_measures = get_sentence_measures(S2, M2, S, M, wsd_measure=False)\n",
    "                mention_overall.append(mention_measures)\n",
    "                \n",
    "                wikify_measures = get_sentence_measures(S2, M2, S, M, wsd_measure=True)\n",
    "                wikify_overall.append(wikify_measures)\n",
    "                \n",
    "                if (max_count !=-1) and (count >= max_count):\n",
    "                    break\n",
    "                    \n",
    "\n",
    "        elapsed = str(timeformat(int(time.time()-start)));\n",
    "        print \"done\"\n",
    "        detailedres ={\"dsname\":dsname, \"method\": method, \n",
    "                      \"max_t\": max_t, \"tp\":overall, \"elapsed\": elapsed, \"ws\": ws}\n",
    "        \n",
    "        \n",
    "        logres(detailedresname, '%s',  json.dumps(detailedres))\n",
    "        \n",
    "        mention_overall_measures = get_overall_measures(mention_overall)    \n",
    "        output = ('mention_evaluation',method, max_t , dsname) + mention_overall_measures + (elapsed,)\n",
    "        \n",
    "        logres(resname, '%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n', output)\n",
    "        \n",
    "        wikify_overall_measures = get_overall_measures(wikify_overall)  \n",
    "        output = ('wikify_evaluation',method, max_t , dsname) + wikify_overall_measures + (elapsed,)\n",
    "            \n",
    "\n",
    "print \"done\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
