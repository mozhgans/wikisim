{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikisim Wikify: Linking Text to Wikipedia\n",
    "\n",
    "* **Armin Sajadi** (sajadi@cs.dal.ca)\n",
    "* **Ryan Amaral**  (amaral@cs.dal.ca)\n",
    "\n",
    "This is a simple and step by step explanation of calculating semantic relatedness using Wikipedia. We start by preprocessing and building the api, that is explained in the following papers papers:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Here First\n",
    "\n",
    "** Make sure you have followed the [setup process](../README.md#Hosting-Wikisim) and have all the requirements before trying to run these scripts **\n",
    "\n",
    "\n",
    "\n",
    "# Table of Context\n",
    "\n",
    "** [WSD Utils](#WSD-Utils) **\n",
    "\n",
    "** [Coherence Module](#Coherence-Module) **\n",
    "\n",
    "** [Testing Coherence](#Testing-Coherence)**\n",
    "\n",
    "** [WSD Module](#WSD-Module) **\n",
    "\n",
    "** [Genetrate Train Data Repository for WSD](#Genetrate-Train-Data-Repository-for-WSD) **\n",
    "\n",
    "** [Train the LTR Model](#Train-the-LTR-Model) **\n",
    "\n",
    "** [Mention Detection](#Mention-Detection) **\n",
    "\n",
    "** [Generate Train Data Repository For Mention Detection](#Generate-Train-Data-Repository-For-Mention-Detection) **\n",
    "\n",
    "** [Train SVC Model for Mention Detection](#Train-SVC-Model-for-Mention-Detection) **\n",
    "\n",
    "** [Wikification API](#Wikification-API) **\n",
    "\n",
    "** [Testing Wikification](#Testing-Wikification) **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Frequent used Datastructures and Terminology\n",
    "**Mention Detection**: Finding strings that can potential refere to a concept in Wikipedia\n",
    "\n",
    "**WSD** : *Having the mentions*, finding the correct concept. In WSD we a assume the mentions are given\n",
    "\n",
    "**Wikification**: Mention Detection + WSD\n",
    "\n",
    "**S**: segmented (tokenized) sentence [w1, ..., wn]\n",
    "\n",
    "**M**: mensions [(m1, e1), ... , (mj,ej)] where mi is an index of S (S[mi] is the mention) and ej is the entity it referes to\n",
    "\n",
    "**C**: Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "             where cij is the jth candidate for ith mention and pij is the relative frequency of cij\n",
    "             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSD Utils\n",
    "Generating candidates, calculating measures, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wsd_util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wsd_util.py \n",
    "\"\"\"A few general modules for disambiguation\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "\n",
    "import sys, os\n",
    "from itertools import chain\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "import unicodedata\n",
    "\n",
    "dirname = os.path.dirname(__file__)\n",
    "sys.path.insert(0,os.path.join(dirname, '..'))\n",
    "from wikisim.config import *\n",
    "from wikisim.calcsim import *\n",
    "from requests.packages.urllib3 import Retry\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "dirname = os.path.dirname(__file__)\n",
    "MODELDIR = os.path.join(dirname, \"../models\")\n",
    "\n",
    "session = requests.Session()\n",
    "http_retries = Retry(total=20,\n",
    "                backoff_factor=.1)\n",
    "http = requests.adapters.HTTPAdapter(max_retries=http_retries)\n",
    "session.mount('http://', http)\n",
    "\n",
    "\n",
    "def generate_candidates(S, M, max_t=20, enforce=False):\n",
    "    \"\"\" Given a sentence list (S) and  a mentions list (M), returns a list of candiates\n",
    "        Inputs:\n",
    "            S: segmented sentence [w1, ..., wn]\n",
    "            M: mensions [m1, ... , mj]\n",
    "            max_t: maximum candiate per mention\n",
    "            enforce: Makes sure the \"correct\" entity is among the candidates\n",
    "        Outputs:\n",
    "         Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "             where cij is the jth candidate for ith mention and pij is the relative frequency of cij\n",
    "    \n",
    "    \"\"\"\n",
    "    candslist=[]\n",
    "    for m in M:\n",
    "        \n",
    "        clist = anchor2concept(S[m[0]])\n",
    "        if not clist:\n",
    "            clist=((0L,1L),)\n",
    "        \n",
    "        clist = sorted(clist, key=lambda x: -x[1])\n",
    "        clist = clist[:max_t]\n",
    "        \n",
    "        smooth=0    \n",
    "        if enforce:          \n",
    "            wid = title2id(m[1])            \n",
    "    #         if wid is None:\n",
    "    #             raise Exception(m[1].encode('utf-8') + ' not found')\n",
    "            \n",
    "                        \n",
    "            trg = [(i,(c,f)) for i,(c,f) in enumerate(clist) if c==wid]\n",
    "            if not trg:\n",
    "                trg=[(len(clist), (wid,0))]\n",
    "                smooth=1\n",
    "\n",
    "                \n",
    "            if smooth==1 or trg[0][0]>=max_t: \n",
    "                if clist:\n",
    "                    clist.pop()\n",
    "                clist.append(trg[0][1])\n",
    "            \n",
    "        s = sum(c[1]+smooth for c in clist )        \n",
    "        clist = [(c,float(f+smooth)/s) for c,f in clist ]\n",
    "            \n",
    "        candslist.append(clist)\n",
    "    return  candslist \n",
    "\n",
    "\n",
    "def solr_escape(s):\n",
    "    \"\"\"\n",
    "        Escape a string for solr\n",
    "    \"\"\"\n",
    "    #ToDo: probably && and || nead to be escaped as a whole, and also AND, OR, NOT are not included\n",
    "    to_sub=re.escape(r'+-&&||!(){}[]^\"~*?:\\/')\n",
    "    return re.sub('[%s]'%(to_sub,), r'\\\\\\g<0>', s)\n",
    "\n",
    "def solr_unescape(s):\n",
    "    \"\"\"\n",
    "        Escape a string for solr\n",
    "    \"\"\"\n",
    "    #ToDo: probably && and || nead to be escaped as a whole, and also AND, OR, NOT are not included\n",
    "    to_sub=re.escape(r'+-&&||!(){}[]^\"~*?:\\/')\n",
    "    return re.sub('\\\\\\([%s])'%(to_sub,), r'\\g<1>', s)\n",
    "\n",
    "def solr_encode(inputstr):\n",
    "    '''This function \"ideally\" should prepare the text in the correct encoding\n",
    "        which is utf-16, but I couldn't (cf. my encoding notes)\n",
    "        so for know, just make everything ascii!\n",
    "        Input: \n",
    "            A unicode string with any encoding\n",
    "        Output: \n",
    "            Ascii encoded string\n",
    "    '''\n",
    "    if type(inputstr) is str:\n",
    "        return inputstr\n",
    "    log('[solr_encode]\\t Encoded to ascii')\n",
    "    return unicodedata.normalize('NFKD', inputstr).encode('ascii', 'ignore')\n",
    "\n",
    "\n",
    "#Evaluation Methods\n",
    "def get_tp(ids, gold_titles):\n",
    "    \"\"\"Returns true positive number in id, compared to gold_titles \n",
    "        this function is used to evaluate WSD\n",
    "       Inputs: goled_titles: The correct titles\n",
    "               ids: The given ids\n",
    "       Outputs: returns a tuple of (true_positives, total_number_of_ids)\n",
    "    \n",
    "    \"\"\"\n",
    "    tp=0\n",
    "    for m,id2 in zip(gold_titles, ids):\n",
    "        if title2id(m[1]) == id2:\n",
    "            tp += 1\n",
    "    return [tp, len(ids)]\n",
    "\n",
    "def get_prec(tp_list):\n",
    "    \"\"\"Returns precision\n",
    "       Inputs: a list of (true_positive and total number) lists\n",
    "       Output: Precision\n",
    "    \"\"\"\n",
    "    overall_tp = 0\n",
    "    simple_count=0\n",
    "    overall_count=0\n",
    "    macro_prec = 0;\n",
    "    for tp, count in tp_list:\n",
    "        if tp is None:\n",
    "            continue\n",
    "        simple_count +=1    \n",
    "        overall_tp += tp\n",
    "        overall_count += count\n",
    "        macro_prec += float(tp)/count\n",
    "        \n",
    "    macro_prec = macro_prec/simple_count\n",
    "    micro_prec = float(overall_tp)/overall_count\n",
    "    \n",
    "    return micro_prec, macro_prec\n",
    "\n",
    "    \n",
    "def get_sentence_measures(S, M, S_gold, M_gold, wsd_measure=False):\n",
    "    ''' Calcuates precision/recall/F1 for mention detection/wsd for a given sentence\n",
    "        Input:\n",
    "            S_gold: The correct tokenized sentence \n",
    "            M_gold: The correct mention\n",
    "            S: The given sentence to evaluate\n",
    "            M: The given mentions to evaluate\n",
    "            wsd_measure: if True, it returns the wikifying measures,\n",
    "                        if false, returns the measures of the mention dection  prcess\n",
    "        Output:\n",
    "            precision, recall, f-measure\n",
    "            \n",
    "    '''\n",
    "                    \n",
    "    Sgi=[]\n",
    "    Mgi=[]\n",
    "    last_index=0\n",
    "    for s in S_gold:\n",
    "        Sgi.append ([last_index, last_index+len(s)])\n",
    "        last_index += len(s)\n",
    "    Mgi = [Sgi[m[0]] for m in M_gold]    \n",
    "                \n",
    "    Sj=[]\n",
    "    last_index=0\n",
    "    for s in S:\n",
    "        Sj.append ([last_index, last_index+len(s)])\n",
    "        last_index += len(s)\n",
    "    Mj = [Sj[m[0]] for m in M]    \n",
    "    \n",
    "    \n",
    "    i=0\n",
    "    j=0\n",
    "    tp=fp=fn=0\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        if i >= len(Mgi):\n",
    "            fp += (len(Mj)-j)\n",
    "            break\n",
    "            \n",
    "        if j >= len(Mj):\n",
    "            fn += (len(Mgi)-i)\n",
    "            break\n",
    "            \n",
    "        if Mgi[i][1] <= Mj[j][0]:\n",
    "            fn += 1\n",
    "            i += 1\n",
    "            continue\n",
    "            \n",
    "        if  Mgi[i][0] >= Mj[j][1]:\n",
    "            fp += 1\n",
    "            j += 1\n",
    "            continue\n",
    "\n",
    "        if wsd_measure:\n",
    "            if title2id(M_gold[i][1]) != title2id(M[j][1]):\n",
    "                fp += 1\n",
    "                i += 1\n",
    "                j += 1\n",
    "                continue\n",
    "            \n",
    "        tp +=1\n",
    "        i += 1\n",
    "        j += 1\n",
    "        \n",
    "    return tp, fp, fn\n",
    "            \n",
    "def get_overall_measures(tp_list):\n",
    "    \"\"\"Returns micro/macro measures, given a list of (tp, fp, fn)\n",
    "       Inputs: a list of (tp, fp, fn) tuples\n",
    "       Output: macro_prec, macro_rec, macro_f1, micro_prec, micro_rec, micro_f1\n",
    "    \"\"\"\n",
    "    overall_tp = overall_fp = overall_fn = 0\n",
    "    macro_prec = macro_rec = macro_f1 = 0;\n",
    "    for tp, fp, fn in tp_list:\n",
    "\n",
    "        overall_tp += tp\n",
    "        overall_fp += fp\n",
    "        overall_fn += fn\n",
    "\n",
    "        prec = float(tp)/(tp+fp) if tp+fp > 0 else 0\n",
    "        rec  = float(tp)/(tp+fn) if tp+fn > 0 else 0\n",
    "        macro_prec += prec\n",
    "        macro_rec  += rec\n",
    "        macro_f1   += 2*(prec * rec)/(prec + rec) if (prec + rec)>0 else 0\n",
    "        \n",
    "    macro_prec = macro_prec/len(tp_list)\n",
    "    macro_rec = macro_rec/len(tp_list)\n",
    "    macro_f1 = macro_f1/len(tp_list)\n",
    "\n",
    "    micro_prec =  float(overall_tp) / (overall_tp + overall_fp)\n",
    "    micro_rec  =  float(overall_tp) / (overall_tp + overall_fn)\n",
    "    micro_f1   = 2*(micro_prec * micro_rec)/(micro_prec + micro_rec)\n",
    "    \n",
    "    return macro_prec, macro_rec, macro_f1, micro_prec, micro_rec, micro_f1\n",
    "     \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Module\n",
    "Calculates two types of coherence given a sentence\n",
    "* Context Coherence : It is based on the similarity of an entity to the other entities in its context\n",
    "* Key Entity Coherence: It is based on the similarity of a an entity to the key-entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile coherence.py \n",
    "\"\"\"Diiferent coherence (context, key-entity) calculation, and \n",
    "    disambiguation.\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "\n",
    "from wsd_util import *\n",
    "import numpy as np\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "\n",
    "def get_candidate_representations(candslist, direction, method):\n",
    "    '''returns an array of vector representations. \n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "      Outputs\n",
    "           cvec_arr: Candidate embeddings, a two dimensional array, each column \n",
    "                   is the representation of a candidate\n",
    "           cveclist_bdrs: a list of pairs (beginning, end), to indicate where \n",
    "                   the embeddings for a concepts indicates start and end. In other words\n",
    "                   The embedding of candidates [ci1...cik] in candslist is\n",
    "                   cvec_arr[cveclist_bdrs[i][0]:cveclist_bdrs[i][1]] \n",
    "    '''\n",
    "    \n",
    "    cframelist=[]\n",
    "    cveclist_bdrs = []\n",
    "    ambig_count=0\n",
    "    for cands in candslist:\n",
    "        if len(candslist)>1:\n",
    "            ambig_count += 1\n",
    "        cands_rep = [conceptrep(encode_entity(c[0], method, get_id=False), method=method, direction=direction, get_titles=False) for c in cands]\n",
    "        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n",
    "        cframelist += cands_rep\n",
    "        \n",
    "    cvec_fr = pd.concat(cframelist, join='outer', axis=1)\n",
    "    cvec_fr.fillna(0, inplace=True)\n",
    "    cvec_arr = cvec_fr.as_matrix().T\n",
    "    return cvec_arr, cveclist_bdrs\n",
    "\n",
    "def entity_to_context_scores(candslist, direction, method):\n",
    "    ''' finds the similarity between each entity and its context representation\n",
    "        Inputs:\n",
    "            candslist: the list of candidates [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            direction: embedding direction\n",
    "            method: similarity method\n",
    "        Returns:\n",
    "           cvec_arr: Candidate embeddings, a two dimensional array, each column \n",
    "           cveclist_bdrs: a list of pairs (beginning, end), to indicate where the \n",
    "                   reperesentation of the candidates for cij reside        \n",
    "           cands_score_list: scroes in the form of [[s11,...s1k],...[sn1,...s1m]]\n",
    "                    where sij  is the similarity of c[i,j] to to ci-th context\n",
    "                    \n",
    "            '''\n",
    "    cvec_arr, cveclist_bdrs =  get_candidate_representations(candslist, direction, method)    \n",
    "    \n",
    "    aggr_cveclist = np.zeros(shape=(len(candslist),cvec_arr.shape[1]))\n",
    "    for i in range(len(cveclist_bdrs)):\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        aggr_cveclist[i]=cvec_arr[b:e].sum(axis=0)\n",
    "    \n",
    "    from itertools import izip\n",
    "    resolved = 0\n",
    "    cands_score_list=[]        \n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        convec=aggr_cveclist[:i].sum(axis=0) + aggr_cveclist[i+1:].sum(axis=0)\n",
    "        S=[]    \n",
    "        for v in cvec:\n",
    "            try:\n",
    "                # We have zero vectors, so this can rais an exception\n",
    "                # or return none                \n",
    "                s = 1-sp.spatial.distance.cosine(convec, v);\n",
    "            except:\n",
    "                s=0                \n",
    "            if np.isnan(s):\n",
    "                s=0\n",
    "            S.append(s)\n",
    "        cands_score_list.append(S)\n",
    "\n",
    "    return cvec_arr, cveclist_bdrs, cands_score_list\n",
    "\n",
    "def key_criteria(cands_score):\n",
    "    ''' helper function for find_key_concept: returns a score indicating how good a key is x\n",
    "        Input:\n",
    "            scroes for candidates [ci1, ..., cik] in the form of (i, [(ri1, si1), ..., (rik, sik)] ) \n",
    "            where (rij,sij) indicates that sij is the similarity of c[i][rij] to to cith context\n",
    "            \n",
    "    '''\n",
    "    if len(cands_score[1])==0:\n",
    "        return -float(\"inf\")    \n",
    "    if len(cands_score[1])==1 or cands_score[1][1][1]==0:\n",
    "        return float(\"inf\")\n",
    "    \n",
    "    return (cands_score[1][0][1]-cands_score[1][1][1]) / cands_score[1][1][1]\n",
    "\n",
    "def find_key_concept(candslist, direction, method):\n",
    "    ''' finds the key entity in the candidate list\n",
    "        Inputs:\n",
    "            candslist: the list of candidates [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            cvec_arr: the array of all embeddings for the candidates\n",
    "            cveclist_bdrs: The embedding vector for each candidate: [[c11,...c1k],...[cn1,...c1m]]\n",
    "        Returns:\n",
    "            cvec_arr: Candidate embeddings, a two dimensional array, each column \n",
    "            cveclist_bdrs: a list of pairs (beginning, end), to indicate where the \n",
    "            key_concept: the concept forwhich one of the candidates is the key entity\n",
    "            key_entity: candidate index for key_cancept that is detected to be key_entity\n",
    "            key_entity_vector: The embedding of key entity\n",
    "            '''\n",
    "    cvec_arr, cveclist_bdrs, cands_score_list = entity_to_context_scores(candslist, direction, method);\n",
    "    S=[sorted(enumerate(S), key=lambda x: -x[1]) for S in cands_score_list]\n",
    "        \n",
    "    key_concept, _ = max(enumerate(S), key=key_criteria)\n",
    "    key_entity = S[key_concept][0][0]\n",
    "    \n",
    "    b,e = cveclist_bdrs[key_concept]\n",
    "    \n",
    "    key_entity_vector =  cvec_arr[b:e][key_entity]    \n",
    "    return cvec_arr, cveclist_bdrs, key_concept, key_entity, key_entity_vector\n",
    "\n",
    "def keyentity_candidate_scores(candslist, direction, method):\n",
    "    '''returns entity scores using key-entity scoring \n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "           \n",
    "       Returns:\n",
    "           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n",
    "    '''\n",
    "    \n",
    "    cvec_arr, cveclist_bdrs, key_concept, key_entity, key_entity_vector = find_key_concept(candslist, direction, method)\n",
    "    \n",
    "    # Iterate \n",
    "    candslist_scores=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        cand_scores=[]\n",
    "\n",
    "        for v in cvec:\n",
    "            try:\n",
    "                # We have zero vectors, so this can rais an exception\n",
    "                # or return none                \n",
    "                d = 1-sp.spatial.distance.cosine(key_entity_vector, v);\n",
    "            except:\n",
    "                d=0                \n",
    "            if np.isnan(d):\n",
    "                d=0\n",
    "            \n",
    "            cand_scores.append(d)    \n",
    "        candslist_scores.append(cand_scores) \n",
    "    return candslist_scores\n",
    "\n",
    "\n",
    "\n",
    "def coherence_scores_driver(C, ws=5, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\"):\n",
    "    \"\"\" Assigns a score to every candidate \n",
    "        Inputs:\n",
    "            C: Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            ws: Windows size for chunking\n",
    "            method: similarity method\n",
    "            direction: embedding type\n",
    "            op_method: disambiguation method, either keyentity or entitycontext\n",
    "        Output:\n",
    "            Candidate Scores\n",
    "        \n",
    "    \"\"\"\n",
    "    windows = [[start, min(start+ws, len(C))] for start in range(0,len(C),ws) ]\n",
    "    last = len(windows)\n",
    "    if last > 1 and windows[last-1][1]-windows[last-1][0]<2:\n",
    "        windows[last-2][1] = len(C)\n",
    "        windows.pop()\n",
    "    scores=[]    \n",
    "    for w in windows:\n",
    "        chunk_c = C[w[0]:w[1]]\n",
    "        if op_method == 'keydisamb':\n",
    "            scores += keyentity_candidate_scores(chunk_c, direction, method)\n",
    "            \n",
    "        if op_method == 'entitycontext':\n",
    "            _, _, candslist_scores = entity_to_context_scores(chunk_c, direction, method);\n",
    "            scores += candslist_scores\n",
    "            \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing Coherence'\n",
    "\"\"\"\n",
    "from coherence import *\n",
    "# S=[\"Carlos\", \"met\", \"David\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "# M=[[0, \"Roberto_Carlos\"], [2, \"David_Beckham\"], [4, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "\n",
    "\n",
    "S=[\"Three\", \"of\", \"the\", \"greatest\", \"guitarists\", \"started\", \"their\", \"career\", \"in\", \"a\", \"single\", \"band\", \":\", \"Clapton\", \",\", \"Beck\", \",\", \"and\", \"Page\", \".\"]\n",
    "M=[[13, \"Eric_Clapton\"], [15, \"Jeff_Beck\"], [18, \"Jimmy_Page\"]]\n",
    "\n",
    "# S=[\"Phoenix, Arizona\"] \n",
    "# M=[[0, \"Phoenix,_Arizona\"]]\n",
    "\n",
    "C = generate_candidates(S, M, max_t=5, enforce=False)\n",
    "print \"Candidates: \", C, \"\\n\"\n",
    "\n",
    "\n",
    "coh_scores = coherence_scores_driver(C, ws=5, method='rvspagerank', direction=DIR_BOTH, op_method=\"entitycontext\")\n",
    "print coh_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSD Module\n",
    "This modules assumes that the mentions are already detected. Given a tokenized sentence with mention markers, it \n",
    "tries to find the target entity in Wikipedia, using several measures:\n",
    "\n",
    "* Popularity\n",
    "* Context similarity coherence\n",
    "* Key entity coherence\n",
    "* String similarity between the mention and the candidate\n",
    "* Textual context similarity\n",
    "* Machine Learned Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wsd.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wsd.py \n",
    "\"\"\"Context-based disambiguation and also Learning-To-Rank combination\n",
    "    of several features.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "from collections import Counter\n",
    "import sys\n",
    "from coherence import *\n",
    "from sklearn.externals import joblib\n",
    "#sys.path.insert(0,'..')\n",
    "\n",
    "#from wikisim.calcsim import *\n",
    "#from wsd.wsd import *\n",
    "# My methods\n",
    "#from senseembed_train_test.ipynb\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "\n",
    "LTR_NROWS_S = 10000\n",
    "LTR_NROWS_L = 50000\n",
    "wsd_model_preprocessor_ = None\n",
    "wsd_model_=None\n",
    "def load_wsd_model(nrows):\n",
    "    global wsd_model_preprocessor_, wsd_model_\n",
    "    \n",
    "    wsd_model_preprocessor_fn = os.path.join(MODELDIR, 'ltr_preprocessor.%s.pkl' %(nrows, ))\n",
    "    if os.path.isfile(wsd_model_preprocessor_fn): \n",
    "        wsd_model_preprocessor_ = joblib.load(open(wsd_model_preprocessor_fn, 'rb'))    \n",
    "        log(\"[load_wsd_model]\\twsd_model_preprocessor file (%s) loaded\" % (wsd_model_preprocessor_fn,))\n",
    "    else:\n",
    "        log(\"[load_wsd_model]\\twsd_model_preprocessor file (%s) not found\" % (wsd_model_preprocessor_fn,))\n",
    "\n",
    "\n",
    "    wsd_model_fn_ = os.path.join(MODELDIR, 'ltr.%s.pkl'%(nrows,))\n",
    "    if os.path.isfile(wsd_model_fn_): \n",
    "        wsd_model_ = joblib.load(open(wsd_model_fn_, 'rb'))    \n",
    "        log(\"[load_wsd_model]\\twsd_model file (%s) loaded\" % (wsd_model_fn_,))\n",
    "    else:\n",
    "        log(\"[load_wsd_model]\\twsd_model file (%s) not found\" % (wsd_model_fn_,))\n",
    "\n",
    "\n",
    "def get_context(anchor, eid, rows=50000):\n",
    "    \"\"\"Returns the context\n",
    "       Inputs: \n",
    "           anchor: the anchor text\n",
    "           eid: The id of the entity this anchor points to\n",
    "       Output:\n",
    "           The context (windows size is, I guess, 20)       \n",
    "    \"\"\"\n",
    "    params={'wt':'json', 'rows':rows}\n",
    "    anchor = solr_escape(anchor)\n",
    "    \n",
    "    q='anchor:\"%s\" AND entityid:%s' % (anchor, eid)\n",
    "    params['q']=q\n",
    "    \n",
    "#     session = session.Session()\n",
    "#     http_retries = Retry(total=20,\n",
    "#                     backoff_factor=.1)\n",
    "#     http = session.adapters.HTTPAdapter(max_retries=http_retries)\n",
    "#     session.mount('http://localhost:8983/solr', http)\n",
    "    \n",
    "    r = session.get(qstr, params=params).json()\n",
    "    if 'response' not in r: \n",
    "        log(\"[get_context]\\t(terminating)\\t%s\",(str(r),))\n",
    "        sys.stdout.flush()\n",
    "        os._exit(0)\n",
    "        \n",
    "    if not r:\n",
    "        return []\n",
    "    return r['response']['docs']\n",
    "\n",
    "#from wsd\n",
    "def word2vec_context_candidate_scores (S, M, candslist, ws=5):\n",
    "    '''returns entity scores using the similarity with their context\n",
    "       Inputs: \n",
    "           S: Sentence\n",
    "           M: Mentions\n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            ws: word size\n",
    "       Returns:\n",
    "           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n",
    "    '''\n",
    "    \n",
    "    candslist_scores=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        pos = M[i][0]\n",
    "        context = S[max(pos-ws,0):pos]+S[pos+1:pos+ws+1]\n",
    "        context_vec = sp.zeros(getword2vec_model().vector_size)\n",
    "        for c in context:\n",
    "            context_vec += getword2vector(c).as_matrix()\n",
    "        cand_scores=[]\n",
    "\n",
    "        for c in cands:\n",
    "            try:\n",
    "                # We have zero vectors, so this can rais an exception\n",
    "                # or return none                \n",
    "                cand_vector = getentity2vector(encode_entity(c[0],'word2vec', get_id=False))\n",
    "                d = 1-sp.spatial.distance.cosine(context_vec, cand_vector);\n",
    "            except:\n",
    "                d=0                \n",
    "            if np.isnan(d):\n",
    "                d=0\n",
    "            \n",
    "            cand_scores.append(d)    \n",
    "        candslist_scores.append(cand_scores) \n",
    "\n",
    "    return candslist_scores\n",
    "\n",
    "#from wsd\n",
    "def word2vec_context_disambiguate(S, M, candslist):\n",
    "    '''Disambiguate a sentence using word-context similarity\n",
    "       Inputs: \n",
    "           S: Sentence\n",
    "           M: Mentions\n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           \n",
    "       Returns: \n",
    "           a list of entity ids and a list of titles\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    candslist_scores = word2vec_context_candidate_scores (S, M, candslist)\n",
    "                      \n",
    "    # Iterate \n",
    "    true_entities = []\n",
    "    for cands, cands_scores in zip(candslist, candslist_scores):\n",
    "        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n",
    "        true_entities.append(cands[max_index][0])\n",
    "\n",
    "    titles = ids2title(true_entities)\n",
    "    return true_entities, titles \n",
    "\n",
    "\n",
    "\n",
    "#from wikisim\n",
    "def get_solr_count(s):\n",
    "    \"\"\" Gets the number of documents the string occurs \n",
    "        NOTE: Multi words should be quoted\n",
    "    Arg:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Returns:\n",
    "        The number of documents\n",
    "    \"\"\"\n",
    "    q='+text:(%s)'%(s,)\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    params={'indent':'on', 'wt':'json', 'q':q, 'rows':0}\n",
    "    r = session.get(qstr, params=params)\n",
    "    D = r.json()['response']\n",
    "    return D['numFound']\n",
    "\n",
    "\n",
    "\n",
    "# Editing Ryan's code\n",
    "def context_to_profile_sim(mention, context, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the relevancy scores of the candidates based on the context.\n",
    "    Args:\n",
    "        mention: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The score for each candidate in the same order as the candidates.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # put text in right format\n",
    "    if not context:\n",
    "        return [0]*len(candidates)\n",
    "    context = solr_escape(context)\n",
    "    mention = solr_escape(mention)\n",
    "    \n",
    "    filter_ids = \" \".join(['id:' +  str(tid) for tid,_ in candidates])\n",
    "        \n",
    "\n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    qst = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    #q='text:('+context+')^1 title:(' + mention+')^1.35'\n",
    "    q='text:('+context+')'\n",
    "    \n",
    "    params={'fl':'id score', 'fq':filter_ids, 'indent':'on',\n",
    "            'q':q, 'wt':'json','rows':len(candidates)}\n",
    "    \n",
    "    \n",
    "    r = session.get(qst, params = params).json()['response']['docs']\n",
    "    id_score_map=defaultdict(float, {long(ri['id']):ri['score'] for ri in r})\n",
    "    id_score=[id_score_map[c] for c,_ in candidates]\n",
    "    return id_score\n",
    "\n",
    "# Important TODO\n",
    "# This queriy is very much skewed toward popularity, better to replace space with AND\n",
    "#!!!! I don't like this implementation, instead of retrieving and counting, better to let the \n",
    "# solr does the counting, \n",
    "def context_to_context_sim(mention, context, candidates, rows=100):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the relevancy scores of the candidates based on the context.\n",
    "    Args:\n",
    "        mentionStr: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The score for each candidate in the same order as the candidates.\n",
    "    \"\"\"\n",
    "    if not context:\n",
    "        return [0]*len(candidates)\n",
    "    \n",
    "    # put text in right format\n",
    "    context = solr_escape(context)\n",
    "    mention = solr_escape(mention)\n",
    "    \n",
    "    filter_ids = \" \".join(['entityid:' +  str(tid) for tid,_ in candidates])\n",
    "    \n",
    "    \n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    qstr = 'http://localhost:8983/solr/enwiki20160305_context/select'\n",
    "    q=\"_context_:(%s) entity:(%s)\" % (context,mention)\n",
    "    q=\"_context_:(%s) \" % (context)\n",
    "    \n",
    "    params={'fl':'entityid', 'fq':filter_ids, 'indent':'on',\n",
    "            'q':q,'wt':'json', 'rows':rows}\n",
    "    r = session.get(qstr, params = params)\n",
    "    cnt = Counter()\n",
    "    \n",
    "    for doc in r.json()['response']['docs']:\n",
    "        cnt[long(doc['entityid'])] += 1\n",
    "    \n",
    "    id_score=[cnt[c] for c,_ in candidates]\n",
    "    return id_score\n",
    "\n",
    "\n",
    "def context_candidate_scores (S, M, candslist, ws=5, method='c2c', skip_current=1):\n",
    "    '''returns entity scores using  context seatch\n",
    "       Inputs: \n",
    "           S: Sentence\n",
    "           M: Mentions\n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            ws: word size\n",
    "            method: Either 'c2p': for context to profile, or 'c2c' for context to context\n",
    "            skip_current: Whether or not include the current mention in the context\n",
    "       Returns:\n",
    "           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n",
    "    '''\n",
    "    candslist_scores=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        pos = M[i][0]\n",
    "        mention=S[pos]\n",
    "        context = S[max(pos-ws,0):pos]+S[pos+skip_current:pos+ws+1]\n",
    "        context=\" \".join(context)\n",
    "        \n",
    "        if method == 'c2p':\n",
    "            cand_scores=context_to_profile_sim(mention, context, cands)\n",
    "        if method == 'c2c':\n",
    "            cand_scores=context_to_context_sim(mention, context, cands)\n",
    "            \n",
    "        candslist_scores.append(cand_scores) \n",
    "\n",
    "    return candslist_scores\n",
    "\n",
    "def mention_to_title_sim(mention, candidates):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Uses Solr to find the string similarity scores between the mention candidates.\n",
    "    Args:\n",
    "        mention: The mention as it appears in the text\n",
    "        context: The words that surround the target word.\n",
    "        candidates: A list of candidates that each have the entity id and its frequency/popularity.\n",
    "    Return:\n",
    "        The score for each candidate in the same order as the candidates.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # put text in right format\n",
    "    mention = solr_escape(mention)\n",
    "    \n",
    "    filter_ids = \" \".join(['id:' +  str(tid) for tid,_ in candidates])\n",
    "        \n",
    "\n",
    "    # select all the docs from Solr with the best scores, highest first.\n",
    "    qst = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "    q='title:(' + mention+')'\n",
    "    \n",
    "    params={'fl':'id score', 'fq':filter_ids, 'indent':'on',\n",
    "            'q':q, 'wt':'json','rows':len(candidates)}\n",
    "    \n",
    "    \n",
    "    r = session.get(qst, params = params).json()['response']['docs']\n",
    "    id_score_map=defaultdict(float, {long(ri['id']):ri['score'] for ri in r})\n",
    "    id_score=[id_score_map[c] for c,_ in candidates]\n",
    "    return id_score\n",
    "\n",
    "def mention_candidate_score(S, M, candslist):\n",
    "    return [mention_to_title_sim(S[m[0]], c) for m,c in zip(M,candslist) ]\n",
    "\n",
    "def popularity_score(candslist):\n",
    "    \"\"\"Retrieves the popularity score from the candslist\n",
    "    \"\"\"\n",
    "    scores=[[s for _, s in cands] for cands in candslist]\n",
    "    return scores\n",
    "\n",
    "def normalize(scores_list):\n",
    "    \"\"\"Normalize a matrix, row-wise\n",
    "    \"\"\"\n",
    "    normalized_scoreslist=[]\n",
    "    for scores in scores_list:\n",
    "        smooth=0\n",
    "        if 0 in scores:\n",
    "            smooth=1\n",
    "        sum_s = sum(s+smooth for s in scores )        \n",
    "        n_scores = [float(s+smooth)/sum_s for s in scores]\n",
    "        normalized_scoreslist.append(n_scores)\n",
    "    return normalized_scoreslist\n",
    "        \n",
    "def normalize_minmax(scores_list):\n",
    "    \"\"\"Normalize a matrix, row-wise, using minmax technique\n",
    "    \"\"\"\n",
    "    normalized_scoreslist=[]\n",
    "    for scores in scores_list:\n",
    "        scores_min = min(scores)        \n",
    "        scores_max = max(scores)        \n",
    "        if scores_min == scores_max:\n",
    "            n_scores = [0]*len(scores)\n",
    "        else:\n",
    "            n_scores = [(float(s)-scores_min)/(scores_max-scores_min) for s in scores]\n",
    "        normalized_scoreslist.append(n_scores)\n",
    "    return normalized_scoreslist\n",
    "\n",
    "def find_max(candslist,candslist_scores):\n",
    "    '''Disambiguate a sentence using a list of candidate-score tuples\n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, s11),...(c1k, s1k)],...[(cn1, sn1),...(c1m, s1m)]]\n",
    "       Returns: \n",
    "           a list of entity ids and a list of titles\n",
    "    '''\n",
    "            \n",
    "    true_entities = []\n",
    "    for cands, cands_scores in zip(candslist, candslist_scores):\n",
    "        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n",
    "        true_entities.append(cands[max_index][0])\n",
    "    titles = ids2title(true_entities)\n",
    "    return true_entities, titles        \n",
    "\n",
    "#Delete, useless\n",
    "def disambiguate_random(C):\n",
    "    '''Disambiguate using the given order (which can be random)\n",
    "        Input:\n",
    "            C: Candlist\n",
    "        Output:\n",
    "            Disambiguated entities\n",
    "    '''\n",
    "    \n",
    "    ids = [c[0][0] for c in C ]\n",
    "    titles= ids2title(ids)\n",
    "    return ids, titles\n",
    "\n",
    "def get_scores(S, M, C, method):\n",
    "    \"\"\" Disambiguate C list using a disambiguation method \n",
    "        Inputs:\n",
    "            S: Sentence\n",
    "            M: Metntions\n",
    "            C: Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            method: similarity method\n",
    "            direction: embedding type\n",
    "            op_method: disambiguation method \n",
    "                        most important ones: ilp (integer linear programming), \n",
    "                                             key: Key Entity based method\n",
    "        \n",
    "    \"\"\"\n",
    "    scores=None\n",
    "    if method == 'popularity'  :\n",
    "        scores = popularity_score(C)\n",
    "    if method == 'keydisamb'  :\n",
    "        scores = coherence_scores_driver(C, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\")\n",
    "    if method == 'entitycontext'  :\n",
    "        scores = coherence_scores_driver(C, method='rvspagerank', direction=DIR_BOTH, op_method=\"entitycontext\")\n",
    "    if method == 'mention2entity'  :\n",
    "        scores = mention_candidate_score (S, M, C)\n",
    "    if method == 'context2context'  :\n",
    "        scores = context_candidate_scores (S, M, C, method='c2c')\n",
    "    if method == 'context2profile'  :\n",
    "        scores = context_candidate_scores (S, M, C, method='c2p')    \n",
    "    if method == 'learned'  :\n",
    "        scores = learned_scores (S, M, C)    \n",
    "        \n",
    "    #scores = normalize_minmax(scores)    \n",
    "    return scores\n",
    "\n",
    "def formated_scores(scores):\n",
    "    \"\"\"Only for pretty-printing\n",
    "    \"\"\"\n",
    "    scores = [['{0:.2f}'.format(s) for s in cand_scores] for cand_scores in scores]\n",
    "    return scores\n",
    "\n",
    "def formated_all_scores(scores):\n",
    "    \"\"\"Only for pretty-printing\n",
    "    \"\"\"\n",
    "    scores = [[tuple('{0:.2f}'.format(s) for s in sub_scores) for sub_scores in cand_scores] for cand_scores in scores]\n",
    "    return scores\n",
    "\n",
    "def get_all_scores(S, M, C):\n",
    "    \"\"\"Give all scores as different lists\n",
    "        Inputs:\n",
    "            S: segmented sentence [w1, ..., wn]\n",
    "            M: mensions [m1, ... , mj]\n",
    "            C: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "\n",
    "        Output:\n",
    "            Scores, in this format [[(c111,.., c1k1),...(cm11,.., cmks)],...[(c1n1,.., pm1s),...(c1m1,.., p1ms)]]\n",
    "            where cijk is the k-th scores for cij candidate\n",
    "        \n",
    "            Scores, in this format [[(c111, c11s),...(c1k1, c1ks)],...[(cn11, pn1s),...(c1m1, p1ms)]]\n",
    "            where cijk is the k-th scores for cij candidate\n",
    "    \"\"\"\n",
    "    all_scores= [get_scores(S, M, C, method) for method in \\\n",
    "           ['popularity','keydisamb','entitycontext','mention2entity','context2context','context2profile']]\n",
    "    return [zip(*s) for s in zip(*all_scores)]\n",
    "\n",
    "\n",
    "\n",
    "def keyentity_disambiguate(candslist, direction=DIR_OUT, method='rvspagerank'):\n",
    "    '''Disambiguate a sentence using key-entity method\n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "       Returns: \n",
    "           a list of entity ids and a list of titles\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    candslist_scores = keyentity_candidate_scores (candslist, direction, method)\n",
    "    # Iterate \n",
    "    true_entities = []\n",
    "    for cands, cands_scores in zip(candslist, candslist_scores):\n",
    "        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n",
    "        true_entities.append(cands[max_index][0])\n",
    "\n",
    "    titles = ids2title(true_entities)\n",
    "    return true_entities, titles  \n",
    "\n",
    "def learned_scores (S, M, candslist):\n",
    "    '''returns entity scores using the learned (learned-to-rank method)\n",
    "       Inputs: \n",
    "           S: Sentence\n",
    "           M: Mentions\n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "       Returns:\n",
    "           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n",
    "    '''\n",
    "    if (wsd_model_preprocessor_ is None) or (wsd_model_ is None):\n",
    "        log('[learned_scores]\\tmodel not loaded')\n",
    "        raise Exception('model not loaded, try load_wsd_model()')\n",
    "    \n",
    "    all_scores=get_all_scores(S,M,candslist)\n",
    "    return [wsd_model_.predict(wsd_model_preprocessor_.transform(cand_scores)) for cand_scores in all_scores] \n",
    "\n",
    "def wsd(S, M, C, method='learned'):\n",
    "    '''Gets a sentence, mentions and candslist, and returns disambiguation\n",
    "       Inputs: \n",
    "           S: Sentence\n",
    "           M: Mentions\n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            method: disambiguation method \n",
    "       Returns: \n",
    "           A disambiguated list in the form of  (true_entities, titles)\n",
    "    \n",
    "    '''\n",
    "    candslist_scores = get_scores(S, M, C, method)\n",
    "    return find_max(C,candslist_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetrate Train Data Repository for WSD\n",
    "Uses and already created dataset (wiki-mentions.30000.json) which contain 30000 Wikipedia paragraphs to generate \n",
    "data for training the Learn2Rank Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile gen_trainrep.py \n",
    "\"\"\" Create a train-set \n",
    "    entity_id, query_id, scores1, score2, ..., scoren, true/false (is it a correct entity)\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "from wsd import *\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "\n",
    "sys.stdout.flush()\n",
    "\n",
    "max_t=20\n",
    "max_count=5000\n",
    "#np.seterr(all='raise')\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wsd')\n",
    "outfile = os.path.join('../datasets/ner/trainrepository.%s.30000.tsv'%(max_count,))\n",
    "if os.path.isfile(outfile): \n",
    "    sys.stderr.write(outfile + \" already exist!\\n\")\n",
    "    #sys.exit()\n",
    "\n",
    "dsname = os.path.join('../datasets/ner/wiki-mentions.30000.json')\n",
    "\n",
    "count = 0          \n",
    "with open(dsname,'r') as ds, open(outfile,'w') as outf:\n",
    "    qid=0\n",
    "    for line in ds:                           \n",
    "        js = json.loads(line.decode('utf-8').strip());\n",
    "        S = js[\"text\"]\n",
    "        M = js[\"mentions\"]\n",
    "        count +=1        \n",
    "        print \"%s:\\tS=%s\\n\\tM=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))        \n",
    "        C = generate_candidates(S, M, max_t=max_t, enforce=False)\n",
    "        all_scores=get_all_scores(S,M,C)\n",
    "        for i in  range(len(C)):\n",
    "            m=M[i]\n",
    "            cands = C[i]\n",
    "            cand_scores = all_scores[i]\n",
    "            wid = title2id(m[1]) \n",
    "            for (eid,_),scores in zip (cands, cand_scores):\n",
    "                is_true_eid = (wid == eid)\n",
    "                string_scores=[str(s) for s in scores]\n",
    "                outf.write(\"\\t\".join([str(eid), str(qid)]+string_scores+[str(int(is_true_eid))])+\"\\n\")\n",
    "            qid += 1\n",
    "        if count >= max_count:\n",
    "            break\n",
    "print \"Done\"             \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the LTR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_ltr.py \n",
    "\"\"\" Train a LambdaMart (LTR) Method\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "import pyltr\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib\n",
    "from wsd import *\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "\n",
    "#Columns = [entity_id, qid, score0, score1, score5, label]\n",
    "outdir = os.path.join(baseresdir, 'wikify')\n",
    "tr_file_name = os.path.join('../datasets/ner/trainrepository.5000.30000.tsv')\n",
    "nrows=50000\n",
    "data=pd.read_table(tr_file_name, nrows=nrows, header=None)\n",
    "\n",
    "# Can't shuffle straighforwardly, I should group by quid, the shuffle\n",
    "# But I guess shuffling is done in the estimator\n",
    "#data = data.sample(frac=1)\n",
    "\n",
    "\n",
    "num_cols = len(data.columns)\n",
    "\n",
    "grouped=data.groupby(1)\n",
    "total_len=len(grouped)\n",
    "group = grouped.filter(lambda x:x.iloc[0,1] >= 0 and x.iloc[0,1] < 0.6*total_len)\n",
    "\n",
    "#Train Data\n",
    "#The following line does does the int-->float conversion, is it reliable? \n",
    "#Should I care later, while testing?\n",
    "X_train = group.iloc[:,2:num_cols-1].as_matrix()\n",
    "\n",
    "# Train the transformer and preprocess X_train\n",
    "ltr_preprocessor = MinMaxScaler()\n",
    "X_train=ltr_preprocessor.fit_transform(X_train)\n",
    "ltr_preprocessor_fn = os.path.join('../model/tmp/ltr_preprocessor.%s.pkl' %(nrows,))\n",
    "joblib.dump(ltr_preprocessor, open(ltr_preprocessor_fn, 'wb'))\n",
    "####\n",
    "\n",
    "y_train = group.iloc[:,num_cols-1].as_matrix()\n",
    "qid_train = group.iloc[:,1].as_matrix()\n",
    "\n",
    "\n",
    "#Validation Data\n",
    "group=grouped.filter(lambda x:x.iloc[0,1] >= 0.6*total_len and x.iloc[0,1] < 0.8*total_len)\n",
    "X_validate = group.iloc[:,2:num_cols-1].as_matrix()\n",
    "X_validate = ltr_preprocessor.transform(X_validate)\n",
    "\n",
    "y_validate = group.iloc[:,num_cols-1].as_matrix()\n",
    "qid_validate = group.iloc[:,1].as_matrix()\n",
    "\n",
    "#Test Data\n",
    "group=grouped.filter(lambda x:x.iloc[0,1] >= 0.8*total_len and x.iloc[0,1] < 1.0*total_len)\n",
    "X_test = group.iloc[:,2:num_cols-1].as_matrix()\n",
    "X_test = ltr_preprocessor.transform(X_test)\n",
    "\n",
    "y_test = group.iloc[:,num_cols-1].as_matrix()\n",
    "qid_test = group.iloc[:,1].as_matrix()\n",
    "\n",
    "monitor = pyltr.models.monitors.ValidationMonitor(\n",
    "     X_validate, y_validate, qid_validate, metric=pyltr.metrics.NDCG(k=10), stop_after=250)\n",
    "model = pyltr.models.LambdaMART(n_estimators=300, learning_rate=0.1, verbose = 1)\n",
    "#lmart.fit(TX, TY, Tqid, monitor=monitor)\n",
    "print \"Training, sample_count: %s\" % (nrows)\n",
    "\n",
    "model.fit(X_train, y_train, qid_train, monitor=monitor)\n",
    "\n",
    "metric = pyltr.metrics.NDCG(k=10)\n",
    "Ts_pred = model.predict(X_test)\n",
    "print 'Random ranking:', metric.calc_mean_random(qid_test, y_test)\n",
    "print 'Our model:', metric.calc_mean(qid_test, y_test, Ts_pred)\n",
    "\n",
    "model_file_name = os.path.join('../model/tmp/ltr.%s.pkl'%(nrows,))\n",
    "joblib.dump(model, open(model_file_name, 'wb'))\n",
    "\n",
    "print 'Model saved'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mention Detection\n",
    "\n",
    "Contains our mention detection modules, we try two methods:\n",
    "* CoreNLP\n",
    "* Train an SVM on top of SolrTextTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mention_detection.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mention_detection.py \n",
    "from mention_detection import *\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from wsd import *\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "\n",
    "#constants\n",
    "CORE_NLP=0\n",
    "LEARNED_MENTION=1\n",
    "\n",
    "SVC_HP_NROWS_S, SVC_HP_CV_S = 10000,1\n",
    "SVC_HR_NROWS_S, SVC_HR_CV_S = 10000,20\n",
    "\n",
    "\n",
    "SVC_HP_NROWS_L, SVC_HP_CV_L = 50000,1\n",
    "SVC_HR_NROWS_L, SVC_HR_CV_L = 50000,20\n",
    "\n",
    "\n",
    "mention_model_preprocessor_=None\n",
    "mention_model_=None\n",
    "\n",
    "def load_mention_model(nrows, svc):\n",
    "    global mention_model_preprocessor_, mention_model_\n",
    "    mention_model_preprocessor_fn = os.path.join(MODELDIR, 'svc_preprocessor.%s.pkl' % (nrows,))\n",
    "    if os.path.isfile(mention_model_preprocessor_fn): \n",
    "        log(\"[load_mention_model]\\tmention_model_preprocessor file (%s) loaded\" % (mention_model_preprocessor_fn,))\n",
    "        mention_model_preprocessor_ = joblib.load(open(mention_model_preprocessor_fn, 'rb'))\n",
    "    else:\n",
    "        log(\"[load_mention_model]\\tmention_model_preprocessor file (%s) not found\" % (mention_model_preprocessor_fn,))\n",
    "\n",
    "\n",
    "    mention_model_fn = os.path.join(MODELDIR, 'svc_mentions_unbalanced.%s.%s.pkl' % (nrows,svc))\n",
    "    if os.path.isfile(mention_model_fn): \n",
    "        mention_model_ = joblib.load(open(mention_model_fn, 'rb'))    \n",
    "        log(\"[load_mention_model]\\tmention_model_ file (%s) loaded\" % (mention_model_fn,))\n",
    "    else:\n",
    "        log(\"[load_mention_model]\\tmention_model_ file (%s) not found\" % (mention_model_fn,))\n",
    "        \n",
    "\n",
    "def tokenize_stanford(text):\n",
    "    addr = 'http://localhost:9001'\n",
    "    params={'annotators': 'tokenize', 'outputFormat': 'json'}\n",
    "    r = session.post(addr, params=params, data=text.encode('utf-8'))    \n",
    "    \n",
    "    return [token['originalText'] for token in r.json()['tokens']]\n",
    "\n",
    "def encode_solrtexttagger_result(text,tags):\n",
    "    \"\"\" Convert the solrtext output to our M,S format\n",
    "        input:\n",
    "            text: The original text\n",
    "            tags: The result of the solrtexttagger\n",
    "        output:\n",
    "            S,M\n",
    "            S: segmented sentence [w1, ..., wn]\n",
    "            M: mensions [m1, ... , mj]\n",
    "    \"\"\"\n",
    "    start=0\n",
    "    termindex=0\n",
    "    S=[]\n",
    "    M=[]\n",
    "    # pass 1, adjust partial mentions. \n",
    "    # approach one, expand (the other could be shrink)\n",
    "    \n",
    "    for tag in tags:\n",
    "        assert text[tag[1]:tag[3]] == tag[5]\n",
    "        seg = text[start:tag[1]]\n",
    "        S += seg.strip().split()\n",
    "        M.append([len(S),'UNKNOWN'])\n",
    "        S += [\" \".join(text[tag[1]:tag[3]].split())]\n",
    "        start = tag[3]\n",
    "        \n",
    "    S += text[start:].strip().split()\n",
    "    return S, M\n",
    "\n",
    "def annotate_with_solrtagger(text):\n",
    "    ''' Annonate a text using solrtexttagger\n",
    "        Input: \n",
    "            text: The input text *must be unicode*\n",
    "        Output:\n",
    "            Annotated text\n",
    "    '''\n",
    "    addr = 'http://localhost:8983/solr/enwikianchors20160305/tag'\n",
    "    params={'overlaps':'LONGEST_DOMINANT_RIGHT', 'tagsLimit':'5000', 'fl':'id','wt':'json','indent':'on','matchText':'true'}\n",
    "    text=solr_escape(text)\n",
    "    r = session.post(addr, params=params, data=text.encode('utf-8'))    \n",
    "\n",
    "    S,M = encode_solrtexttagger_result(text,r.json()['tags'])\n",
    "    return S,M\n",
    "\n",
    "\n",
    "def encode_corenlp_result(text,annotated):\n",
    "    \"\"\" Convert the corenlp output to our M,S format\n",
    "        input:\n",
    "            text: The original text\n",
    "            mentions: The result of the solrtexttagger\n",
    "        output:\n",
    "            S,M\n",
    "            S: segmented sentence [w1, ..., wn]\n",
    "            M: mensions [m1, ... , mj]\n",
    "    \"\"\"\n",
    "    #****** Important ****\n",
    "    #* The indices are not correct if it contains unicode, \n",
    "    #* in case you need to work with the indices, decode to utf-8\n",
    "    #******\n",
    "    S=[]\n",
    "    M=[]\n",
    "    P=[]\n",
    "    # pass 1, adjust partial mentions. \n",
    "    # approach one, expand (the other could be shrink)\n",
    "    \n",
    "    for sentence in annotated['sentences']: \n",
    "        start=0\n",
    "        \n",
    "        for mention in sentence['entitymentions']:\n",
    "            S += [token['originalText'] for token in sentence['tokens'][start:mention['tokenBegin']]]\n",
    "            M.append([len(S),'UNKNOWN'])\n",
    "            mentionstr = mention['text']\n",
    "            S += [mentionstr]\n",
    "            start = mention['tokenEnd']\n",
    "\n",
    "        S += [token['originalText'] for token in sentence['tokens'][start:]]\n",
    "        P += [[token['originalText'],token['pos']] for token in sentence['tokens']]\n",
    "    return S, M, P\n",
    "\n",
    "def annotate_with_corenlp(text):\n",
    "    ''' Annonate a text using coreNLP\n",
    "        Input: \n",
    "            text: The input text\n",
    "        Output:\n",
    "            Annotated text\n",
    "    '''\n",
    "    addr = 'http://localhost:9001'\n",
    "    params={'annotators': 'entitymentions', 'outputFormat': 'json'}\n",
    "    r = session.post(addr, params=params, data=text.encode('utf-8'))    \n",
    "\n",
    "    \n",
    "    S,M, P = encode_corenlp_result(text, r.json())\n",
    "    return S,M,P\n",
    "\n",
    "def solrtagger_pos(S,M,P):\n",
    "    ''' Alligns the tags from corenlp to solrtagger's mentions\n",
    "        Input:\n",
    "            S: Sentence \n",
    "            M: Mentions\n",
    "            P: POS of the mentions, from corenlp\n",
    "        Output:\n",
    "            Q: POS of solrtagger's mentions\n",
    "    '''\n",
    "    Q=[]\n",
    "    j=0\n",
    "    for i in range(len(M)):\n",
    "        m=tokenize_stanford(solr_unescape(S[M[i][0]])) \n",
    "        j_backup=j\n",
    "        q=[]\n",
    "        while j<len(P):\n",
    "            if similar(P[j][0], m[0])> .8:\n",
    "                k=0\n",
    "                while similar(P[j][0], m[k])>0.8:\n",
    "                    #q.append(P[j]) #good for debugging\n",
    "                    q.append(P[j][1]) #good for debugging\n",
    "                    k=k+1\n",
    "                    j=j+1\n",
    "                    if j >= len(P) or k>=len(m):\n",
    "                        break\n",
    "\n",
    "                Q.append(\" \".join(q))\n",
    "                break\n",
    "            j=j+1\n",
    "        if not q:\n",
    "            Q.append(\"OTHER\")\n",
    "            j=j_backup\n",
    "    return Q\n",
    "\n",
    "def get_mention_count(s):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the amount of times that the given string appears as a mention in wikipedia.\n",
    "    Args:\n",
    "        s: the string (can contain AND, OR, ..)\n",
    "    Return:\n",
    "        The amount of times the given string appears as a mention in wikipedia\n",
    "    \"\"\"\n",
    "    \n",
    "    return sum(c for _,c in anchor2concept(s))  \n",
    "\n",
    "def mention_prob(text):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Returns the probability that the text is a mention in Wikipedia.\n",
    "    Args:\n",
    "        text: \n",
    "    Return:\n",
    "        The probability that the text is a mention in Wikipedia.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_mentions = get_mention_count(text)\n",
    "    total_appearances = get_solr_count(text.replace(\".\", \"\"))\n",
    "    if total_appearances == 0:\n",
    "        return 0 # a mention never used probably is not a good link\n",
    "    return float(total_mentions)/total_appearances\n",
    "\n",
    "def get_mention_probs(S,M):\n",
    "    return [mention_prob(S[m[0]]) for m in M]\n",
    "\n",
    "\n",
    "def boil_down_candidate_score(score_list):\n",
    "    return [sum(scores)/len(scores) for scores in scores_list]\n",
    "        \n",
    "    \n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def mention_overlap(S1, M1, S2,M2):\n",
    "    '''Calculates the overlap between two given detected mentions\n",
    "        Input:\n",
    "            S1: Source Setnence\n",
    "            M1: Source Mention\n",
    "            S2: Destination Sentence\n",
    "            M2: Destination mention            \n",
    "        Output: A 0/1 vector of size M1, each element shows whether M1[i] is also in M2\n",
    "    '''\n",
    "    is_detected = []\n",
    "    for m1 in M1:\n",
    "        found = 0\n",
    "        for m2 in M2:\n",
    "            if similar(S1[m1[0]], S2[m2[0]])>0.8:\n",
    "                found=1\n",
    "        is_detected.append(found)\n",
    "    return is_detected\n",
    "\n",
    "def detect_and_score_mentions(text, max_t=5):\n",
    "    \"\"\"Give\n",
    "        Uses solrtagger to detect mentions, and score them\n",
    "        Inputs:\n",
    "            text: Given text\n",
    "        Output:\n",
    "            Scores, in this format [[(c111, c11s),...(c1k1, c1ks)],...[(cn11, pn1s),...(c1m1, p1ms)]]\n",
    "            where cijk is the k-th scores for cij candidate\n",
    "    \"\"\"\n",
    "    text = solr_encode(text)\n",
    "    solr_S, solr_M = annotate_with_solrtagger(text)\n",
    "    # max_t does not have to equal the number of candidates in wsd, it's just to \n",
    "    # get an average relevancy\n",
    "    solr_C = generate_candidates(solr_S, solr_M, max_t=max_t, enforce=False)\n",
    "    \n",
    "    \n",
    "    wsd_scores = [[sum(sc)/len(sc) for sc in get_scores(solr_S, solr_M, solr_C, method)] for method in \\\n",
    "               ['popularity','entitycontext','mention2entity','context2context','context2profile']]\n",
    "\n",
    "    mention_scores=[]\n",
    "    mention_scores.extend(wsd_scores)\n",
    "    mention_scores.append(get_mention_probs(solr_S, solr_M))\n",
    "    \n",
    "    core_S, core_M, core_P = annotate_with_corenlp(text)\n",
    "    overlap_with_corenlp = mention_overlap(solr_S, solr_M, core_S,core_M)\n",
    "    mention_scores.append(overlap_with_corenlp)\n",
    "    \n",
    "    pos_list = solrtagger_pos(solr_S, solr_M,core_P)\n",
    "    mention_scores.append(pos_list)\n",
    "    \n",
    "    return solr_S, solr_M, zip(*mention_scores)\n",
    "\n",
    "\n",
    "def get_learned_mentions(text):\n",
    "    if (mention_model_preprocessor_ is None) or (mention_model_ is None):\n",
    "        log('[mention_models]\\tmodel not loaded')\n",
    "        raise Exception('model not loaded, try load_mention_model()')\n",
    "        \n",
    "    S_solr,M_solr,scores = detect_and_score_mentions(text)\n",
    "    M_scores=[]\n",
    "    for sc_vec in scores:\n",
    "        # Unintuitive: When fitting, the first column was the mention_id, which was ignored!\n",
    "        # And the preprocessor needs the exact column names!\n",
    "        sc_frame = pd.DataFrame([sc_vec], columns=[str(i+1) for i in range(len(sc_vec))])\n",
    "        X = mention_model_preprocessor_.transform(sc_frame)\n",
    "        M_scores.append(mention_model_.predict(X))\n",
    "    M = [m for m_s, m in zip(M_scores, M_solr) if m_s==1]\n",
    "    return S_solr, M\n",
    "    \n",
    "def detect_mentions(text, mentionmethod=CORE_NLP):\n",
    "    if mentionmethod == CORE_NLP:\n",
    "        S, M, _ = annotate_with_corenlp(text)        \n",
    "    if mentionmethod == LEARNED_MENTION:\n",
    "        S, M =  get_learned_mentions(text)\n",
    "    return S, M\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mention Detection Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mention_detection import *\n",
    "text = \"I want to Brazil to visit Romario, but David was in Real Madrid and I couldn't eat Kebab\"\n",
    "S, M = detect_mentions(text, mentionmethod = CORE_NLP)\n",
    "for m in M:\n",
    "    print S[m[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Train Data Repository For Mention Detection\n",
    "\n",
    "Uses and already created dataset (wiki-mentions.30000.json) which contain 30000 Wikipedia paragraphs to generate \n",
    "data for training the SVM Model for mention detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile gen_trainrep_for_mention.py \n",
    "\"\"\" Create a train-set \n",
    "    entity_id, query_id, scores1, score2, ..., scoren, true/false (is it a correct entity)\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "from mention_detection import *\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "\n",
    "\n",
    "max_count=5000\n",
    "skip_lines=0\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wsd')\n",
    "outfile = os.path.join('../datasets/ner/mentiontrainrepository.%s.30000.tsv'%(max_count,))\n",
    "if os.path.isfile(outfile): \n",
    "    sys.stderr.write(outfile + \" already exist!\\n\")\n",
    "    sys.exit()\n",
    "\n",
    "dsname = os.path.join('../datasets/ner/wiki-mentions.30000.json')\n",
    "\n",
    "count = 0  \n",
    "mention_id = 0\n",
    "with open(dsname,'r') as ds, open(outfile,'w') as outf:\n",
    "    for line in ds:                           \n",
    "        count +=1  \n",
    "        if count <= skip_lines:\n",
    "            continue\n",
    "            \n",
    "        js = json.loads(line.decode('utf-8').strip());\n",
    "        S = js[\"text\"]\n",
    "        M = js[\"mentions\"]\n",
    "        text= \" \".join(S)\n",
    "        print \"%s:\\tS=%s\\n\\tM=%s\\ttext=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'),text.encode('utf-8'))        \n",
    "        \n",
    "        solr_S, solr_M, scores = detect_and_score_mentions(text)\n",
    "        correct_mention = mention_overlap(solr_S, solr_M, S, M)\n",
    "        for i in  range(len(solr_M)):\n",
    "            string_scores=[str(s) for s in scores[i]]\n",
    "            outf.write(\"\\t\".join([str(mention_id)] + string_scores+[str(correct_mention[i])])+\"\\n\")\n",
    "            mention_id += 1\n",
    "        if count >= max_count:\n",
    "            break\n",
    "print \"Done\"             \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVC Model for Mention Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_svc.py \n",
    "''' Trains an SVC for mention detection\n",
    "'''\n",
    "from mention_detection import *\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn_pandas import gen_features\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "def downsample_negatives(X_train, y_train, frac=0.2):\n",
    "\n",
    "    pos_index = y_train==1\n",
    "    X_train_pos = X_train[pos_index,:]\n",
    "    y_train_pos = y_train[pos_index]\n",
    "\n",
    "    neg_index = y_train==0\n",
    "    X_train_neg = X_train[neg_index,:]\n",
    "    y_train_neg = y_train[neg_index]\n",
    "\n",
    "\n",
    "    X_train_neg, y_train_neg = sklearn.utils.resample(X_train_neg, y_train_neg, \n",
    "                                                n_samples = int(frac*len(X_train_neg)), replace=False)    \n",
    "\n",
    "    X_train_downsampled = np.vstack([X_train_pos, X_train_neg])\n",
    "    y_train_downsampled = np.hstack([y_train_pos, y_train_neg])\n",
    "\n",
    "    X_train_downsampled_shuffled, y_train_downsampled_shuffled = sklearn.utils.shuffle(X_train_downsampled, y_train_downsampled)\n",
    "    return X_train_downsampled_shuffled, y_train_downsampled_shuffled\n",
    "\n",
    "home = '/users/grad/sajadi'\n",
    "\n",
    "tr_file_name = os.path.join('../datasets/ner/mentiontrainrepository.5000.30000.tsv')\n",
    "pos_col=['8']\n",
    "nrows=50000\n",
    "data=pd.read_table(tr_file_name, header=None, nrows=nrows)\n",
    "data.columns = [str(c) for c in data.columns]\n",
    "\n",
    "# Shuffle, Shuffle and Shuffle!\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "\n",
    "num_cols = len(data.columns)\n",
    "X  = data.iloc[:,1:num_cols-1]\n",
    "y  = data.iloc[:,num_cols-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "#Preprocess X_train\n",
    "feature_def = gen_features(\n",
    "     columns=[[c] for c in X_train.columns[:7]],\n",
    "     classes=[MinMaxScaler]\n",
    " )\n",
    "\n",
    "feature_def += ((pos_col, [LabelBinarizer()]),)\n",
    "\n",
    "svc_preprocessor = DataFrameMapper(feature_def)\n",
    "X_train = svc_preprocessor.fit_transform(X_train)\n",
    "svc_preprocessor_fn = os.path.join('../model/tmp/svc_preprocessor.%s.pkl' % (nrows,))\n",
    "joblib.dump(svc_preprocessor, open(svc_preprocessor_fn, 'wb'))\n",
    "X_test = svc_preprocessor.transform(X_test)\n",
    "#####\n",
    "\n",
    "#Didn't help!!\n",
    "#X_train, y_train = downsample_negatives(X_train, y_train)\n",
    "\n",
    "for cv in [1,10,20]:\n",
    "    print \"Training, sample_count: %s\\tcv:%s\" % (nrows, cv)\n",
    "    clf = svm.SVC(kernel='linear', class_weight={1:cv})\n",
    "    clf.fit(X_train, y_train)  \n",
    "    y_pred = clf.predict(X_test)\n",
    "    measures = metrics.precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "    model_file_name = os.path.join('../model/tmp/svc_mentions_unbalanced.%s.%s.pkl' % (nrows,cv))\n",
    "    joblib.dump(clf, open(model_file_name, 'wb'))\n",
    "    print \"measures: \", measures\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print 'Model saved'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikification API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wikify.py \n",
    "from __future__ import division\n",
    "from mention_detection import *\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "HIGH_PREC_SMALL = 1\n",
    "HIGH_REC_SMALL = 2\n",
    "HIGH_PREC_LARGE = 3\n",
    "HIGH_REC_LARGE = 4\n",
    " \n",
    "\n",
    "def get_wikifify_params(opt):\n",
    "    if opt == HIGH_PREC_SMALL:\n",
    "        return SVC_HP_NROWS_S, SVC_HP_CV_S, LTR_NROWS_S\n",
    "    \n",
    "    if opt == HIGH_REC_SMALL: \n",
    "        return SVC_HR_NROWS_S, SVC_HR_CV_S, LTR_NROWS_S\n",
    "    \n",
    "    if opt == HIGH_PREC_LARGE: \n",
    "        return SVC_HP_NROWS_L, SVC_HP_CV_L, LTR_NROWS_L\n",
    "    \n",
    "    if opt == HIGH_REC_LARGE: \n",
    "        return SVC_HR_NROWS_L, SVC_HR_CV_L, LTR_NROWS_L\n",
    "\n",
    "\n",
    "def wikify_string(line, mentionmethod=CORE_NLP, max_t=20):\n",
    "    S,M = detect_mentions(line, mentionmethod)      \n",
    "    C = generate_candidates(S, M, max_t=max_t, enforce=False)\n",
    "    E = wsd(S, M, C, method='learned')\n",
    "    for m,e in zip(M,E[1]):\n",
    "        m[1]=e\n",
    "    return S,M\n",
    "\n",
    "def wikify_a_line(line, mentionmethod=CORE_NLP):\n",
    "    ''' Annotate a single line \n",
    "        Input:\n",
    "            line: The given string\n",
    "            mentionmethod: The mention detection method\n",
    "        Output:\n",
    "            Annotated Sentence inwhich mentiones are hyper-linked to the Wikipedia concepts\n",
    "    '''\n",
    "    S, M = wikify_string(line, mentionmethod) \n",
    "    for m in M: \n",
    "        S[m[0]]=\"<a href=https://en.wikipedia.org/wiki/%s>%s</a>\"  % (S[m[0]],m[1])\n",
    "    S_reconcat = \" \".join(S)\n",
    "    return S_reconcat\n",
    "            \n",
    "def wikify_api(text, mentionmethod=CORE_NLP):\n",
    "    outlist=[]\n",
    "    for line in text.splitlines():\n",
    "        outlist.append(wikify_a_line(line, mentionmethod))\n",
    "    return \"\\n\".join(outlist).decode('utf-8')\n",
    "\n",
    "def wikify_from_file_api(infilename, outfilename, mentionmethod=CORE_NLP):\n",
    "    with open(infilename) as infile, open(outfilename, 'w') as outfile:\n",
    "        for line in infilename.readlines():\n",
    "            wikified = wikify_a_line(text, mentionmethod)\n",
    "            outfile.write(wikified + \"\\n\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Wikification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SVC_MODEL_HIGH_RECALL_NROWS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7358c167db71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwikify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msvc_nrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvc_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC_MODEL_HIGH_RECALL_NROWS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVC_MODEL_HIGH_RECALL_CV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mload_mention_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvc_nrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvc_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SVC_MODEL_HIGH_RECALL_NROWS' is not defined"
     ]
    }
   ],
   "source": [
    "from wikify import *\n",
    "\n",
    "svc_nrows, svc_cv = \n",
    "\n",
    "load_mention_model(svc_nrows, svc_cv)\n",
    "\n",
    "ltr_nrows=10000\n",
    "load_wsd_model(ltr_nrows)\n",
    "\n",
    "S=[\"David\", \"and\", \"Victoria\", \"named\", \"their\", \"children\", \"Brooklyn\", \",\", \"Romeo\", \",\", \"Cruz\", \",\", \"and\", \"Harper Seven\", \".\"]\n",
    "M=[[0, \"David_Beckham\"], [2, \"Victoria_Beckham\"]]\n",
    "\n",
    "S=[\"Nixon\", \"resigned\", \"after\", \"Watergate\", \"despite\", \"his\", \"success\", \"in\", \"the\", \"Ping-Pong Diplomacy\", \"with\", \"China\", \".\"]\n",
    "M=[[0, \"Richard_Nixon\"], [3, \"Watergate_scandal\"], [9, \"Ping_Pong_Diplomacy\"], [11, \"People's_Republic_of_China\"]]\n",
    "text = \" \".join(S).decode('utf-8')\n",
    "S1,M1 = detect_mentions(text, mentionmethod=LEARNED_MENTION)      \n",
    "print S1\n",
    "print M1\n",
    "S2,M2 = wikify_string(text, mentionmethod=LEARNED_MENTION)\n",
    "print S2\n",
    "print M2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
