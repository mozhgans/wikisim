{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will walk you through creating the Solr Database and the MariaDB  table containing the  anchor-texts\n",
    "\n",
    "**Performing these steps can be tricky and there are better option [see README](../../README.md#hosting)**\n",
    "\n",
    "* **Make sure you have alreay followed the [the required steps Steps](../../README.md#hosting) explained in [README.md](../../README.md)**\n",
    "* **You can find most of the intermediate files in our website reporitory (the permament address can be found [here](../../README.md#api)) **\n",
    "\n",
    "* Note that all of the paths are hard-coded and you need to modify them, consistently, and carefully!**\n",
    "\n",
    "### Step 0: Install [Solr](http://lucene.apache.org/solr/)\n",
    "I assume it is installed in `~/backup/`\n",
    "\n",
    "### Step 1. Download the Wikipedia\n",
    "\n",
    "Download the Wikipedia dump to `../../../data/enwiki20160305/original`\n",
    "\n",
    "wget https://dumps.wikimedia.org/enwiki/20160305/enwiki-20160305-pages-articles.xml.bz2 -P `../../../data/enwiki20160305/original`\n",
    "\n",
    "### Step 2: Run the following cell to create the `toannonated.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile toannonated.py\n",
    "'''Converts the output of WikiExtractor.py to a text file with each line a json object\n",
    "    Each object has the following fields:\n",
    "    id: Wikipedia id\n",
    "    title: The title\n",
    "    opening_text: The first paragraph of the page\n",
    "    opening_annotation: The annotation of the oppenint text which is itself another json objec\n",
    "    text: The text\n",
    "    annotation\": The annotation of the text, which is itself another json object \n",
    "\n",
    "'''\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import urllib\n",
    "import sys\n",
    "from HTMLParser import HTMLParser\n",
    "sys.path.insert(0,'..')\n",
    "from wikisim.wikipedia import *\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "\n",
    "fileinput = sys.stdin\n",
    "\n",
    "# import StringIO\n",
    "# fileinput = StringIO.StringIO(inputstr)\n",
    "def replacelinks(text):\n",
    "    hp = HTMLParser()\n",
    "    \n",
    "    annotations = []\n",
    "    deltaStringLength = 0\n",
    "    hrefreg=r'<a href=\"([^\"]+)\">([^>]+)</a>'\n",
    "    ms = re.finditer(hrefreg, text)\n",
    "    \n",
    "    \n",
    "    for m in ms:     \n",
    "\n",
    "        url = m.group(1)\n",
    "        # in the parser, url->encode->quote->encode (does nothing, already unicode)->write to file\n",
    "        # we already have decoded while reading, we have to encode back, unquote, and decode, so that\n",
    "        # the last encode on writing can make it back to unicode\n",
    "        url=url.encode('utf-8')\n",
    "        url =  urllib.unquote(url)\n",
    "        url = url.decode('utf-8')\n",
    "        #sometimes we have [[&]] that has been encoded to &amp; \n",
    "        #sometimes we have [[&amp]] that has been encoded to &amp;amp; \n",
    "        #so we unexcape twice!\n",
    "        \n",
    "        url=hp.unescape(url)\n",
    "        url=hp.unescape(url)\n",
    "        url=url.replace(u\"\\xA0\",\" \")\n",
    "        \n",
    "        x = url.find(\"#\")\n",
    "        if x!=-1:\n",
    "            url=url[:x]\n",
    "        #BUG: so what if x==0?  maybe the current approach, empty \"dest\" is not bad\n",
    "        antext = m.group(2)\n",
    "        if '//' not in url:\n",
    "            annotations.append({\n",
    "                \"url\"    :   url, \n",
    "                \"surface_form\" :   antext, \n",
    "                \"from\"  :   m.start() - deltaStringLength,\n",
    "                \"to\"    :   m.start() - deltaStringLength+len(antext)\n",
    "            })\n",
    "\n",
    "        deltaStringLength += len(m.group(0)) - len(antext)\n",
    "\n",
    "    #As a second step, replace all links in the article by their label\n",
    "    text = re.sub(hrefreg, lambda m:m.group(2), text)  \n",
    "    return annotations, text\n",
    "def process():\n",
    "    hp = HTMLParser()\n",
    "    rstart=r'<doc id=\"(.*)\" url=\"(.*)\" title=\"(.*)\">'\n",
    "    rend=r'</doc>'\n",
    "\n",
    "#     open_ann= open ('open_annotation.json', 'w')\n",
    "#     ann= open ('annotation.json', 'w')\n",
    "#     wikitext=open('wikitext.json','w')\n",
    "    state=0\n",
    "    page=\"\"\n",
    "    textlist=[]\n",
    "    while True:\n",
    "        line = fileinput.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.decode('utf-8').strip()\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "        if re.match(rend,line):\n",
    "            # If you want to check the title, make sure to encode!\n",
    "            if textlist and (id2title(wid) is not None):\n",
    "                opening_ann, opening_text = replacelinks(opening_text)\n",
    "                ann,text = replacelinks(\"\\n\".join(textlist))\n",
    "                \n",
    "#                 ElasticSearch   \n",
    "#                 Buggy: convert annotations to text or do something about it\n",
    "#                 print json.dumps({\"index\":{\"_type\":\"page\",\"_id\":wid}}, ensure_ascii=False).encode('utf-8')\n",
    "#                 page={\"title\": wtitle, \"opening_text\":opening_text, \"opening_annotation\":opening_ann,\n",
    "#                       \"text\":text, \"annotation\": ann}\n",
    "                \n",
    "#                 print json.dumps(page, ensure_ascii=False).encode('utf-8')\n",
    "                \n",
    "#               General\n",
    "                page={\"id\":wid, \"title\": wtitle, \"opening_text\":opening_text, \"opening_annotation\":opening_ann,\n",
    "                      \"text\":text, \"annotation\": ann}\n",
    "                \n",
    "                print json.dumps(page, ensure_ascii=False).encode('utf-8')\n",
    "                \n",
    "            textlist=[]    \n",
    "            state=0\n",
    "            continue\n",
    "        if state==0:\n",
    "            if re.match('<doc', line):\n",
    "                ms = re.match(rstart, line)\n",
    "                wid=ms.group(1)\n",
    "                wtitle=hp.unescape(ms.group(3)).replace(u\"\\xA0\",\" \")\n",
    "\n",
    "                state = 1\n",
    "            continue    \n",
    "\n",
    "        \n",
    "        if state==1:\n",
    "            state=2\n",
    "            continue\n",
    "        textlist.append(line)\n",
    "            \n",
    "        if state==2:\n",
    "            opening_text=line\n",
    "            state=3\n",
    "            continue\n",
    "            \n",
    "#     wikitext.close()\n",
    "#     ann.close()\n",
    "if __name__ == \"__main__\": process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Anchors\n",
    "### Step 3. Run the following cell to create `extractanchors.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile extractanchors.py\n",
    "''' Extract all the anchor text with their target url\n",
    "'''\n",
    "import json\n",
    "import sys\n",
    "import urllib\n",
    "import StringIO\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "fileinput = sys.stdin\n",
    "\n",
    "#fileinput = StringIO.StringIO(inputstr)\n",
    "while True:\n",
    "    line = fileinput.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    line = line.decode('utf-8').strip()\n",
    "    js = json.loads(line)    \n",
    "    if \"annotation\" not in js:\n",
    "        continue\n",
    "    for a in js[\"annotation\"]:\n",
    "        print u\"{0}\\t{1}\".format(a[\"surface_form\"],a[\"url\"].replace(\" \",\"_\")).encode('utf-8')\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Run  [WikiExtractor](https://github.com/attardi/wikiextractor)\n",
    "In case you face a problem with the latest one, try the version we are keeping in this directory\n",
    "\n",
    "`mkdir ../../../data/enwiki20160305/texts`\n",
    "\n",
    "`nohup ./WikiExtractor.py -o ../../../data/enwiki20160305/texts/xmltexts -b 1000M  -l  --no-templates ../../../data/enwiki20160305/original/enwiki-20160305-pages-articles.xml.bz2 > wikiext.log 2>&1 &`\n",
    "\n",
    "### Step 5. Annotate The Output\n",
    "`time cat ../../../data/enwiki20160305/texts/xmltexts/wiki_* | python toannonated.py > ../../../data/enwiki20160305/texts/enwiki-20160305-annonated.json`\n",
    "\n",
    "### Step 6. Extract the Anchors\n",
    "\n",
    "`time cat ../../../data/enwiki20160305/texts/enwiki-20160305-annonated.json | python extractanchors.py > ../../../data/enwiki20160305/texts/enwiki-20160305-anchor-titles-from-text`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting anchors with titles\n",
    "There are some titles with no anchors, such as:\n",
    "\n",
    "* Meiluawati\n",
    "\n",
    "* ZdenÄ›k_Svoboda\n",
    "\n",
    "### Step 7. Run the following cell to create `titles2anchors.py`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile titles2anchors.py\n",
    "''' Add the titles to anchor list\n",
    "'''\n",
    "import MySQLdb\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "_db = MySQLdb.connect(host=\"127.0.0.1\",port=3307,user='root',passwd=\"emilios\",db=\"enwiki20160305\")\n",
    "_cursor = _db.cursor()\n",
    "_cursor.execute(\"select page_title from page where page_namespace=0\");\n",
    "rows = _cursor.fetchall();\n",
    "for row in rows:\n",
    "    title=row[0]\n",
    "    surface_form = title;\n",
    "    i=surface_form.find('(')\n",
    "    if i!=-1:\n",
    "        surface_form = surface_form[:i]\n",
    "    surface_form = surface_form.replace('_', ' ')\n",
    "    surface_form = surface_form.strip()\n",
    "    \n",
    "    print surface_form+\"\\t\"+title\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Adding Titles to Anchors\n",
    "\n",
    "`time python titles2anchors.py > ../../../data/enwiki20160305/texts/enwiki-20160305-anchor-from-titles.tsv`\n",
    "\n",
    "`cp ../../../data/enwiki20160305/texts/enwiki-20160305-anchor-titles-from-text.tsv  ../../../data/enwiki20160305/texts/enwiki-20160305-anchor-titles.tmp.tsv`\n",
    "\n",
    "`cat ../../../data/enwiki20160305/texts/enwiki-20160305-anchor-from-titles.tsv >> ../../../data/enwiki20160305/texts/enwiki-20160305-anchor-titles.tmp.tsv`\n",
    "\n",
    "\n",
    "### Step 9 . Prepare the anchor-titles file\n",
    "`time cat ../../../data/enwiki20160305/texts/enwiki-20160305-anchor-titles.tmp.tsv | sort| uniq -c | sed 's/^ *//;s/ /\\t/' > ../../../data/enwiki20160305/texts/enwiki-20160305-anchor-titles.tsv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving redirects \n",
    "We want each anchor to map to a non-redirect page\n",
    "\n",
    "### Step 10. Run the following cell to create `resolveredirs.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile resolveredirs.py\n",
    "'''Incase an anchor is pointing to a redirect page, it replaces the target to the final destination\n",
    "'''\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../cgi-bin/')\n",
    "from collections import defaultdict\n",
    "import imp\n",
    "import json\n",
    "import urllib\n",
    "from wikipedia import *\n",
    "import StringIO\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "fileinput = sys.stdin\n",
    "errout = sys.stderr\n",
    "#fileinput = StringIO.StringIO(inputstr)\n",
    "pairdict = defaultdict(int)\n",
    "while True:\n",
    "    line = fileinput.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    line = line.strip()\n",
    "    tp = line.split(\"\\t\")\n",
    "    if len(tp) !=3:\n",
    "        errout.write(\"Line Error: \" + line)\n",
    "        continue\n",
    "    freq,anchor,title=tp\n",
    "    wid=title2id(title)\n",
    "    if wid is None:\n",
    "        title=title[0].upper()+title[1:]\n",
    "    wid=title2id(title)\n",
    "    if wid is None:\n",
    "        errout.write(anchor+\"\\t\"+title+\"\\t\"+str(freq)+\"\\n\")\n",
    "        continue\n",
    "    pairdict[(anchor,wid)] += int(freq)\n",
    "for (anchor,wid),freq in pairdict.iteritems():\n",
    "    print anchor+\"\\t\"+str(wid)+\"\\t\"+str(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11. Applying the resolver\n",
    "\n",
    "`time cat ../../../data/enwiki20160305/texts/enwiki-20160305-anchor-titles.tsv | python resolveredirs.py > ../../../data/enwiki20160305/texts/enwiki-20160305-anchor-ids.tsv 2> ../../../data/enwiki20160305/texts/enwiki-20160305-anchor-ids.dead.tsv`\n",
    "\n",
    "### Step 12. Importing to mysql\n",
    "`cd mysqltables`\n",
    "\n",
    "`bash importall ../../../../data/enwiki20160305/texts 20160305 root emilios`\n",
    "\n",
    "`cd ..`\n",
    "\n",
    "### Step 13. Extracting anchors lists\n",
    "`cat ../../../data/enwiki20160305/texts/enwiki-20160305-anchor-ids.tsv| cut -f1 -d$'\\t'| sort | uniq > ../../../data/enwiki20160305/texts/enwiki-20160305-anchors.tsv`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToSolr v2. converts the annotations to a jsonstring\n",
    "The following script converts our annotated Wikipedia to something Solr can accept\n",
    "\n",
    "It simply encode the json objects containing the annotations to a string, to stop solr from importing them\n",
    "\n",
    "### Step 14. Run the following cell to create `tosolr.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing solrsettings/tosolr.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile solrsettings/tosolr.py\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import StringIO\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "\n",
    "fileinput = sys.stdin\n",
    "\n",
    "#fileinput = StringIO.StringIO(inputstr)\n",
    "\n",
    "n=1000\n",
    "count=0;\n",
    "f=None\n",
    "os.mkdir(\"chunks\")\n",
    "seperator=''\n",
    "firstline=None\n",
    "while True:\n",
    "    line = fileinput.readline()\n",
    "    if not line:\n",
    "        break\n",
    "        \n",
    "    line = line.decode('utf-8').strip()\n",
    "    if not line:\n",
    "        continue\n",
    "        \n",
    "    if count%n==0:\n",
    "        if f is not None:\n",
    "            f.write('\\n]\\n')\n",
    "            f.close()\n",
    "        f=open('chunks/chunk'+str(int(count/n)).zfill(10)+'.json','w')    \n",
    "        f.write('[\\n')\n",
    "        firstline=True\n",
    "        seperator=\"\"\n",
    "    count +=1\n",
    "    \n",
    "    page=json.loads(line)\n",
    "    page[\"annotation\"]=json.dumps(page[\"annotation\"], ensure_ascii=False)\n",
    "    page[\"opening_annotation\"]=json.dumps(page[\"opening_annotation\"], ensure_ascii=False)\n",
    "    f.write(seperator+json.dumps(page, ensure_ascii=False).encode('utf-8'))\n",
    "    if firstline:\n",
    "        firstline=False\n",
    "        seperator=\",\\n\"\n",
    "    \n",
    "    \n",
    "if f is not None:\n",
    "    f.write('\\n]\\n')\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 15. Preparing solr importable\n",
    "`cd solrsettings`\n",
    "\n",
    "`time python tosolr.py <../../../../data/enwiki20160305/texts/enwiki-20160305-annonated.json`\n",
    "\n",
    "### Step 16. creating solr nodes\n",
    "\n",
    "`solr create -c enwiki20160305`\n",
    "\n",
    "`solr stop`\n",
    "\n",
    "`cp conf/* ~/backup/solr-6.0.0/server/solr/enwiki20160305/conf/`\n",
    "\n",
    "`solr start`\n",
    "\n",
    "### Step17. importing to solr\n",
    "\n",
    "`nohup time bash loadwiki >load.txt 2>&1 &`\n",
    "\n",
    "`cd ..` \n",
    "\n",
    "**Check the count**\n",
    "\n",
    "curl 'http://localhost:8983/solr/enwiki20160305/select?indent=on&q=*:*&rows=0&wt=json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [SolrTextTagger](https://github.com/OpenSextant/SolrTextTagger) preparation\n",
    "\n",
    "### Step 18. Prepare SolrTextTagger\n",
    "\n",
    "* `bin/solr create -c enwikianchors20160305`\n",
    "\n",
    "* `bin/solr stop`\n",
    "\n",
    "* `cp -r ~/backup/projects/wikisim/preparation_script/text/solrtagger/lib/ server/solr/`\n",
    "\n",
    "* `cp ~/backup/projects/wikifier/preparation_script/solrtagger/conf/* server/solr/enwikianchors20160305/conf/`\n",
    "\n",
    "* `curl -X POST --data-binary @enwiki-20160305-anchors.tsv.txt -H 'Content-type:application/csv' \\\n",
    "  'http://localhost:8983/solr/enwikianchors20160305/update?commit=true&optimize=true&separator=%09&encapsulator=%00&fieldnames=name'`\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample code to check Random Access to paragraphs with links\n",
    "### You can safely ignore this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import requests\n",
    "import json \n",
    "qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "q='title:Bruce_Lee'\n",
    "params={'indent':'on', 'wt':'json', 'q':q}\n",
    "r = requests.get(qstr, params=params)\n",
    "r =  r.json()['response']['docs'][0]\n",
    "\n",
    "pos=[m.start() for m in re.finditer('\\n', r['text'])]    \n",
    "n = index = random.randint(0,len(pos)-1)\n",
    "s = pos[n]\n",
    "e = pos[n+1] if n < len(pos)-1 else len(r['text'])\n",
    "par = r['text'][s:e]\n",
    "print r['title']\n",
    "print \"par no:\",n, par[:10],\"...\", par[len(par)-10:]\n",
    "for ann in json.loads(r['annotation']):\n",
    "    #print ann\n",
    "    if int(ann['from']) < s: \n",
    "        continue\n",
    "    if int(ann['to']) >= e: \n",
    "        break\n",
    "    surfintext = r['text'][ann['from']:ann['to']]\n",
    "    print \"detected: %s, in annotation: %s, title: %s\" % (surfintext, ann[u'surface_form'], ann['url'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the context core\n",
    "\n",
    "We will create a solr core that indexes the context (a window of size 20) for any anchor text  \n",
    "\n",
    "We will use this core for disambiguation. The implementation is multi-threaded to make the process as fast as possible.\n",
    "\n",
    "# Step 19. Run the following script to create `entitycontext_th.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing entitycontext_th.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile entitycontext_th.py \n",
    "\n",
    "''' Generate a json file (contexts.json) where eahc line is an anchor text \n",
    "    with its left and right context\n",
    "'''\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import string\n",
    "import time\n",
    "import sys\n",
    "from multiprocessing import Pool, Process, Manager \n",
    "import functools\n",
    "sys.path.insert(0,'../..')\n",
    "\n",
    "from memapi.memwiki import *\n",
    "\n",
    "startTime = time.time()\n",
    "load_tables()\n",
    "print 'wiki loaded to memory'    \n",
    "print time.time()-startTime\n",
    "sys.stdout.flush()\n",
    "\n",
    "batch_rows=30000\n",
    "init_start=0\n",
    "process_no=30\n",
    "max_rows=-1\n",
    "window_size=10\n",
    "\n",
    "qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "q='*:*'\n",
    "#q='id:\"60508\"'\n",
    "\n",
    "params={'indent':'on', 'wt':'json', 'q':q, \"start\":init_start,\n",
    "        \"rows\":batch_rows}\n",
    "home = os.path.expanduser(\"~\");\n",
    "#titles_out.write('kalam\\n')\n",
    "\n",
    "start=0\n",
    "max_count=0\n",
    "count_nottop=0\n",
    "\n",
    "def find_nth(text, c, i, n, direction=0):\n",
    "    if direction ==0:\n",
    "        offset = text.find(c, i+1)\n",
    "    else:    \n",
    "        offset = text.rfind(c, 0,i)\n",
    "    while offset >= 0 and n > 1:\n",
    "        if direction ==0:\n",
    "            offset = text.find(c, offset+len(c))\n",
    "        else:    \n",
    "            offset = text.rfind(c, 0, offset)\n",
    "        n -= 1\n",
    "    return offset\n",
    "\n",
    "        \n",
    "def getsentence(r, ann):\n",
    "    text = r[\"text\"]\n",
    "    source_entity=r['id']\n",
    "    source_title=r['title']\n",
    "    \n",
    "    entity = ann['url'] if ann['url']!=\"\" else r['title']\n",
    "    entityid = title2id(entity)\n",
    "    if entityid is None:\n",
    "        entity=entity[0].upper()+entity[1:]    \n",
    "        entityid = title2id(entity)\n",
    "    if entityid is None:\n",
    "        return None\n",
    "    trans_tab = dict.fromkeys(map(ord, string.punctuation), None)\n",
    "    \n",
    "    s = find_nth(text,' ', ann['from'],2*window_size,1)\n",
    "\n",
    "    if s==-1:\n",
    "        s=0\n",
    "    l = text[s:ann['from']].translate(trans_tab).strip().split()\n",
    "    l=\" \".join(l[-window_size:])\n",
    "\n",
    "    s = find_nth(text,' ', ann['from'],2*window_size,0)\n",
    "    if s==-1:\n",
    "        s=len(text)\n",
    "    r = text[ann['to']:].translate(trans_tab).strip().split()\n",
    "    r=\" \".join(r[:window_size])\n",
    "    pno = text[:ann['from']].count('\\n')\n",
    "    return {\"left\":l, \"anchor\":ann['surface_form'],\"source_entity\":source_entity,\"source_title\":source_title,\n",
    "            \"entity\": entity, \"entityid\":entityid, \"right\": r, \"paragraph_no\":pno}\n",
    "\n",
    "\n",
    "def process(r, tq, cq):\n",
    "    tq.put(r['title'].encode('utf-8'))\n",
    "    annotations = json.loads(r['annotation'])    \n",
    "    for ann in annotations:\n",
    "        sentence = getsentence(r, ann)\n",
    "        if sentence is None:\n",
    "            continue\n",
    "        entity_context = json.dumps(sentence, ensure_ascii=False).encode('utf-8')\n",
    "        cq.put(entity_context)\n",
    "        \n",
    "pool =Pool(process_no) \n",
    "\n",
    "def worker(fname, q):\n",
    "    w = open(fname,'w')\n",
    "    print \"Writer started\"\n",
    "    sys.stdout.flush()\n",
    "    while True:\n",
    "        s = q.get()\n",
    "        if s=='kill':\n",
    "            print \"Writer worker closing\"\n",
    "            sys.stdout.flush()\n",
    "            break\n",
    "        w.write(s+\"\\n\")\n",
    "    w.close()    \n",
    "    \n",
    "manager= Manager()\n",
    "\n",
    "titles_name = os.path.join(home,'backup/data/enwiki20160305/cmod/titles.txt')\n",
    "title_q = manager.Queue()\n",
    "\n",
    "cont_name = os.path.join(home,'backup/data/enwiki20160305/cmod/contexts.json')\n",
    "cont_q = manager.Queue()\n",
    "\n",
    "\n",
    "title_proc = Process(target=worker, args=(titles_name, title_q))\n",
    "title_proc.start()   \n",
    "        \n",
    "cont_proc = Process(target=worker, args=(cont_name, cont_q))\n",
    "cont_proc.start()   \n",
    "\n",
    "\n",
    "start=init_start\n",
    "rows=0\n",
    "while True:\n",
    "    params[\"start\"] = start\n",
    "\n",
    "    r = requests.get(qstr, params=params)\n",
    "    print (\"A batch retrieved: \"+ str(start)+'\\n')\n",
    "    sys.stdout.flush()\n",
    "    if len(r.json()['response']['docs'])==0:\n",
    "        break\n",
    "    #print r.json()['response']['docs']\n",
    "    D = r.json()['response']['docs']\n",
    "        \n",
    "    pool.map(functools.partial(process, tq=title_q, cq=cont_q ), D)\n",
    "    #pool.close() \n",
    "    #pool.join() \n",
    "    \n",
    "    start += batch_rows\n",
    "    rows += batch_rows\n",
    "    if max_rows !=-1 and rows >= max_rows:\n",
    "        break\n",
    "        \n",
    "cont_q.put('kill')    \n",
    "title_q.put('kill')\n",
    "\n",
    "title_proc.join()\n",
    "cont_proc.join()\n",
    "\n",
    "print 'done'    \n",
    "\n",
    "print time.time()-startTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 20. Run the script to create `contexts.json`\n",
    "\n",
    "`mkdir ../../../data/enwiki20160305/cmod`\n",
    "\n",
    "`python entitycontext_th.py`\n",
    "\n",
    "\n",
    "\n",
    "### Step 21. Split the file\n",
    "`cd solrcontexts`\n",
    "\n",
    "`mkdir chunks`\n",
    "\n",
    "`cd chunks`\n",
    "\n",
    "`cat ../../../../../data/enwiki20160305/cmod/contexts.json | split -a 10 -l 500 - basename`\n",
    "\n",
    "`cd ..`\n",
    "\n",
    "### Step 22. Preparing solr importable\n",
    "`bash tosolr.sh`\n",
    "\n",
    "### Step 23. creating solr nodes\n",
    "\n",
    "\n",
    "`solr create -c enwiki20160305_context`\n",
    "\n",
    "`solr stop`\n",
    "\n",
    "`cp conf/* ../../../wikipedia/solr-6.0.0/server/solr/enwiki20160305_context/conf/`\n",
    "\n",
    "`solr start`\n",
    "\n",
    "\n",
    "### Step 24. importing to solr\n",
    "`nohup time bash loadwiki >load.txt 2>&1 &`\n",
    "\n",
    "`cd ..\n",
    "\n",
    "**Check the count**\n",
    "\n",
    "curl 'http://localhost:8983/solr/enwiki20160305_context/select?indent=on&q=*:*&rows=0&wt=json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
