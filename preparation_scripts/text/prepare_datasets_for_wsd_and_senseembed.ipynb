{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing wsd datasets\n",
    "## It converts the baselines to the json format \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import MySQLdb\n",
    "import json\n",
    "# _db = MySQLdb.connect(host=\"127.0.0.1\",port=3307,user='root',passwd=\"emilios\",db=\"enwiki20160305\")\n",
    "# _cursor = _db.cursor()\n",
    "# _cursor.execute(\"\"\"SELECT * FROM `anchors` group\"\"\", (wid,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing wikimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import urllib\n",
    "import sys\n",
    "import requests\n",
    "import MySQLdb\n",
    "\n",
    "from HTMLParser import HTMLParser\n",
    "sys.path.insert(0,'../..')\n",
    "from wikisim.wikipedia import *\n",
    "\n",
    "def qualify(r, min_mention=2, min_abmig=2):\n",
    "    \n",
    "    # condition 1\n",
    "    if len(r['opening_annotation']) < min_mention:\n",
    "        #print \"c1 failed\"\n",
    "        return None\n",
    "    \n",
    "    nottop = False\n",
    "    found_min_ambig=False\n",
    "    annotations = json.loads(r['opening_annotation'])\n",
    "    for ann in annotations:\n",
    "        surf = ann['surface_form']\n",
    "        linktitle = ann['url']\n",
    "        wid = title2id(linktitle.encode('utf-8').replace(' ','_'))\n",
    "        # condition 2\n",
    "        if wid is None:\n",
    "            #print \"c2 failed\", linktitle\n",
    "            return None\n",
    "        \n",
    "        allids = anchor2concept(surf)\n",
    "        # condition 3\n",
    "        if not allids:\n",
    "            #print \"c3 failed\"\n",
    "            return None\n",
    "        # condition 4\n",
    "        if len(allids) >= min_abmig:\n",
    "            found_min_ambig = True\n",
    "        allids = sorted(allids, key=lambda k:-k[1])\n",
    "        \n",
    "        # condition 5\n",
    "        #print allids\n",
    "        if allids[0][0] != wid:\n",
    "            nottop = True\n",
    "    if not found_min_ambig:\n",
    "        #print \"c4 failed\"\n",
    "        return None\n",
    "    return nottop                                                                                   \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test qualify\n",
    "#Conference\n",
    "#Oladapo\n",
    "#r={'opening_annotation':[{'url':'Bruce_Lee', 'surface_form': 'Bruce Lee'}, {'url':'Sanandaj', 'surface_form': 'Sanandaj'}]}\n",
    "#r={'opening_annotation':[{'url':'Oladapo', 'surface_form': 'Oladapo'}, {'url':'Do_while_loop', 'surface_form': 'do while loop'}]}\n",
    "#r={'opening_annotation':[{'url':'David_Beckham', 'surface_form': 'David'}, {'url':'Do_while_loop', 'surface_form': 'do while loop'}]}\n",
    "#print qualify(r)\n",
    "#print len(anchor2concept('Bruce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  2520 , count_nottop:  1000\n",
      "count:  3600\n",
      "count:  4655 , count_nottop:  2000\n",
      "count:  6837 , count_nottop:  3000\n",
      "count:  7200\n",
      "count:  9041 , count_nottop:  4000\n",
      "count:  10800 , count_nottop:  4747\n",
      "count:  11389 , count_nottop:  5000\n",
      "count:  13353 , count_nottop:  6000\n",
      "count:  14400\n",
      "count:  15253 , count_nottop:  7000\n",
      "count:  17504 , count_nottop:  8000\n",
      "count:  18000\n",
      "count:  19727 , count_nottop:  9000\n",
      "count:  21600 , count_nottop:  9773\n",
      "count:  22188 , count_nottop:  10000\n",
      "count:  25200\n",
      "count:  28800\n",
      "count:  32400\n",
      "count:  36000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "wanted=36000\n",
    "wanted_nottop=10000\n",
    "\n",
    "reopen()\n",
    "qstr = 'http://localhost:8983/solr/enwiki20160305/select'\n",
    "q='*:*'\n",
    "rows=1000\n",
    "params={'indent':'on', 'wt':'json', 'fl':'title opening_text opening_annotation', 'q':q, \"start\":0,\n",
    "        \"rows\":rows}\n",
    "home = os.path.expanduser(\"~\");\n",
    "output = open(os.path.join(home,'backup/datasets/ner/wiki.%s.json'%(wanted, )), 'w')\n",
    "\n",
    "start=0\n",
    "count=0\n",
    "count_nottop=0\n",
    "enough=False\n",
    "while True:\n",
    "    params[\"start\"] = start\n",
    "    start += rows\n",
    "    r = requests.get(qstr, params=params)\n",
    "    #print r.json()['response']['docs']\n",
    "    for d in r.json()['response']['docs']:\n",
    "        #print d[\"id\"]; enough=True; break\n",
    "        if not (count < wanted or count_nottop < wanted_nottop):\n",
    "            enough = True\n",
    "            break\n",
    "        qd = qualify(d) \n",
    "        if qd is None:\n",
    "            continue\n",
    "            \n",
    "        if count_nottop < wanted_nottop and qd :\n",
    "            output.write(json.dumps(d, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "            count_nottop += 1\n",
    "            count += 1\n",
    "            if ((count) % int(wanted/10) ==0) or (count_nottop) % int(wanted_nottop/10) ==0:\n",
    "                print \"count: \", count, \", count_nottop: \", count_nottop\n",
    "            continue\n",
    "        if count < wanted:\n",
    "            count +=1\n",
    "            output.write(json.dumps(d, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "            if (count) % int(wanted/10) ==0:   \n",
    "                print \"count: \", count\n",
    "            \n",
    "    #break\n",
    "    if enough:\n",
    "        break\n",
    "print 'done'        \n",
    "output.close()    \n",
    "#r=json.loads(r['opening_annotation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting json to input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import sys, os, json\n",
    "import string\n",
    "home = os.path.expanduser(\"~\");\n",
    "wanted=36000\n",
    "jsname = os.path.join(home,'backup/datasets/ner/wiki.%s.json'%(wanted,))\n",
    "outjsname = os.path.join(home,'backup/datasets/ner/wiki-mentions.%s.json'%(wanted,))\n",
    "\n",
    "\n",
    "    \n",
    "def splittext(text,url):\n",
    "    start=0\n",
    "    termindex=0\n",
    "    t=[]\n",
    "    mentions=[]\n",
    "    # pass 1, adjust partial mentions. \n",
    "    # approach one, expand (the other could be shrink)\n",
    "    \n",
    "    for u in url:\n",
    "        seg = text[start:u['from']]\n",
    "        t += seg.strip().split()\n",
    "        mentions.append([len(t),u['url']])\n",
    "        t+=[\" \".join(text[u['from']:u['to']].split())]\n",
    "        start = u['to']\n",
    "        \n",
    "    t += text[start:].strip().split()\n",
    "    return t, mentions\n",
    "\n",
    "with open(jsname) as jf, open(outjsname,'w') as outjs:\n",
    "    for line in jf:\n",
    "        line = line.strip().decode('utf-8')\n",
    "        js = json.loads(line)\n",
    "        text = js['opening_text'] \n",
    "        url = json.loads(js['opening_annotation'])\n",
    "        t, m = splittext(text, url)\n",
    "        outjs.write((json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False)+'\\n').encode('utf-8'))\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing KORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import sys, os, json\n",
    "import string\n",
    "from HTMLParser import HTMLParser\n",
    "hp = HTMLParser()\n",
    "home = os.path.expanduser(\"~\");\n",
    "\n",
    "sys.path.append('../../cgi-bin/')\n",
    "%aimport wikipedia\n",
    "from wikipedia import *\n",
    "reopen()\n",
    "\n",
    "\n",
    "infname = os.path.join(home,'backup/datasets/ner/KORE50/AIDA.tsv')\n",
    "outjsname = os.path.join(home,'backup/datasets/ner/kore.json')\n",
    "url_col = 3\n",
    "\n",
    "t=[]\n",
    "m=[]\n",
    "with open(infname) as inf, open(outjsname,'w') as outjs:\n",
    "    state = 0\n",
    "    for line in inf:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            line = line.decode('unicode-escape')\n",
    "        except:\n",
    "            pass            \n",
    "        if line.startswith('-DOCSTART-'):\n",
    "            if m and t:\n",
    "                outjs.write (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "            t = []\n",
    "            m = []\n",
    "            continue\n",
    "                \n",
    "        w = line.split('\\t')\n",
    "        if len(w) == 1:\n",
    "            t.append(w[0])\n",
    "            continue\n",
    "            \n",
    "        if w[1] == 'B':\n",
    "            if title2id(w[3]) is None:\n",
    "                print w[3], \" not found\"\n",
    "            if w[url_col] != '--NME--' and (title2id(w[3]) is not None):\n",
    "                m.append((len(t), w[3]))\n",
    "            t.append(w[2])\n",
    "            \n",
    "        if w[1] == 'I':\n",
    "            continue\n",
    "    if m and t:\n",
    "        outjs.write (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "        \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"Rudi_V\\u00f6ller\"\n",
    "print x.decode('unicode-escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Aida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "\n",
    "import sys, os, json\n",
    "import string\n",
    "import re\n",
    "\n",
    "from HTMLParser import HTMLParser\n",
    "hp = HTMLParser()\n",
    "home = os.path.expanduser(\"~\");\n",
    "\n",
    "sys.path.append('../../cgi-bin/')\n",
    "%aimport wikipedia\n",
    "from wikipedia import *\n",
    "reopen()\n",
    "\n",
    "pattern = re.compile(\"http://en.wikipedia.org/wiki/(.*)\")\n",
    "\n",
    "\n",
    "infname = os.path.join(home,'backup/datasets/ner/Aida01/aida-yago2-dataset/AIDA-YAGO2-annotations.tsv')\n",
    "#infname = os.path.join(home,'backup/datasets/ner/aida-shalam.tsv')\n",
    "outjsname = os.path.join(home,'backup/datasets/ner/aida.json')\n",
    "#outjsname = os.path.join(home,'backup/datasets/ner/aida-shalam.json')\n",
    "# set it to 3 for AIDA, \n",
    "url_col = 4\n",
    "pattern = re.compile(\"http://en.wikipedia.org/wiki/(.*)\")\n",
    "\n",
    "t=[]\n",
    "m=[]\n",
    "with open(infname) as inf, open(outjsname,'w') as outjs:\n",
    "    state = 0\n",
    "    for line in inf:\n",
    "        line = line.strip().decode('utf-8')\n",
    "        #line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith('-DOCSTART-'):\n",
    "            if m and t:\n",
    "                outjs.write (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "                #print (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "            t = []\n",
    "            m = []\n",
    "            continue\n",
    "                \n",
    "        w = line.split('\\t')\n",
    "            \n",
    "        if w[1] == '--NME--':\n",
    "            continue\n",
    "        title = w[2]\n",
    "        r = re.match(pattern, title)\n",
    "        title = r.group(1)\n",
    "        rwd = title2id(title)\n",
    "            \n",
    "        rwd = title2id(title)\n",
    "            \n",
    "        anchors = id2anchor(rwd)\n",
    "        if not anchors:\n",
    "            print \"no anchor for: \", title\n",
    "            continue\n",
    "                            \n",
    "        c=max(anchors, key=lambda k:k[1])[0]    \n",
    "        m.append((len(t), title))\n",
    "        t.append(c.decode('utf-8'))\n",
    "            \n",
    "    if m and t:\n",
    "        outjs.write (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "        #print (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "                \n",
    "print \"done\"        \n",
    "        \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing msnbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from os import listdir\n",
    "from os.path import isfile, join, expanduser\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('../../wikisim/')\n",
    "from wikipedia import *\n",
    "home = expanduser(\"~\");\n",
    "mypath = join(home, 'backup/datasets/WikificationACL2011Data/AQUAINT/Problems/')\n",
    "outjsname = join(home,'backup/datasets/ner/aquaint.json')\n",
    "outjs=open(outjsname,'w')\n",
    "dsnames = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "for ds in dsnames:\n",
    "    \n",
    "    with open(join(mypath, ds)) as f:\n",
    "        filestr=f.read()\n",
    "    #print filestr\n",
    "    filestr=filestr.decode(encoding='cp1252').replace('&','&amp;').encode(encoding='utf-8')\n",
    "    root=ET.fromstring(filestr)\n",
    "    \n",
    "    \n",
    "    t=[]\n",
    "    m=[]\n",
    "    i=0\n",
    "    for child in root.findall('ReferenceInstance'):\n",
    "        anchor=child.find('SurfaceForm').text.strip()\n",
    "        url = child.find('ChosenAnnotation').text.strip()\n",
    "        if url == '*null*':\n",
    "            continue\n",
    "        if url[:29] !='http://en.wikipedia.org/wiki/':\n",
    "            print url[:29]\n",
    "            raise Exception('bad format')\n",
    "        url=url[29:]\n",
    "        if title2id(url) is None:\n",
    "            print ds,':',url, \" not found\"\n",
    "            continue\n",
    "        \n",
    "        t.append(anchor)\n",
    "        m.append([i,url])\n",
    "        i +=1\n",
    "        \n",
    "    outjs.write (json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False).encode('utf-8')+'\\n')\n",
    "outjs.close()\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing msnbc/Aquaint with Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from os import listdir\n",
    "from os.path import isfile, join, expanduser\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('../../wikisim/')\n",
    "from wikipedia import *\n",
    "home = expanduser(\"~\");\n",
    "\n",
    "ds_dir = 'AQUAINT'\n",
    "#ds_dir = 'MSNBC'\n",
    "p_path = join(home, 'backup/datasets/WikificationACL2011Data', ds_dir )\n",
    "\n",
    "out_jsname = join(home,'backup/datasets/ner/',ds_dir+'.txt.json')\n",
    "\n",
    "text_path = join(p_path,'RawTexts' )\n",
    "problem_path = join(p_path,'Problems' )\n",
    "\n",
    "outjs=open(out_jsname,'w')\n",
    "def replace_bad_chars(s):\n",
    "    return s.replace(\"\\x85\", '.').replace(\"\\x91\",\"'\").replace(\"\\x92\",\"'\").replace(\"\\x93\",'\"')\\\n",
    "            .replace(\"\\x94\", '\"').replace(\"\\x96\", '-').replace(\"\\x97\", '-').replace(\"\\xfc\",'x')\n",
    "\n",
    "dsnames = [f for f in listdir(problem_path) if isfile(join(problem_path, f)) and (re.match(r'.*\\.htm$', f) or re.match(r'.*\\.txt$', f))]\n",
    "def splittext(text,url):\n",
    "    start=0\n",
    "    termindex=0\n",
    "    t=[]\n",
    "    mentions=[]\n",
    "    # pass 1, adjust partial mentions. \n",
    "    # approach one, expand (the other could be shrink)\n",
    "    \n",
    "    for u in url:\n",
    "        seg = text[start:u['from']]\n",
    "        t += seg.strip().split()\n",
    "        mentions.append([len(t),u['url']])\n",
    "        t+=[\" \".join(text[u['from']:u['to']].split())]\n",
    "        start = u['to']\n",
    "        \n",
    "    t += text[start:].strip().split()\n",
    "    return t, mentions\n",
    "\n",
    "def get_annotation(filename):\n",
    "    with open(join(problem_path, filename)) as f:\n",
    "        annotstr=f.read()\n",
    "    #print filestr\n",
    "    #annotstr=annotstr.decode(encoding='cp1252').replace('&','&amp;').encode(encoding='utf-8')\n",
    "    \n",
    "    annotstr=replace_bad_chars(annotstr).replace('&','&amp;').strip()\n",
    "    root=ET.fromstring(annotstr)\n",
    "    \n",
    "    \n",
    "    t=[]\n",
    "    for child in root.findall('ReferenceInstance'):\n",
    "        anchor=child.find('SurfaceForm').text.strip()\n",
    "        url = child.find('ChosenAnnotation').text.strip()\n",
    "        if url == '*null*':\n",
    "            continue\n",
    "        if url[:29] !='http://en.wikipedia.org/wiki/':\n",
    "            print url[:29]\n",
    "            raise Exception('bad format')\n",
    "        url=url[29:]\n",
    "        if title2id(url) is None:\n",
    "            #print ds,':',url, \" not found\"\n",
    "            continue\n",
    "        ufrom = int(child.find('Offset').text.strip()) \n",
    "        uto = ufrom + int(child.find('Length').text.strip())\n",
    "        t.append({'from':ufrom, 'to':uto, \n",
    "                'url':url, 'anchor': anchor})\n",
    "    return t\n",
    "\n",
    "for ds in dsnames:\n",
    "    print ds\n",
    "    url = get_annotation(ds)\n",
    "    with open(join(text_path, ds)) as f:\n",
    "        text=f.read()\n",
    "    text=replace_bad_chars(text).decode('utf-8')\n",
    "    \n",
    "    #break\n",
    "    t, m = splittext(text, url)\n",
    "    outjs.write((json.dumps({\"text\":t, \"mentions\":m}, ensure_ascii=False)+'\\n').encode('utf-8'))\n",
    "    #print filestr\n",
    "\n",
    "        \n",
    "outjs.close()\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open ('/home/sajadi/backup/datasets/ner/MSNBC.txt.json') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.decode('utf-8')\n",
    "        jline = json.loads(line)\n",
    "        for ann in jline['mentions']:\n",
    "            print jline['text'][ann[0]],'-->', ann[1]\n",
    "        count +=1\n",
    "        if count >= 10:\n",
    "            break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
