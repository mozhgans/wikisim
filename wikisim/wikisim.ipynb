{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Semantic Relatedness using Wikipedia\n",
    "\n",
    "* **Armin Sajadi** - Faculty of Computer Science\n",
    "* **Dr. Evangelos Milios** - Faculty of Computer Science\n",
    "* **Dr. Vlado Kešelj** – Faculty of Computer Science\n",
    "\n",
    "This is a simple and step by step explanation of calculating semantic relatedness using Wikipedia. We start by preprocessing and building the api, that is explained in the following papers papers:\n",
    "\n",
    "* Armin Sajadi, Evangelos E. Milios, Vlado Keselj, \"Vector Space Representation of Wikipedia Concepts\", Submitted to NLDB 2017\n",
    "\n",
    "\n",
    "### Public Resources\n",
    "* Weservice: (http://web.cs.dal.ca/~sajadi/wikisim/)\n",
    "* Source Code: (https://github.com/asajadi/wikisim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Here First\n",
    "### If you want to use our pre-pared datasets [Recommended] \n",
    "[Using Prepared Tables](#Using-Prepared-Tables)\n",
    "\n",
    "### If you want to experience another Wikipedia Database dump\n",
    "[Start From Preprocessing](#Preprocessing)\n",
    "\n",
    "# Table of Context\n",
    "\n",
    "**[Preparing The New Wiki Dump](#Preparing-The-New-Wiki-Dump)**\n",
    "\n",
    "**[Wikipedia Interface](#Wikipedia-Interface)**\n",
    "\n",
    "**[Fast Pagerank Implementation](#Fast-Pagerank-Implementation)**\n",
    "\n",
    "**[Calculating Semantic Relatedness](#Calculating-Semantic-Relatedness)**\n",
    "\n",
    "**[A Simple Example](#A-Simple-Example)**\n",
    "\n",
    "**[Calculating All The Embeddngs](#Calculating-All-The-Embeddngs)**\n",
    "\n",
    "**[Visualizing The Embeddings](#Visualizing-The-Embeddings)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing The New Wiki Dump\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "**Note: You can skip the step by step processing section completely by running `bash preprocess`**. \n",
    "\n",
    "** This is intended to guide you in case on of the steps goes wrong!**\n",
    "\n",
    "\n",
    "The first step is to download the wikipedia database dumps and import them to mysql. We do a preprocessing on the sql dumps for mainly three reasons:\n",
    "\n",
    "* The tables are huge, containing many column and rows we do not use. Removing the unnessary information, that includes unused columns (such as time stamps, viewed count of the pages or categories) and all the information about talk pages, media files or user draft pages, can dramatically decreas the size of the tables.\n",
    "\n",
    "* Forming **synonym Rings**. We extend the concept of synonym ring to Wikipedia (similar to what is called synset in Wordnet). In Wikipedia, redirection stands for equivallency, for example Car --> Automobile. But it's not always this easy and you can find all sorts of weired redirection, like:\n",
    "\n",
    "![](../resrc/sr.jpg)\n",
    "\n",
    "   We iterate through redirectins and remove cycles, dangling redirections and also all the chains. This process forms clusters of redirections around main pages. Then we go through all other tables (pagelinks and  category links) and replace any redirected page by its main article, the result would be much more neated, and makes the rest of the process faster.\n",
    "\n",
    "\n",
    "* We remove garbage, links to non existing pages, self links, mismatching namespaces, and many other incosistencies that you can find the details in the source code).\n",
    "\n",
    "* We apply some strategic changes, like instead of source id --> destination title format of the pagelinks, we use source id --> dest id, which is faster and preferrabel for out case. \n",
    "\n",
    "To complete this step, download and run the parser (written in Java) that prunes these files. You can run the following cells, but due to a known bug with ipython, you can't see bash progress messages untill the job is finished. So a better option would be simply running the scripts from bash and skipping the remaining of this section. In this case, running each cell create a script in the [preparation_scripts/] directory with the name indicated as the argument of `writefile` at the begining of the cell. If you want to run the cell directly, ** comment the first line and uncomment the second line**.\n",
    "\n",
    "## Downloading\n",
    "\n",
    "Download the following files and decompress them (we assume the path to be ~/Downloads/wikidumps):\n",
    "\n",
    "https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-page.sql.gz\n",
    "\n",
    "https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pagelinks.sql.gz \n",
    "\n",
    "https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-redirect.sql.gz\n",
    "\n",
    "https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-category.sql.gz\n",
    "\n",
    "https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-categorylinks.sql.gz\n",
    "\n",
    "### Or\n",
    "**Use the the prepared following script that download wikipedia dumps to the the default `~/Downloads/wikidumps` directory and decompress them**\n",
    "\n",
    "\n",
    "`bash download.sh`\n",
    "\n",
    "`bash decompress.sh`\n",
    "\n",
    "\n",
    "## Parsing Database dumps\n",
    "\n",
    "The following java file  does the preprosseing (parsing) wikipedia dumps and creates the processed tables (ending in `main.sql`) and several log files of the errors\n",
    "\n",
    "*Note*: you might need to recompile (`javac ProcessSQLDumps.java`) \n",
    "\n",
    "run by\n",
    "\n",
    "`java ProcessSQLDumps ~/Downloads/wikidumps`\n",
    "\n",
    "### Preparing mysql\n",
    "Running the folling cell will set some variable in mysql for maximum performance (if you have enoguh physical memory). Replace \\$1 and \\$2 with the actuall user and password of the user, or run the script as:\n",
    "\n",
    "`bash setupmysql.sh <user> <pass>`\n",
    "\n",
    "\n",
    "## Actuall Importing\n",
    "\n",
    "```mysql -u <user> -p<pass> -e 'CREATE SCHEMA `enwikilast` DEFAULT CHARACTER SET binary;'```\n",
    "\n",
    "`./importall  ~/Downloads/wikidumps last <user> <pass>`\n",
    "\n",
    "This might take several hours \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Prepared Tables\n",
    "## Download\n",
    "Download the following files and decompress them to a dir (we assume the path to be ~/Downloads/wikidumps)\n",
    "\n",
    "[](cgm6.research.cs.dal.ca/~sajadi/wikisim/downloads/enwiki-20160305-page.main.tsv.gz)\n",
    "\n",
    "cgm6.research.cs.dal.ca/~sajadi/wikisim/downloads/enwiki-20160305-redirect.main.tsv.gz\n",
    "\n",
    "cgm6.research.cs.dal.ca/~sajadi/wikisim/downloads/enwiki-20160305-pagelinks.main.tsv.gz\n",
    "\n",
    "cgm6.research.cs.dal.ca/~sajadi/wikisim/downloads/enwiki-20160305-category.main.tsv.gz\n",
    "\n",
    "cgm6.research.cs.dal.ca/~sajadi/wikisim/downloads/enwiki-20160305-categorylinks.main.tsv.gz\n",
    "\n",
    "cgm6.research.cs.dal.ca/~sajadi/wikisim/downloads/enwiki-20160305-pagelinksorderedin.main.tsv.gz\n",
    "\n",
    "cgm6.research.cs.dal.ca/~sajadi/wikisim/downloads/enwiki-20160305-pagelinksorderedout.main.tsv.gz\n",
    "\n",
    "### Preparing mysql\n",
    "Running the folling cell will set some variable in mysql for maximum performance (if you have enoguh physical memory. replace \\$1 and \\$2 with the actuall user and password of the user, or run the script as:\n",
    "\n",
    "`bash setupmysql.sh <user> <pass>`\n",
    "\n",
    "\n",
    "## Actuall Importing\n",
    "Run:\n",
    "\n",
    "mysql -u <user> -p<pass> -e 'CREATE SCHEMA `enwiki20160305` DEFAULT CHARACTER SET binary;'\n",
    "\n",
    "./importall  ~/Downloads/wikidumps enwiki20160305 <user> <pass>\n",
    "\n",
    "This might take several hours \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Interface\n",
    "This is the main interface to Wikipedia database and provides basic functions given a pages, such as its:\n",
    "\n",
    "* id or title\n",
    "* synonym ring\n",
    "* linkage\n",
    "* in or out neighborhood. \n",
    "\n",
    "**You might need to modify, user, password and portnumbers**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wikipedia.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wikipedia.py \n",
    "\"\"\"A General Class to interact with Wiki datasets\"\"\"\n",
    "# uncomment\n",
    "\n",
    "import sys;\n",
    "import os\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import cPickle as pickle\n",
    "import MySQLdb\n",
    "\n",
    "from utils import * # uncomment\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\", \"Evangelo Milios\", \"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "\n",
    "DISABLE_CACHE=False;\n",
    "MAX_GRAPH_SIZE=1000000\n",
    "\n",
    "DIR_IN=0;\n",
    "DIR_OUT=1;\n",
    "DIR_BOTH=2;\n",
    "_db = MySQLdb.connect(host=\"127.0.0.1\",port=3307,user='amaral',passwd=\"123456\",db=\"enwiki20160305\")\n",
    "_cursor = _db.cursor()\n",
    "#WIKI_SIZE = 10216236;\n",
    "#WIKI_SIZE = 13670498; #2016\n",
    "WIKI_SIZE = 5576365; #no redirect, 2016\n",
    "def close():\n",
    "    global _db, _cursor;\n",
    "    if _cursor is not None: \n",
    "        _cursor.close();\n",
    "        _db.close();\n",
    "    _cursor=_db=None;\n",
    "def reopen():\n",
    "    global _db, _cursor;\n",
    "    if _db is None:\n",
    "        _db = MySQLdb.connect(host=\"127.0.0.1\",port=3307,user='amaral',passwd=\"123456\",db=\"enwiki20160305\")\n",
    "        _cursor = _db.cursor()\n",
    "        \n",
    "def load_table(tbname, limit=-1):\n",
    "    \"\"\" Returns a list, containing a whole table     \n",
    "    \n",
    "    Args: \n",
    "        tbname: Table Name\n",
    "    Returns: \n",
    "        The list of rows\n",
    "    \"\"\"\n",
    "    if limit!=-1:\n",
    "        q = \"\"\"SELECT * FROM `%s` limit %s\"\"\" % (tbname, limit)\n",
    "    else:\n",
    "        q = \"\"\"SELECT * FROM `%s`\"\"\" % (tbname,)\n",
    "        \n",
    "    _cursor.execute(q)\n",
    "    rows = _cursor.fetchall();\n",
    "    return rows\n",
    "    \n",
    "    \n",
    "def id2title(wid):\n",
    "    \"\"\" Returns the title for a given id\n",
    "\n",
    "    Args: \n",
    "        wid: Wikipedia id       \n",
    "    Returns: \n",
    "        The title of the page\n",
    "    \"\"\"\n",
    "    title=None;\n",
    "\n",
    "    _cursor.execute(\"\"\"SELECT * FROM `page` where page_id = %s\"\"\", (wid,))\n",
    "    row= _cursor.fetchone();\n",
    "    if row is not None:\n",
    "        title=row[2];          \n",
    "    return title;\n",
    "\n",
    "def ids2title(wids):\n",
    "    \"\"\" Returns the titles for given list of wikipedia ids \n",
    "\n",
    "    Args: \n",
    "        wids: A list of Wikipedia ids          \n",
    "    Returns: \n",
    "        The list of titles\n",
    "    \"\"\"\n",
    "\n",
    "    wid_list = [str(wid) for wid in wids] ;\n",
    "    order = ','.join(['page_id'] + wid_list) ;\n",
    "    wid_str = \",\".join(wid_list)\n",
    "    query = \"SELECT page_id, page_title FROM `page` where page_id in ({0})\" \\\n",
    "    .format(wid_str, order);\n",
    "    _cursor.execute(query);\n",
    "    rows = _cursor.fetchall();\n",
    "    rows_dict = dict(rows)\n",
    "    titles = [rows_dict[wid] for wid in wids]\n",
    "    return titles;\n",
    "\n",
    "def encode_for_db(instr):\n",
    "    if isinstance(instr, unicode):\n",
    "        instr = instr.encode('utf-8')  \n",
    "    return instr\n",
    "        \n",
    "def normalize_str(title):\n",
    "    \n",
    "    title = encode_for_db(title)\n",
    "    title = title.replace(' ','_')\n",
    "    return title\n",
    "def title2id(title):\n",
    "    \"\"\" Returns the id for a given title\n",
    "\n",
    "    Args: \n",
    "        wid: Wikipedia id          \n",
    "    Returns: \n",
    "        The title of the page\n",
    "    \"\"\"        \n",
    "    wid=None;\n",
    "    title = normalize_str(title)\n",
    "    _cursor.execute(\"\"\"SELECT * FROM `page` where page_title=%s and page_namespace=0\"\"\", (title,))\n",
    "    row= _cursor.fetchone();\n",
    "    if row is not None:\n",
    "        wid = getredir_id(row[0]) if row[3] else row[0];\n",
    "    return wid;\n",
    "\n",
    "def is_ambiguous(wid):\n",
    "    _cursor.execute(\"\"\"SELECT * FROM `categorylinks` WHERE `categorylinks`.cl_from=%s and `categorylinks`.cl_to=19204864;\"\"\", (wid,))\n",
    "    row= _cursor.fetchone();\n",
    "    return not (row is None)    \n",
    "\n",
    "def getredir_id(wid):\n",
    "    \"\"\" Returns the target of a redirected page \n",
    "\n",
    "    Args:\n",
    "        wid: wikipedia id of the page\n",
    "    Returns:\n",
    "        The id of the target page\n",
    "    \"\"\"\n",
    "    rid=None\n",
    "\n",
    "    _cursor.execute(\"\"\"select * from redirect where rd_from=%s;\"\"\", (wid,));\n",
    "    row= _cursor.fetchone();\n",
    "    if row is not None:\n",
    "        rid=row[1]\n",
    "    return rid \n",
    "\n",
    "def resolveredir(wid):\n",
    "    tid = getredir_id(wid);\n",
    "    if tid is not None:\n",
    "        wid = tid;    \n",
    "    return wid\n",
    "\n",
    "def getredir_title(wid):\n",
    "    \"\"\" Returns the target title of a redirected page \n",
    "\n",
    "    Args:\n",
    "        wid: wikipedia id of the page\n",
    "    Returns:\n",
    "        The title of the target page\n",
    "    \"\"\"\n",
    "    \n",
    "    title=None;\n",
    "    _cursor.execute(\"\"\" select page_title from redirect INNER JOIN page\n",
    "                  on redirect.rd_to = page.page_id \n",
    "                  where redirect.rd_from =%s;\"\"\", (wid));\n",
    "    row=_cursor.fetchone()\n",
    "    if row is not  None:\n",
    "        title=row[0];\n",
    "    return title;\n",
    "\n",
    "def synonymring_titles(wid):\n",
    "    \"\"\" Returns the synonim ring of a page\n",
    "\n",
    "    Example: synonymring_titles('USA')={('U.S.A', 'US', 'United_States_of_America', ...)}\n",
    "\n",
    "    Args:\n",
    "        wid: the wikipedia id\n",
    "    Returns:\n",
    "        all the titles in its synonym ring\n",
    "    \"\"\"\n",
    "    wid = resolveredir(wid)\n",
    "    _cursor.execute(\"\"\"(select page_title from page where page_id=%s) union \n",
    "                 (select page_title from redirect INNER JOIN page\n",
    "                    on redirect.rd_from = page.page_id \n",
    "                    where redirect.rd_to =%s);\"\"\", (wid,wid));\n",
    "    rows=_cursor.fetchall();\n",
    "    if rows:\n",
    "        rows = tuple(r[0] for r in rows)\n",
    "    return rows;\n",
    "\n",
    "\n",
    "def anchor2concept(anchor):\n",
    "    \"\"\" Returns the targets of an anchor text\n",
    "\n",
    "    Args:\n",
    "        anchor: anchor\n",
    "        \n",
    "    Returns:\n",
    "        The list of the titles of the linked pages\n",
    "    \"\"\"\n",
    "  \n",
    "    anchor = encode_for_db(anchor)\n",
    "        \n",
    "    _cursor.execute(\"\"\"select anchors.id, anchors.freq from anchors inner join page on anchors.id=page.page_id where anchors.anchor=%s;\"\"\", (anchor,))\n",
    "    rows =_cursor.fetchall()\n",
    "#     if rows:\n",
    "#         rows = tuple(r[0] for r in rows)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def id2anchor(wid):\n",
    "    \"\"\" Returns the targets of an anchor text\n",
    "\n",
    "    Args:\n",
    "        anchor: anchor\n",
    "        \n",
    "    Returns:\n",
    "        The list of the titles of the linked pages\n",
    "    \"\"\"\n",
    "    _cursor.execute(\"\"\"select anchor , freq from anchors where id=%s\"\"\", (wid,))\n",
    "    rows =_cursor.fetchall()\n",
    "#     if rows:\n",
    "#         rows = tuple(r[0] for r in rows)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def _getlinkedpages_query(id, direction):\n",
    "    query=\"(SELECT {0} as lid FROM pagelinks where ({1} = {2}))\"\n",
    "    if direction == DIR_IN:\n",
    "        query=query.format(\"pl_from\",\"pl_to\",id);\n",
    "    elif direction == DIR_OUT:\n",
    "        query=query.format(\"pl_to\",\"pl_from\",id);\n",
    "    return query;\n",
    "\n",
    "def getlinkedpages(wid,direction):\n",
    "    \"\"\" Returns the linkage for a node\n",
    "\n",
    "    Args:\n",
    "        id: the wikipedia id\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "    Returns:\n",
    "        The list of the ids of the linked pages\n",
    "    \"\"\"\n",
    "    _cursor.execute(_getlinkedpages_query(wid, direction));\n",
    "    rows =_cursor.fetchall()\n",
    "    if rows:\n",
    "        rows = tuple(r[0] for r in rows)\n",
    "    return rows\n",
    "\n",
    "def e2i(wids):\n",
    "    elist=[];\n",
    "    edict=dict();\n",
    "    last=0;    \n",
    "    for wid in itertools.chain(*iters):\n",
    "        if wid not in edict:\n",
    "            edict[wid]=last;\n",
    "            elist.append(wid);\n",
    "            last +=1; \n",
    "    return elist, edict;\n",
    "\n",
    "def getneighbors(wid, direction):\n",
    "    \"\"\" Returns the neighborhood for a node\n",
    "\n",
    "    Args:\n",
    "        id: the wikipedia id\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "    Returns:\n",
    "        The vector of ids, and the 2d array sparse representation of the graph, in the form of\n",
    "        array([[row1,col1],[row2, col2]]). This form is flexible for general use or be converted to scipy.sparse \n",
    "        formats\n",
    "    \"\"\"\n",
    "    log('[getneighbors started]\\twid = %s, direction = %s', wid, direction)\n",
    "    \n",
    "    idsquery = \"\"\"(select  {0} as lid) union {1}\"\"\".format(wid,_getlinkedpages_query(wid,direction));\n",
    "\n",
    "    _cursor.execute(idsquery);\n",
    "\n",
    "\n",
    "    rows = _cursor.fetchall();\n",
    "    if len(rows)<2:\n",
    "        log('[getneighbors]\\tERROR: empty')\n",
    "        return (), sp.array([])\n",
    "    \n",
    "    \n",
    "    neighids = tuple(r[0] for r in rows);\n",
    "    if len(neighids)>MAX_GRAPH_SIZE:\n",
    "        log('[getneighbors]\\tERROR: too big, %s neighbors', len(neighids))\n",
    "        return (), sp.array([])\n",
    "\n",
    "    \n",
    "    id2row = dict(zip(neighids, range(len(neighids))))\n",
    "\n",
    "    neighbquery=  \"\"\"select lid,pl_to as n_l_to from\n",
    "                     ({0}) a  inner join\n",
    "                     pagelinks on lid=pl_from\"\"\".format(idsquery);\n",
    "\n",
    "    links=_cursor.execute(neighbquery);\n",
    "\n",
    "    links = _cursor.fetchall();\n",
    "    \n",
    "    #links = tuple((id2row(u), id2row(v)) for u, v in links if (u in id2row) and (v in id2row));\n",
    "    links = sp.array([[id2row[u], id2row[v]] for u, v in links if (u in id2row) and (v in id2row)]);\n",
    "    \n",
    "    log('Graph extracted, %s nodes and %s linkes', len(neighids), len(links) )\n",
    "    log('[getneighbors]\\tfinished')\n",
    "    return (neighids,links)\n",
    "\n",
    "def deletefromcache(wid, direction):\n",
    "    wid = resolveredir(wid)\n",
    "    if direction in [DIR_IN, DIR_BOTH] : \n",
    "        query =    \"\"\"delete from {0} where cache_id={1}\"\"\".format('pagelinksorderedin', wid) \n",
    "        _cursor.execute(query);\n",
    "    if direction in [DIR_OUT, DIR_BOTH]: \n",
    "        query =    \"\"\"delete from {0} where cache_id={1}\"\"\".format('pagelinksorderedout', wid) \n",
    "        _cursor.execute(query);\n",
    "    \n",
    "def clearcache():\n",
    "    if DISABLE_CACHE:\n",
    "        return;\n",
    "    _cursor.execute(\"delete  from pagelinksorderedin\");\n",
    "    _cursor.execute(\"delete  from pagelinksorderedout\");\n",
    "\n",
    "def checkcache(wid, direction):\n",
    "    log('[checkcache started]\\twid = %s, direction = %s', wid, direction)\n",
    "    if DISABLE_CACHE:\n",
    "        log('[checkcache]\\tDisabled')\n",
    "        return None\n",
    "    \n",
    "\n",
    "    \n",
    "    em=None\n",
    "    \n",
    "    if direction == DIR_IN: \n",
    "        tablename = 'pagelinksorderedin';\n",
    "        colname = 'in_neighb'\n",
    "    elif direction == DIR_OUT: \n",
    "        tablename = 'pagelinksorderedout';\n",
    "        colname = 'out_neighb';\n",
    "    query =    \"\"\"select {0} from {1} where cache_id={2}\"\"\".format(colname, tablename, wid)\n",
    "    _cursor.execute(query);\n",
    "    row = _cursor.fetchone();\n",
    "    if row is not None:\n",
    "        values, index = pickle.loads(row[0])\n",
    "        log('[checkcache]\\tfound')\n",
    "        if not index:        \n",
    "            log('[checkcache]\\tempty embedding')\n",
    "        em=pd.Series(values, index=index)\n",
    "    else:\n",
    "        log('[checkcache]\\tnot found')\n",
    "\n",
    "    log('[checkcache]\\tfinished')\n",
    "    return em\n",
    "\n",
    "\n",
    "def cachescores(wid, em, direction):\n",
    "    log('[cachescores started]\\twid = %s, direction = %s', wid, direction)\n",
    "    if DISABLE_CACHE:\n",
    "        log('[cachescores]\\tDisabled')\n",
    "        return\n",
    "\n",
    "    if direction == DIR_IN: \n",
    "        tablename = 'pagelinksorderedin';\n",
    "        colname = 'in_neighb'\n",
    "\n",
    "    elif direction == DIR_OUT: \n",
    "        tablename = 'pagelinksorderedout';\n",
    "        colname = 'out_neighb';\n",
    "        \n",
    "    idscstr = pickle.dumps((em.values.tolist(), em.index.values.tolist()), pickle.HIGHEST_PROTOCOL)\n",
    "    _cursor.execute(\"\"\"insert into %s values (%s,'%s');\"\"\" %(tablename, wid, _db.escape_string(idscstr)));\n",
    "    \n",
    "    \n",
    "    log('cachescores finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "Some small helper function for reporting purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile utils.py \n",
    "\"\"\"Utility functions\"\"\"\n",
    "# uncomment\n",
    "\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\", \"Evangelo Milios\", \"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "def readds(url, usecols=None):    \n",
    "    data = pd.read_table(url, header=None, usecols=usecols)\n",
    "    return data\n",
    "\n",
    "DISABLE_LOG=True;\n",
    "\n",
    "def clearlog(logfile):\n",
    "    with open(logfile, 'w'):\n",
    "        pass;\n",
    "\n",
    "def logres(outfile, instr, *params):\n",
    "    outstr = instr % params;\n",
    "    with open(outfile, 'a') as f:\n",
    "        f.write(\"[%s]\\t%s\\n\" % (str(datetime.datetime.now()) , outstr));          \n",
    "        \n",
    "def log(instr, *params):\n",
    "    if DISABLE_LOG:\n",
    "        return\n",
    "    logres(logfile, instr, *params)\n",
    "    \n",
    "if not DISABLE_LOG:    \n",
    "    outdir = '../out'    \n",
    "    logfile=os.path.join(outdir, 'log.txt');\n",
    "    if not os.path.exists(logfile):\n",
    "        if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "        log('log created') \n",
    "        os.chmod(logfile, 0777)    \n",
    "    \n",
    "    \n",
    "def timeformat(sec):\n",
    "    return datetime.timedelta(seconds=sec)\n",
    "\n",
    "def str2delta(dstr):\n",
    "    r=re.match(('((?P<d>\\d+) day(s?), )?(?P<h>\\d+):(?P<m>\\d+):(?P<s>\\d*\\.\\d+|\\d+)'),dstr)\n",
    "    d,h,m,s=r.group('d'),r.group('h'),r.group('m'),r.group('s')\n",
    "    d=int(d) if d is not None else 0\n",
    "    h,m,s = int(h), int(m), float(s)    \n",
    "    return datetime.timedelta(days=d, hours=h, minutes=m, seconds=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Pagerank Implementation\n",
    "\n",
    "Here we have the actuall implementation of pagerank. Two implemenation are provided, both inspired  by the sparse fast solutions given in **Cleve Moler**'s book, [*Experiments with MATLAB*](http://www.mathworks.com/moler/index_ncm.html). The power method is much faster with enough precision for our task. Our benchmarsk shows that this implementation is faster than networkx implementation magnititude of times\n",
    "\n",
    "The input is a 2d array, each row of the array is an edge of the graph [[a,b], [c,d]], a and b are the node numbers. \n",
    "(In case you want to caclulate reall page rank, uncomment the line that transposes the adjacency matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile pagerank.py \n",
    "\"\"\"Two implementations of PageRank.\n",
    "\n",
    "Pythom implementations of Matlab original in Cleve Moler, Experiments with MATLAB.\n",
    "\"\"\"\n",
    "# uncomment\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.sparse as sprs\n",
    "import scipy.spatial\n",
    "import scipy.sparse.linalg \n",
    "\n",
    "from utils import * # uncomment\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\", \"Evangelo Milios\", \"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "\n",
    "def create_csr(Z):\n",
    "    \"\"\" Creates a csr presentation from 2darray presentation and \n",
    "        calculates the pagerank\n",
    "    Args:\n",
    "        G: input graph in the form of a 2d array, such as [[2,0], [1,2], [2,1]]\n",
    "    Returns:\n",
    "        Pagerank Scores for the nodes\n",
    "    \n",
    "    each row of the array is an edge of the graph [[a,b], [c,d]], a and b are the node numbers. \n",
    "\n",
    "    \"\"\"   \n",
    "    rows = Z[:,0];\n",
    "    cols = Z[:,1];\n",
    "    n = max(max(rows), max(cols))+1;\n",
    "    G=sprs.csr_matrix((sp.ones(rows.shape),(rows,cols)), shape=(n,n));\n",
    "    return G\n",
    "\n",
    "def pagerank_sparse(G, p=0.85, personalize=None, reverse=False):\n",
    "    \"\"\" Calculates pagerank given a csr graph\n",
    "    \n",
    "    Args:\n",
    "        G: a csr graph.\n",
    "        p: damping factor\n",
    "        personlize: if not None, should be an array with the size of the nodes\n",
    "                    containing probability distributions. It will be normalized automatically\n",
    "        reverse: If true, returns the reversed-pagerank \n",
    "        \n",
    "    Returns:\n",
    "        Pagerank Scores for the nodes\n",
    "     \n",
    "    \"\"\"\n",
    "    log('[pagerank_sparse]\\tstarted')\n",
    "\n",
    "    if not reverse:\n",
    "        G=G.T;\n",
    "\n",
    "    n,n=G.shape\n",
    "    c=sp.asarray(G.sum(axis=0)).reshape(-1)\n",
    "    r=sp.asarray(G.sum(axis=1)).reshape(-1)\n",
    "\n",
    "    k=c.nonzero()[0]\n",
    "\n",
    "    D=sprs.csr_matrix((1/c[k],(k,k)),shape=(n,n))\n",
    "\n",
    "    if personalize is None:\n",
    "        e=sp.ones((n,1))\n",
    "    else:\n",
    "        e = personalize/sum(personalize);\n",
    "        \n",
    "    I=sprs.eye(n)\n",
    "    X1 = sprs.linalg.spsolve((I - p*G.dot(D)), e);\n",
    "\n",
    "    X1=X1/sum(X1)\n",
    "    log('[pagerank_sparse]\\tfinished')\n",
    "    return X1\n",
    "def pagerank_sparse_power(G, p=0.85, max_iter = 100, personalize=None, reverse=False):\n",
    "    \"\"\" Calculates pagerank given a csr graph\n",
    "    \n",
    "    Args:\n",
    "        G: a csr graph.\n",
    "        p: damping factor\n",
    "        max_iter: maximum number of iterations\n",
    "        personlize: if not None, should be an array with the size of the nodes\n",
    "                    containing probability distributions. It will be normalized automatically\n",
    "        reverse: If true, returns the reversed-pagerank \n",
    "        \n",
    "    Returns:\n",
    "        Pagerank Scores for the nodes\n",
    "     \n",
    "    \"\"\"\n",
    "    log('[pagerank_sparse_power]\\tstarted')\n",
    "    \n",
    "    if not reverse: \n",
    "        G=G.T;\n",
    "\n",
    "    n,n=G.shape\n",
    "    c=sp.asarray(G.sum(axis=0)).reshape(-1)\n",
    "    r=sp.asarray(G.sum(axis=1)).reshape(-1)\n",
    "\n",
    "    k=c.nonzero()[0]\n",
    "\n",
    "    D=sprs.csr_matrix((1/c[k],(k,k)),shape=(n,n))\n",
    "\n",
    "    if personalize is None:\n",
    "        e=sp.ones((n,1))\n",
    "    else:\n",
    "        e = personalize/sum(personalize);\n",
    "        \n",
    "    z = (((1-p)*(c!=0) + (c==0))/n)[sp.newaxis,:]\n",
    "    G = p*G.dot(D)\n",
    "    x = e/n\n",
    "    oldx = sp.zeros((n,1));\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    while sp.linalg.norm(x-oldx) > 0.001:\n",
    "        oldx = x\n",
    "        x = G.dot(x) + e.dot(z.dot(x))\n",
    "        iteration += 1\n",
    "        if iteration >= max_iter:\n",
    "            break;\n",
    "    x = x/sum(x)\n",
    "    \n",
    "    log('# of iterations: %s, normdiff: %s', iteration, sp.linalg.norm(x-oldx))\n",
    "    log('[pagerank_sparse_power]\\tfinished')\n",
    "    return x.reshape(-1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile embedding.py\n",
    "\n",
    "from wikipedia import * # uncomment\n",
    "from pagerank import * # uncomment\n",
    "import gensim\n",
    "\n",
    "#from utils import * # uncomment\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\", \"Evangelo Milios\", \"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "_word2vec_model = None\n",
    "def getword2vec_model():\n",
    "    \"\"\" returns the word2vec model\n",
    "    \"\"\"\n",
    "    \n",
    "    return _word2vec_model\n",
    "\n",
    "def conceptrep(wid, method ='rvspagerank', direction=DIR_BOTH, get_titles=True, cutoff=None):\n",
    "    \"\"\" Calculates well-known similarity metrics between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "        method:\n",
    "            rvspagerank: rvs-pagerank embedding\n",
    "            word2vec : wor2vec representation\n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "        \n",
    "    if method =='rvspagerank':\n",
    "        return conceptrep_rvs(wid, direction, get_titles, cutoff)\n",
    "    if 'word2vec' in method:\n",
    "        return getword2vector(wid)\n",
    "\n",
    "\n",
    "def concept_embedding(wid, direction):\n",
    "    \"\"\" Calculates concept embedding to be used in relatedness\n",
    "    \n",
    "    Args:\n",
    "        wid: wikipedia id\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "        \n",
    "    Returns:\n",
    "        The neighbor ids, their scores and the whole neighorhood graph (for visualization purposes)\n",
    "        \n",
    "    \"\"\"\n",
    "    log('[concept_embedding started]\\twid = %s, direction = %s', wid, direction)\n",
    "\n",
    "    if direction == DIR_IN or direction==DIR_OUT:\n",
    "        em = _concept_embedding_io(wid, direction)\n",
    "    if direction == DIR_BOTH:\n",
    "        em = _concept_embedding_both(wid, direction)\n",
    "    log('[concept_embedding]\\tfinished')\n",
    "    return em\n",
    "    \n",
    "def _concept_embedding_io(wid, direction):\n",
    "    wid = resolveredir(wid)\n",
    "    cached_em = checkcache(wid, direction);\n",
    "    if cached_em is not None:\n",
    "        return cached_em;\n",
    "\n",
    "    (ids, links) = getneighbors(wid, direction);\n",
    "    if ids:\n",
    "        scores = pagerank_sparse_power(create_csr(links), reverse=True)\n",
    "        em = pd.Series(scores, index=ids) \n",
    "    else:\n",
    "        em = pd.Series([], index=[])  \n",
    "    cachescores(wid, em, direction);\n",
    "    return em\n",
    "            \n",
    "\n",
    "def _concept_embedding_both(wid, direction):            \n",
    "        in_em = _concept_embedding_io(wid, DIR_IN);\n",
    "        out_em = _concept_embedding_io(wid, DIR_OUT )\n",
    "        if (in_em is None) or (out_em is None):\n",
    "            return None;\n",
    "        return in_em.add(out_em, fill_value=0)/2\n",
    "\n",
    "def conceptrep_rvs(wid, direction, get_titles=True, cutoff=None):\n",
    "    \"\"\" Finds a representation for a concept\n",
    "    \n",
    "        Concept Representation is a vector of concepts with their score\n",
    "    Arg:\n",
    "        wid: Wikipedia id\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "        titles: include titles in the embedding (not needed for mere calculations)\n",
    "        cutoff: the first top cutoff dimensions (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        the vecotr of ids, their titles and theirs scores. It also returns the\n",
    "        graph for visualization purposes. \n",
    "    \"\"\"\n",
    "    \n",
    "    log('[conceptrep started]\\twid = %s, direction = %s', wid, direction)\n",
    "    \n",
    "    em=concept_embedding(wid, direction);    \n",
    "    if em.empty:\n",
    "        return em;\n",
    "    \n",
    "    \n",
    "    #ids = em.keys();\n",
    "    \n",
    "    if cutoff is not None:\n",
    "        em = em.sort_values(ascending=False)\n",
    "        em = em[:cutoff]\n",
    "    if get_titles:\n",
    "        em = pd.Series(zip(ids2title(em.index), em.values.tolist()), index=em.index)\n",
    "    log ('[conceptrep]\\tfinished')\n",
    "    return em\n",
    "\n",
    "def gensim_loadmodel(model_path):\n",
    "    \"\"\" Loads the word2vec model \n",
    "    Arg:\n",
    "        model_path: path to the model\n",
    "    \"\"\"\n",
    "    global _word2vec_model\n",
    "    log('[getsim_word2vec]\\tloading: %s', model_path)\n",
    "    _word2vec_model = gensim.models.Word2Vec.load(model_path)                \n",
    "    log('[getsim_word2vec]\\loaded')\n",
    "    return _word2vec_model\n",
    "    \n",
    "def getword2vector(wid):\n",
    "    wid_s=str(wid)\n",
    "    wid_s = 'id_'+ wid_s\n",
    "    if wid_s not in _word2vec_model.vocab:\n",
    "        return  pd.Series(sp.zeros(_word2vec_model.vector_size))\n",
    "    return pd.Series(_word2vec_model[wid_s])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Semantic Relatedness\n",
    "The idea is get the neighborhood graph for each concept and calculating the similarity by embedding the graph into a vector and then perforiming cosine similarity. \n",
    "\n",
    "The process can be illustrated like this:\n",
    "    ![](../resrc/alg.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting calcsim.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile calcsim.py \n",
    "\"\"\"Calculating Relatedness.\"\"\"\n",
    "# uncomment\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "from embedding import *\n",
    "#from collections import defaultdict\n",
    "import json\n",
    "import math\n",
    "from scipy import stats\n",
    "from config import *\n",
    "#from utils import * # uncomment\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\", \"Evangelo Milios\", \"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "#constants\n",
    "\n",
    "\n",
    "def _unify_ids_scores(*id_sc_tuple):\n",
    "    uids, id2in = e2i(*(ids for ids, _ in id_sc_tuple));\n",
    "    \n",
    "    uscs=tuple();            \n",
    "    for ids,scs in id_sc_tuple:\n",
    "        scs_u=sp.zeros(len(id2in))\n",
    "        scs_u[[id2in[wid] for wid in ids]] = scs;            \n",
    "        uscs += (scs_u,)                \n",
    "    return uids, uscs       \n",
    "\n",
    "    \n",
    "def getsim_word2vec(id1, id2):\n",
    "    \"\"\" Calculates wor2vec similarity between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    model  = getword2vec_model()\n",
    "    if model is None:\n",
    "        log('[getsim_word2vec]\\tmodel not loaded')\n",
    "        raise Exception('model not loaded, try gensim_loadmodel()')\n",
    "        \n",
    "    if id1 not in model.vocab:\n",
    "        #print '%s,%s skipped, %s not in vocab ' % (id1, id2, id1)\n",
    "        return 0\n",
    "    if id2 not in model.vocab:\n",
    "        #print '%s,%s skipped, %s not in vocab ' % (id1, id2, id2)\n",
    "        return 0\n",
    "    return model.similarity(id1, id2)\n",
    "\n",
    "\n",
    "def getsim_wlm(id1, id2):\n",
    "    \"\"\" Calculates wlm (ngd) similarity between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    in1 = set(getlinkedpages(id1, DIR_IN))\n",
    "    in2 = set(getlinkedpages(id2, DIR_IN))\n",
    "    f1 = len(in1)\n",
    "    f2 = len(in2)\n",
    "    f12=len(in1.intersection(in2))\n",
    "    dist = (sp.log(max(f1,f2))-sp.log(f12))/(sp.log(WIKI_SIZE)-sp.log(min(f1,f2)));\n",
    "    if (f1==0) or (f2==0) or (f12==0):\n",
    "        return 0;\n",
    "    sim = 1-dist if dist <=1 else 0\n",
    "    return sim\n",
    "\n",
    "def getsim_cocit(id1, id2):\n",
    "    \"\"\" Calculates co-citation similarity between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    in1 = set(getlinkedpages(id1, DIR_IN))\n",
    "    in2 = set(getlinkedpages(id2, DIR_IN))\n",
    "    f1 = len(in1)\n",
    "    f2 = len(in2)\n",
    "    if (f1==0) or (f2==0):\n",
    "        return 0;\n",
    "    \n",
    "    f12=len(in1.intersection(in2))\n",
    "    sim = (f12)/(f1+f2-f12);\n",
    "    return sim\n",
    "\n",
    "\n",
    "def getsim_coup(id1, id2):\n",
    "    \"\"\" Calculates coupler similarity between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    in1 = set(getlinkedpages(id1, DIR_OUT))\n",
    "    in2 = set(getlinkedpages(id2, DIR_OUT))\n",
    "    f1 = len(in1)\n",
    "    f2 = len(in2)\n",
    "    if (f1==0) or (f2==0):\n",
    "        return 0;\n",
    "    \n",
    "    f12=len(in1.intersection(in2))\n",
    "    sim = (f12)/(f1+f2-f12);\n",
    "    return sim\n",
    "\n",
    "def getsim_ams(id1, id2):\n",
    "    \"\"\" Calculates amlser similarity between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    in1 = set(getlinkedpages(id1, DIR_IN))\n",
    "    out1 = set(getlinkedpages(id1, DIR_OUT))\n",
    "    link1 = in1.union(out1)\n",
    "    \n",
    "    in2 = set(getlinkedpages(id2, DIR_IN))\n",
    "    out2 = set(getlinkedpages(id2, DIR_OUT))\n",
    "    link2 = in2.union(out2)\n",
    "    \n",
    "    f1 = len(link1)\n",
    "    f2 = len(link2)\n",
    "    if (f1==0) or (f2==0):\n",
    "        return 0;\n",
    "    \n",
    "    f12=len(link1.intersection(link2))\n",
    "    sim = (f12)/(f1+f2-f12);\n",
    "    return sim\n",
    "\n",
    "\n",
    "def getsim_emb(id1,id2, direction):\n",
    "    \"\"\" Calculates the similarity between two concepts\n",
    "    Arg:\n",
    "        id1, id2: the two concepts\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "        \n",
    "    Returns:\n",
    "        The similarity score\n",
    "    \"\"\"\n",
    "    em1 = concept_embedding(id1, direction);\n",
    "    em2 = concept_embedding(id2, direction);\n",
    "    if em1.empty or em2.empty:\n",
    "        return 0;\n",
    "    \n",
    "    em1, em2 = em1.align(em2, fill_value=0)\n",
    "#     print em1\n",
    "#     print em2\n",
    "    return 1-sp.spatial.distance.cosine(em1.values,em2.values);\n",
    "\n",
    "def getsim(id1,id2, method='rvspagerank', direction=DIR_BOTH, sim_method=None):\n",
    "    \"\"\" Calculates well-known similarity metrics between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "        method:\n",
    "            wlm: Wikipedia-Miner method\n",
    "            cocit: cocitation\n",
    "            coup: coupling\n",
    "            ams: amsler\n",
    "            rvspagerank: ebedding based similarity (in our case, \n",
    "                 reversed-page rank method)\n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    log('[getsim started]\\method = %s, direction = %s, id1=%s, id2=%s', method, direction, id1, id2)\n",
    "    \n",
    "    if method=='rvspagerank':\n",
    "        sim = getsim_emb(id1,id2, direction)\n",
    "    elif method=='wlm':\n",
    "        sim = getsim_wlm(id1,id2)\n",
    "    elif method=='cocit':\n",
    "        sim = getsim_cocit(id1,id2)\n",
    "    elif method=='coup':\n",
    "        sim = getsim_coup(id1,id2)\n",
    "    elif method=='ams':\n",
    "        sim = getsim_ams(id1,id2)\n",
    "    elif 'word2vec' in  method:\n",
    "        sim = getsim_word2vec(id1, id2)\n",
    "    elif sim_method is not None:    \n",
    "        sim = sim_method(id1,id2)\n",
    "    else:\n",
    "        sim=None\n",
    "    log('[getsim]\\tfinished')\n",
    "    return sim\n",
    "\n",
    "ENTITY_TITLE = 0\n",
    "ENTITY_ID = 1\n",
    "ENTITY_ID_STR = 2\n",
    "ENTITY_ID_ID_STR = 3\n",
    "    \n",
    "def encode_entity(term1, term2, entity_encoding):\n",
    "    if entity_encoding==ENTITY_TITLE:\n",
    "        return term1, term2\n",
    "    \n",
    "    term1 = title2id(term1)\n",
    "    term2 = title2id(term2)\n",
    "    if entity_encoding==ENTITY_ID_STR:\n",
    "        term1 = str(term1)\n",
    "        term2 = str(term2)\n",
    "        \n",
    "    if entity_encoding==ENTITY_ID_ID_STR:\n",
    "        term1 = 'id_'+term1\n",
    "        term2 = 'id_'+term2\n",
    "    return term1, term2\n",
    "    \n",
    "def getsim_file(infilename, outfilename, method='rvspagerank', direction=DIR_BOTH, sim_method=None, entity_encoding=ENTITY_ID):\n",
    "    \"\"\" Batched (file) similarity.\n",
    "    \n",
    "    Args: \n",
    "        infilename: tsv file in the format of pair1    pair2   [goldstandard]\n",
    "        outfilename: tsv file in the format of pair1    pair2   similarity\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "        entity_encoding: how the entity is represented in the dataset\n",
    "                        ENTITY_TITLE = simple entity\n",
    "                        ENTITY_ID = integer id\n",
    "                        ENTITY_ID_STR = str id\n",
    "                        ENTITY_ID_ID_STR = id_entityid\n",
    "                        \n",
    "    Returns:\n",
    "        vector of scores, and Spearmans's correlation if goldstandard is given\n",
    "    \"\"\"\n",
    "    log('[getsim_file started]\\t%s -> %s', infilename, outfilename)\n",
    "    outfile = open(outfilename, 'w');\n",
    "    dsdata=readds(infilename);\n",
    "    gs=[];\n",
    "    scores=[];\n",
    "    spcorr=None;\n",
    "    for row in dsdata.itertuples():   \n",
    "        log('processing %s, %s', row[1], row[2])\n",
    "        if (row[1]=='null') or (row[2]=='null'):\n",
    "            continue;\n",
    "        if len(row)>3: \n",
    "            gs.append(row[3]);\n",
    "            \n",
    "        term1, term2 = encode_entity(row[1], row[2], entity_encoding)\n",
    "            \n",
    "        if (term1 is None) or (term2 is None):\n",
    "            sim=0;\n",
    "        else:\n",
    "            sim=getsim(term1, term2, method, direction, sim_method);\n",
    "        outfile.write(\"\\t\".join([str(row[1]), str(row[2]), str(sim)])+'\\n')\n",
    "        scores.append(sim)\n",
    "    outfile.close();\n",
    "    if gs:\n",
    "        spcorr = sp.stats.spearmanr(scores, gs);\n",
    "    log('[getsim_file]\\tfinished')\n",
    "    return scores, spcorr\n",
    "\n",
    "    \n",
    "\n",
    "def getembed_file(infilename, outfilename, direction, get_titles=False, cutoff=None):\n",
    "    \"\"\" Batched (file) concept representation.\n",
    "    \n",
    "    Args: \n",
    "        infilename: tsv file in the format of pair1    pair2   [goldstandard]\n",
    "        outfilename: tsv file in the format of pair1    pair2   similarity\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "        titles: include titles in the embedding (not needed for mere calculations)\n",
    "        cutoff: the first top cutoff dimensions (None for all)        \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    log('[getembed_file started]\\t%s -> %s', infilename, outfilename)\n",
    "    outfile = open(outfilename, 'w');\n",
    "    dsdata=readds(infilename, usecols=[0]);\n",
    "    scores=[];\n",
    "    for row in dsdata.itertuples():        \n",
    "        wid = title2id(row[1])\n",
    "        if wid is None:\n",
    "            em=pd.Series();\n",
    "        else:\n",
    "            em=conceptrep(wid, method='rvspagerank', direction = direction, \n",
    "                          get_titles = get_titles, cutoff=cutoff)\n",
    "        outfile.write(row[1]+\"\\t\"+em.to_json()+\"\\n\")\n",
    "    outfile.close();\n",
    "    log('[getembed_file]\\tfinished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%aimport calcsim\n",
    "\n",
    "%aimport wikipedia\n",
    "\n",
    "from wikipedia import * # uncomment\n",
    "from calcsim import *   # uncomment\n",
    "# Examples\n",
    "reopen()\n",
    "direction = DIR_IN\n",
    "\n",
    "page_title1 = 'Abortion' \n",
    "print ('page_title: ', page_title1)\n",
    "\n",
    "page_id1 = title2id(page_title1)\n",
    "print (\"id: \", page_id1)\n",
    "\n",
    "sr1 = synonymring_titles(page_id1)\n",
    "print (\"synonym ring: %s\\n \" % str(sr1[:5]))\n",
    "\n",
    "rep1=conceptrep(page_id1, method='rvspagerank', direction = direction,  get_titles=True, cutoff=5)\n",
    "print (\"Concept Representation:  %s\\n\" % rep1.to_json())\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "page_title2 = 'Miscarriage' \n",
    "print ('page_title: ', page_title2)\n",
    "\n",
    "page_id2 = title2id(page_title2)\n",
    "print (\"id: \", page_id2)\n",
    "\n",
    "sr2 = synonymring_titles(page_id2)\n",
    "print (\"synonym ring: %s\\n \" % str(sr2[:5]))\n",
    "\n",
    "rep2=conceptrep(page_id2, method='rvspagerank', direction = direction,  get_titles=True, cutoff=5)\n",
    "print (\"Concept Representation: %s\\n\" % rep2.to_json())\n",
    "\n",
    "\n",
    "\n",
    "sim = getsim(page_id1, page_id2,'rvspagerank',DIR_IN)\n",
    "print (\"similarity\", sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating All The Embeddngs\n",
    "### Note: This step might take several days\n",
    "This step can be safely skipped and let the caching mechansism happens gradually over time, but if you have some heavy task, it is worth to invest some time and calculate the embeddings off-line\n",
    "\n",
    "### First. Get a list of the id pages\n",
    "```\n",
    "SELECT page_id\n",
    "INTO OUTFILE '~/backup/wikipedia/20160305/edited/enwiki-20160305-page.dumped.ssv'\n",
    "FROM page\n",
    "where page_namespace=0 and page_is_redirect=0 ;\n",
    "```\n",
    "### Second. Starting the precalculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile preembed.py \n",
    "\"\"\"Pre calculation of the embeddings\"\"\"\n",
    "\n",
    "from config import *\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload\n",
    "\n",
    "# %aimport calcsim\n",
    "from calcsim import *\n",
    "direction = DIR_IN;\n",
    "dirstr = graphtype(direction)\n",
    "#wid_fname  = os.path.join(home, 'backup/wikipedia/20160305/embed/enwiki-20160305-page.dumped.ssv')\n",
    "wid_fname = os.path.join(home, 'backup/wikipedia/20160305/embed/enwiki-20160305-embeddings.'+dirstr+'.dead_2.ssv')\n",
    "\n",
    "done_fname = os.path.join(home, 'backup/wikipedia/20160305/embed/enwiki-20160305-embeddings.'+dirstr+'.done.ssv')\n",
    "dead_fname = os.path.join(home, 'backup/wikipedia/20160305/embed/enwiki-20160305-embeddings.'+dirstr+'.dead_3.ssv')\n",
    "rewrite = True\n",
    "lastwid = \"\"\n",
    "if os.path.exists(done_fname):\n",
    "    with open(done_fname) as done_f:\n",
    "        for lastwid in done_f:\n",
    "            pass\n",
    "        if lastwid is not None:\n",
    "            lastwid = lastwid.strip() \n",
    "            \n",
    "\n",
    "wid_f = open(wid_fname)\n",
    "done_f = open(done_fname, 'a')\n",
    "dead_f = open(dead_fname, 'a')\n",
    "    \n",
    "if lastwid:\n",
    "    for line in wid_f:\n",
    "        if line.strip() == lastwid:\n",
    "            break\n",
    "    print \"Continuing from \", lastwid\n",
    "else: \n",
    "    print \"Fresh start\"\n",
    "    \n",
    "for line in wid_f:\n",
    "    wid = line.strip().split('\\t')[0]\n",
    "    if rewrite:\n",
    "        deletefromcache(wid, direction)\n",
    "    em = concept_embedding(wid, direction)\n",
    "    if em.empty:\n",
    "        count = str(len(getlinkedpages(wid, direction)))\n",
    "        dead_f.write(wid+'\\t'+id2title(wid)+'\\t'+count+'\\n')\n",
    "    done_f.write(wid+'\\n')\n",
    "wid_f.close()\n",
    "done_f.close()\n",
    "dead_f.close()\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing The Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from calcsim import *\n",
    "\n",
    "import json\n",
    "from IPython.display import Javascript\n",
    "\n",
    "cre1 = conceptrep(title2id('Tehran'), method='rvspagerank', direction = DIR_OUT, get_titles=True, cutoff=5);\n",
    "cre2 = conceptrep(title2id('Sanandaj'), method='rvspagerank', direction = DIR_OUT, get_titles=True, cutoff=5);\n",
    "\n",
    "\n",
    "#runs arbitrary javascript, client-side\n",
    "Javascript(\"\"\"\n",
    "           window.vizObj1={};window.vizObj2={};\n",
    "           \"\"\".format(cre1.to_json(), cre2.to_json()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "require.config({\n",
    "    paths: {\n",
    "        d3:'//cgm6.research.cs.dal.ca/~sajadi/wikisim/js/d3',\n",
    "        d3_cloud:'//cgm6.research.cs.dal.ca/~sajadi/wikisim/js/d3.layout.cloud',\n",
    "        simple_draw:'//cgm6.research.cs.dal.ca/~sajadi/wikisim/js/simpledraw'\n",
    "\n",
    "    }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "function createWords(cp){\n",
    "\n",
    "    var titles=[];\n",
    "    var scores=[];\n",
    "\n",
    "    for (var key in cp){ \n",
    "        if (cp.hasOwnProperty(key)) {\n",
    "            titles.push(cp[key][0])\n",
    "            scores.push(cp[key][1])\n",
    "        }\n",
    "    }\n",
    "    var sum = scores.reduce(function(a, b) {return a + b;});\n",
    "    var min = Math.min.apply(null, scores)\n",
    "    var max = Math.max.apply(null, scores)\n",
    "    \n",
    "    scores=scores.map(function(a){return (a/sum)*90+20});\n",
    "    var words=[];\n",
    "    for (var i = 0; i<titles.length; i++) {\n",
    "        words.push({\"text\":titles[i], \"size\": scores[i]})\n",
    "    }\n",
    "    return words;\n",
    "}\n",
    "\n",
    "var words1=createWords(window.vizObj1);\n",
    "//element.text(JSON.stringify(words1));\n",
    "var words2=createWords(window.vizObj2);\n",
    "require(['d3','d3_cloud', 'simple_draw'], function(d3,d3_cloud, simple_draw){\n",
    "    $(\"#chart1\").remove();\n",
    "    element.append(\"<div id='chart1' style='width:49%; height:500px; float:left; border-style:solid'> </div>\");\n",
    "    simpledraw(words1, chart1);\n",
    "    \n",
    "    $(\"#chart2\").remove();\n",
    "    element.append(\"<div id='chart2' style='width:49%; margin-left:2%; height:500px; float:left; border-style:solid'> </div>\");\n",
    "    simpledraw(words2, '#chart2');    \n",
    "    \n",
    "});    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
