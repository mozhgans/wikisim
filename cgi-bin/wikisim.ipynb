{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Semantic Relatedness using Wikipedia\n",
    "\n",
    "* **Armin Sajadi** - Faculty of Computer Science\n",
    "* **Dr. Evangelos Milios** - Faculty of Computer Science\n",
    "* **Dr. Vlado Kešelj** – Faculty of Computer Science\n",
    "* **Dr. Jeannette C.M. Janssen** - Mathematics & Statistics\n",
    "\n",
    "This is a simple and step by step explanation of calculating semantic relatedness using Wikipedia. We start by preprocessing and building the api, that is explained in the following papers papers:\n",
    "\n",
    "* Armin Sajadi, Evangelos E. Milios, Vlado Keselj, Jeannette C. M. Janssen, \"Domain-Specific Semantic Relatedness from Wikipedia Structure: A Case Study in Biomedical Text\", CICLing (1) 2015: 347-360 [(bib)](http://dblp.uni-trier.de/rec/bibtex/conf/cicling/SajadiMKJ15) [(pdf)](http://link.springer.com/chapter/10.1007%2F978-3-319-18111-0_26)\n",
    "\n",
    "* Armin Sajadi,\"Graph-Based Domain-Speciﬁc Semantic Relatedness from Wikipedia\", Canadian AI 2014, LNAI 8436, pp. 381–386, 2014 [(bib)](../resrc/caai14.bib) [(pdf)](http://link.springer.com/chapter/10.1007%2F978-3-319-06483-3_42#)\n",
    "\n",
    "### Public Resources\n",
    "* Weservice: (http://ares.research.cs.dal.ca/~sajadi/wikisim)\n",
    "* Source Code: (https://github.com/asajadi/wikisim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "**Note: You can skip the step by step processing section completely by running `bash preprocess`**. \n",
    "\n",
    "** This is intended to guide you in case on of the steps goes wrong!**\n",
    "\n",
    "\n",
    "The first step is to download the wikipedia database dumps and import them to mysql. We do a preprocessing on the sql dumps for mainly three reasons:\n",
    "\n",
    "* The tables are huge, containing many column and rows we do not use. Removing the unnessary information, that includes unused columns (such as time stamps, viewed count of the pages or categories) and all the information about talk pages, media files or user draft pages, can dramatically decreas the size of the tables.\n",
    "\n",
    "* Forming **synonym Rings**. We extend the concept of synonym ring to Wikipedia (similar to what is called synset in Wordnet). In Wikipedia, redirection stands for equivallency, for example Car --> Automobile. But it's not always this easy and you can find all sorts of weired redirection, like:\n",
    "\n",
    "![](../resrc/sr.jpg)\n",
    "\n",
    "   We iterate through redirectins and remove cycles, dangling redirections and also all the chains. This process forms clusters of redirections around main pages. Then we go through all other tables (pagelinks and  category links) and replace any redirected page by its main article, the result would be much more neated, and makes the rest of the process faster.\n",
    "\n",
    "\n",
    "* We remove garbage, links to non existing pages, self links, mismatching namespaces, and many other incosistencies that you can find the details in the source code).\n",
    "\n",
    "* We apply some strategic changes, like instead of source id --> destination title format of the pagelinks, we use source id --> dest id, which is faster and preferrabel for out case. \n",
    "\n",
    "To complete this step, download and run the parser (written in Java) that prunes these files. You can run the following cells, but due to a known bug with ipython, you can't see bash progress messages untill the job is finished. So a better option would be simply running the scripts from bash and skipping the remaining of this section. In this case, running each cell create a script in the [preparation_scripts/] directory with the name indicated as the argument of `writefile` at the begining of the cell. If you want to run the cell directly, ** comment the first line and uncomment the second line**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading\n",
    "\n",
    "The script will download wikipedia dumps to the the default `~/Downloads/wikidumps` directory\n",
    "Run the script by:\n",
    "\n",
    "`bash download.sh`\n",
    "\n",
    "followed by\n",
    "\n",
    "`bash decompress.sh`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "raw_mimetype": "text/markdown"
   },
   "outputs": [],
   "source": [
    "%%writefile ../preparation_scripts/download.sh\n",
    "#%%system\n",
    "\n",
    "#Downloading the datasets, it might take a while, and make sure the destination exists\n",
    "\n",
    "wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-page.sql.gz            \\\n",
    "\t-P ~/Downloads/wikidumps\n",
    "wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pagelinks.sql.gz      \\\n",
    "\t -P ~/Downloads/wikidumps\n",
    "wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-redirect.sql.gz       \\\n",
    "\t -P ~/Downloads/wikidumps\n",
    "wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-category.sql.gz       \\\n",
    "\t-P ~/Downloads/wikidumps\n",
    "wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-categorylinks.sql.gz \\\n",
    "\t-P ~/Downloads/wikidumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile ../preparation_scripts/decompress.sh\n",
    "\n",
    "#%%system\n",
    "gunzip -c ~/Downloads/wikidumps/enwiki-latest-page.sql.gz\t\t\\\n",
    "\t\t> ~/Downloads/wikidumps/enwiki-latest-page.sql \t\t\n",
    "gunzip -c ~/Downloads/wikidumps/enwiki-latest-pagelinks.sql.gz \t\\\n",
    "\t\t> ~/Downloads/wikidumps/enwiki-latest-pagelinks.sql\t\n",
    "gunzip -c ~/Downloads/wikidumps/enwiki-latest-redirect.sql.gz\t\t\\\n",
    "\t\t> ~/Downloads/wikidumps/enwiki-latest-redirect.sql\t\n",
    "gunzip -c ~/Downloads/wikidumps/enwiki-latest-category.sql.gz\t\t\\\n",
    "\t\t> ~/Downloads/wikidumps/enwiki-latest-category.sql\t\n",
    "gunzip -c ~/Downloads/wikidumps/enwiki-latest-categorylinks.sql.gz\t\\\n",
    "\t\t> ~/Downloads/wikidumps/enwiki-latest-categorylinks.sql\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "The following java file  does the preprosseing (parsing) wikipedia dumps and creates the processed tables (ending in `main.sql`) and several log files of the errors\n",
    "\n",
    "*Note*: you might need to recompile (`javac ProcessSQLDumps.java`) \n",
    "\n",
    "run by\n",
    "\n",
    "`bash parsdumps.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile ../preparation_scripts/parsdumps.sh\n",
    "\n",
    "#%%system\n",
    "java ProcessSQLDumps ~/Downloads/wikidumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing mysql\n",
    "Running the folling cell will set some variable in mysql for maximum performance (if you have enoguh physical memory. replace \\$1 and \\$2 with the actuall user and password of the user, or run the script as:\n",
    "\n",
    "`bash setupmysql.sh <user> <pass>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile ../preparation_scripts/setupmysql.sh\n",
    "\n",
    "#%%system\n",
    "\n",
    "mysql -u $1 -p$2 -e 'set global key_buffer_size=4*1024*1024*1024;'\n",
    "mysql -u $1 -p$2 -e 'set global bulk_insert_buffer_size=1*1024*1024*1024;'\n",
    "mysql -u $1 -p$2 -e 'set global query_cache_size = 4*1024*1024*1024;'\n",
    "mysql -u $1 -p$2 -e 'set global query_cache_limit = 4*1024*1024*1024;'\n",
    "mysql -u $1 -p$2 -e 'set global tmp_table_size = 4*1024*1024*1024;'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Actuall Importing\n",
    "Run:\n",
    "\n",
    "`bash startimport.sh <user> <pass>` \n",
    "\n",
    "This might take several hours \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile ../preparation_scripts/startimport.sh\n",
    "\n",
    "#%%system\n",
    "mysql -u $1 -p$2 -e 'CREATE SCHEMA `enwikilast` DEFAULT CHARACTER SET binary;'\n",
    "./importall  ~/Downloads/wikidumps last $1 $2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Interface\n",
    "This is the main interface to Wikipedia database and provides basic functions given a pages, such as its:\n",
    "* id or title\n",
    "* synonym ring\n",
    "* linkage\n",
    "* in or out neighborhood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wikipedia.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wikipedia.py \n",
    "\"\"\"A General Class to interact with Wiki datasets\"\"\"\n",
    "# uncomment\n",
    "\n",
    "import MySQLdb\n",
    "import sys;\n",
    "import os\n",
    "import scipy as sp\n",
    "from collections import defaultdict\n",
    "import cPickle as pickle\n",
    "\n",
    "from utils import * # uncomment\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\", \"Evangelo Milios\", \"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "\n",
    "DISABLE_CACHE=False;\n",
    "MAX_GRAPH_SIZE=30000\n",
    "\n",
    "DIR_IN=0;\n",
    "DIR_OUT=1;\n",
    "DIR_BOTH=2;\n",
    "_db = MySQLdb.connect(host=\"127.0.0.1\",port=3306,user='root',passwd=\"emilios\",db=\"enwiki20140102\")\n",
    "_cursor = _db.cursor()\n",
    "WIKI_SIZE = 10216236;\n",
    "\n",
    "def close():\n",
    "    global _db, _cursor;\n",
    "    if _cursor is not None: \n",
    "        _cursor.close();\n",
    "        _db.close();\n",
    "    _cursor=_db=None;\n",
    "def reopen():\n",
    "    global _db, _cursor;\n",
    "    if _db is None:\n",
    "        _db = MySQLdb.connect(host=\"127.0.0.1\",port=3306,user='root',passwd=\"emilios\",db=\"enwiki20140102\")\n",
    "        _cursor = _db.cursor()\n",
    "        \n",
    "\n",
    "def id2title(wid):\n",
    "    \"\"\" Returns the title for a given id\n",
    "\n",
    "    Args: \n",
    "        wid: Wikipedia id       \n",
    "    Returns: \n",
    "        The title of the page\n",
    "    \"\"\"\n",
    "    title=None;\n",
    "\n",
    "    _cursor.execute(\"\"\"SELECT * FROM `page` where page_id = %s\"\"\", (wid,))\n",
    "    row= _cursor.fetchone();\n",
    "    if row is not None:\n",
    "        title=row[2];          \n",
    "    return title;\n",
    "\n",
    "def ids2title(wids):\n",
    "    \"\"\" Returns the titles for given list of wikipedia ids \n",
    "\n",
    "    Args: \n",
    "        wids: A list of Wikipedia ids          \n",
    "    Returns: \n",
    "        The list of titles\n",
    "    \"\"\"\n",
    "\n",
    "    wid_list = [str(wid) for wid in wids] ;\n",
    "    order = ','.join(['page_id'] + wid_list) ;\n",
    "    wid_str = \",\".join(wid_list)\n",
    "    query = \"SELECT page_title FROM `page` where page_id in ({0}) order by field ({1})\" \\\n",
    "    .format(wid_str, order);\n",
    "    _cursor.execute(query);\n",
    "    rows = _cursor.fetchall();\n",
    "    if rows:\n",
    "        rows = tuple(r[0] for r in rows)\n",
    "    return rows;\n",
    "\n",
    "\n",
    "def title2id(title):\n",
    "    \"\"\" Returns the id for a given title\n",
    "\n",
    "    Args: \n",
    "        wid: Wikipedia id          \n",
    "    Returns: \n",
    "        The title of the page\n",
    "    \"\"\"        \n",
    "    wid=None;\n",
    "\n",
    "    _cursor.execute(\"\"\"SELECT * FROM `page` where page_title=%s and page_namespace=0\"\"\", (title,))\n",
    "    row= _cursor.fetchone();\n",
    "    if row is not None:\n",
    "        wid = getredir_id(row[0]) if row[3] else row[0];\n",
    "    return wid;\n",
    "\n",
    "def getredir_id(wid):\n",
    "    \"\"\" Returns the target of a redirected page \n",
    "\n",
    "    Args:\n",
    "        wid: wikipedia id of the page\n",
    "    Returns:\n",
    "        The id of the target page\n",
    "    \"\"\"\n",
    "    rid=None\n",
    "\n",
    "    _cursor.execute(\"\"\"select * from redirect where rd_from=%s;\"\"\", (wid,));\n",
    "    row= _cursor.fetchone();\n",
    "    if row is not None:\n",
    "        rid=row[1]\n",
    "    return rid \n",
    "\n",
    "\n",
    "def getredir_title(wid):\n",
    "    \"\"\" Returns the target title of a redirected page \n",
    "\n",
    "    Args:\n",
    "        wid: wikipedia id of the page\n",
    "    Returns:\n",
    "        The title of the target page\n",
    "    \"\"\"\n",
    "    \n",
    "    title=None;\n",
    "    _cursor.execute(\"\"\" select page_title from redirect INNER JOIN page\n",
    "                  on redirect.rd_to = page.page_id \n",
    "                  where redirect.rd_from =%s;\"\"\", (wid));\n",
    "    row=_cursor.fetchone()\n",
    "    if row is not  None:\n",
    "        title=row[0];\n",
    "    return title;\n",
    "\n",
    "def synonymring_titles(wid):\n",
    "    \"\"\" Returns the synonim ring of a page\n",
    "\n",
    "    Example: synonymring_titles('USA')={('U.S.A', 'US', 'United_States_of_America', ...)}\n",
    "\n",
    "    Args:\n",
    "        wid: the wikipedia id\n",
    "    Returns:\n",
    "        all the titles in its synonym ring\n",
    "    \"\"\"\n",
    "\n",
    "    tid = getredir_id(wid);\n",
    "    if tid is not None:\n",
    "        wid = tid;\n",
    "    _cursor.execute(\"\"\"(select page_title from page where page_id=%s) union \n",
    "                 (select page_title from redirect INNER JOIN page\n",
    "                    on redirect.rd_from = page.page_id \n",
    "                    where redirect.rd_to =%s);\"\"\", (wid,wid));\n",
    "    rows=_cursor.fetchall();\n",
    "    if rows:\n",
    "        rows = tuple(r[0] for r in rows)\n",
    "    return rows;\n",
    "\n",
    "def _getlinkedpages_query(id, direction):\n",
    "    query=\"(SELECT {0} as lid FROM pagelinks where ({1} = {2}))\"\n",
    "    if direction == DIR_IN:\n",
    "        query=query.format(\"pl_from\",\"pl_to\",id);\n",
    "    elif direction == DIR_OUT:\n",
    "        query=query.format(\"pl_to\",\"pl_from\",id);\n",
    "    return query;\n",
    "\n",
    "def getlinkedpages(wid,direction):\n",
    "    \"\"\" Returns the linkage for a node\n",
    "\n",
    "    Args:\n",
    "        id: the wikipedia id\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "    Returns:\n",
    "        The list of the ids of the linked pages\n",
    "    \"\"\"\n",
    "    _cursor.execute(_getlinkedpages_query(wid, direction));\n",
    "    rows =_cursor.fetchall()\n",
    "    if rows:\n",
    "        rows = tuple(r[0] for r in rows)\n",
    "    return rows\n",
    "\n",
    "def e2i(wids):\n",
    "    elist=[];\n",
    "    edict=dict();\n",
    "    last=0;    \n",
    "    for wid in itertools.chain(*iters):\n",
    "        if wid not in edict:\n",
    "            edict[wid]=last;\n",
    "            elist.append(wid);\n",
    "            last +=1; \n",
    "    return elist, edict;\n",
    "\n",
    "def getneighbors(wid, direction):\n",
    "    \"\"\" Returns the neighborhood for a node\n",
    "\n",
    "    Args:\n",
    "        id: the wikipedia id\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "    Returns:\n",
    "        The vector of ids, and the 2d array sparse representation of the graph, in the form of\n",
    "        array([[row1,col1],[row2, col2]]). This form is flexible for general use or be converted to scipy.sparse \n",
    "        formats\n",
    "    \"\"\"\n",
    "    log('[getneighbors started]\\twid = %s, direction = %s', wid, direction)\n",
    "    \n",
    "    idsquery = \"\"\"(select  {0} as lid) union {1}\"\"\".format(wid,_getlinkedpages_query(wid,direction));\n",
    "\n",
    "    _cursor.execute(idsquery);\n",
    "\n",
    "\n",
    "    rows = _cursor.fetchall();\n",
    "    if len(rows)<2:\n",
    "        return (), sp.array([])\n",
    "    \n",
    "    \n",
    "    neighids = tuple(r[0] for r in rows);\n",
    "    if len(neighids)>MAX_GRAPH_SIZE is None:\n",
    "        log('[getneighbors]\\tGraph with %s nodes is too big, exiting', len(neighids))\n",
    "        return (), sp.array([])\n",
    "\n",
    "    \n",
    "    id2row = dict(zip(neighids, range(len(neighids))))\n",
    "\n",
    "    neighbquery=  \"\"\"select lid,pl_to as n_l_to from\n",
    "                     ({0}) a  inner join\n",
    "                     pagelinks on lid=pl_from\"\"\".format(idsquery);\n",
    "\n",
    "    links=_cursor.execute(neighbquery);\n",
    "\n",
    "    links = _cursor.fetchall();\n",
    "    \n",
    "    #links = tuple((id2row(u), id2row(v)) for u, v in links if (u in id2row) and (v in id2row));\n",
    "    links = sp.array([[id2row[u], id2row[v]] for u, v in links if (u in id2row) and (v in id2row)]);\n",
    "    \n",
    "    log('Graph extracted, %s nodes and %s linkes', len(neighids), len(links) )\n",
    "    log('[getneighbors]\\tfinished')\n",
    "    return (neighids,links)\n",
    "\n",
    "def clearcache():\n",
    "    if DISABLE_CACHE:\n",
    "        return;\n",
    "    _cursor.execute(\"delete  from pagelinksorderedin\");\n",
    "    _cursor.execute(\"delete  from pagelinksorderedout\");\n",
    "\n",
    "def checkcache(wid, direction):\n",
    "    log('[checkcache started]\\twid = %s, direction = %s', wid, direction)\n",
    "    if DISABLE_CACHE:\n",
    "        log('[checkcache]\\tDisabled')\n",
    "        return None\n",
    "    \n",
    "    em=None\n",
    "    \n",
    "    if direction == DIR_IN: \n",
    "        tablename = 'pagelinksorderedin';\n",
    "        colname = 'in_neighb'\n",
    "    elif direction == DIR_OUT: \n",
    "        tablename = 'pagelinksorderedout';\n",
    "        colname = 'out_neighb';\n",
    "    query =    \"\"\"select {0} from {1} where cache_id={2}\"\"\".format(colname, tablename, wid)\n",
    "    _cursor.execute(query);\n",
    "    row = _cursor.fetchone();\n",
    "    if row is not None:\n",
    "        em=defaultdict(int, pickle.loads(row[0]))\n",
    "    log('[checkcache]\\tfinished')\n",
    "    return em\n",
    "\n",
    "\n",
    "def cachescores(wid, em, direction):\n",
    "    log('[cachescores started]\\twid = %s, direction = %s', wid, direction)\n",
    "    if DISABLE_CACHE:\n",
    "        log('[cachescores]\\tDisabled')\n",
    "        return\n",
    "\n",
    "    if direction == DIR_IN: \n",
    "        tablename = 'pagelinksorderedin';\n",
    "        colname = 'in_neighb'\n",
    "\n",
    "    elif direction == DIR_OUT: \n",
    "        tablename = 'pagelinksorderedout';\n",
    "        colname = 'out_neighb';\n",
    "        \n",
    "    idscstr=pickle.dumps(em, pickle.HIGHEST_PROTOCOL);\n",
    "    _cursor.execute(\"\"\"insert into %s values (%s,'%s');\"\"\" %(tablename, wid, _db.escape_string(idscstr)));\n",
    "    \n",
    "    \n",
    "    log('cachescores finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "Some small helper function for reporting purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile utils.py \n",
    "\"\"\"Utility functions\"\"\"\n",
    "# uncomment\n",
    "\n",
    "import itertools\n",
    "import scipy as sp\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\", \"Evangelo Milios\", \"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "def readds(url):    \n",
    "    data = sp.genfromtxt(url, dtype=None)\n",
    "    return data\n",
    "\n",
    "def logres(outfile, instr, *params):\n",
    "    outstr = instr % params;\n",
    "    with open(outfile, 'a') as f:\n",
    "        f.write(\"[%s]\\t%s\\n\" % (str(datetime.datetime.now()) , outstr));          \n",
    "        \n",
    "def log(instr, *params):\n",
    "    logres(logfile, instr, *params)\n",
    "\n",
    "outdir = '../out'    \n",
    "logfile=os.path.join(outdir, 'log.txt');\n",
    "if not os.path.exists(logfile):\n",
    "    log('log created') \n",
    "    os.chmod(logfile, 0777)    \n",
    "    \n",
    "    \n",
    "def timeformat(sec):\n",
    "    return datetime.timedelta(seconds=sec)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast [Reversed] Pagerank Implementation\n",
    "\n",
    "Here we have the actuall implementation of pagerank. Two implemenation are provided, both inspired  by the sparse fast solutions given in **Cleve Moler**'s book, [*Experiments with MATLAB*](http://www.mathworks.com/moler/index_ncm.html). The power method is much faster with enough precision for our task. Our benchmarsk shows that this implementation is faster than networkx implementation magnititude of times\n",
    "\n",
    "The input is a 2d array, each row of the array is an edge of the graph [[a,b], [c,d]], a and b are the node numbers. \n",
    "(In case you want to caclulate reall page rank, uncomment the line that transposes the adjacency matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile pagerank.py \n",
    "\"\"\"Two implementations of PageRank.\n",
    "\n",
    "Pythom implementations of Matlab original in Cleve Moler, Experiments with MATLAB.\n",
    "\"\"\"\n",
    "# uncomment\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.sparse as sprs\n",
    "import scipy.spatial\n",
    "import scipy.sparse.linalg \n",
    "\n",
    "from utils import * # uncomment\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\", \"Evangelo Milios\", \"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "\n",
    "def create_csr(Z):\n",
    "    \"\"\" Creates a csr presentation from 2darray presentation and \n",
    "        calculates the pagerank\n",
    "    Args:\n",
    "        G: input graph in the form of a 2d array, such as [[2,0], [1,2], [2,1]]\n",
    "    Returns:\n",
    "        Pagerank Scores for the nodes\n",
    "    \n",
    "    each row of the array is an edge of the graph [[a,b], [c,d]], a and b are the node numbers. \n",
    "\n",
    "    \"\"\"   \n",
    "    rows = Z[:,0];\n",
    "    cols = Z[:,1];\n",
    "    n = max(max(rows), max(cols))+1;\n",
    "    G=sprs.csr_matrix((sp.ones(rows.shape),(rows,cols)), shape=(n,n));\n",
    "    return G\n",
    "\n",
    "def pagerank_sparse(G, p=0.85, personalize=None, reverse=False):\n",
    "    \"\"\" Calculates pagerank given a csr graph\n",
    "    \n",
    "    Args:\n",
    "        G: a csr graph.\n",
    "        p: damping factor\n",
    "        personlize: if not None, should be an array with the size of the nodes\n",
    "                    containing probability distributions. It will be normalized automatically\n",
    "        reverse: If true, returns the reversed-pagerank \n",
    "        \n",
    "    Returns:\n",
    "        Pagerank Scores for the nodes\n",
    "     \n",
    "    \"\"\"\n",
    "    log('[pagerank_sparse]\\tstarted')\n",
    "\n",
    "    if not reverse:\n",
    "        G=G.T;\n",
    "\n",
    "    n,n=G.shape\n",
    "    c=sp.asarray(G.sum(axis=0)).reshape(-1)\n",
    "    r=sp.asarray(G.sum(axis=1)).reshape(-1)\n",
    "\n",
    "    k=c.nonzero()[0]\n",
    "\n",
    "    D=sprs.csr_matrix((1/c[k],(k,k)),shape=(n,n))\n",
    "\n",
    "    if personalize is None:\n",
    "        e=sp.ones((n,1))\n",
    "    else:\n",
    "        e = personalize/sum(personalize);\n",
    "        \n",
    "    I=sprs.eye(n)\n",
    "    X1 = sprs.linalg.spsolve((I - p*G.dot(D)), e);\n",
    "\n",
    "    X1=X1/sum(X1)\n",
    "    log('[pagerank_sparse]\\tfinished')\n",
    "    return X1\n",
    "def pagerank_sparse_power(G, p=0.85, max_iter = 100, personalize=None, reverse=False):\n",
    "    \"\"\" Calculates pagerank given a csr graph\n",
    "    \n",
    "    Args:\n",
    "        G: a csr graph.\n",
    "        p: damping factor\n",
    "        max_iter: maximum number of iterations\n",
    "        personlize: if not None, should be an array with the size of the nodes\n",
    "                    containing probability distributions. It will be normalized automatically\n",
    "        reverse: If true, returns the reversed-pagerank \n",
    "        \n",
    "    Returns:\n",
    "        Pagerank Scores for the nodes\n",
    "     \n",
    "    \"\"\"\n",
    "    log('[pagerank_sparse_power]\\tstarted')\n",
    "    \n",
    "    if not reverse: \n",
    "        G=G.T;\n",
    "\n",
    "    n,n=G.shape\n",
    "    c=sp.asarray(G.sum(axis=0)).reshape(-1)\n",
    "    r=sp.asarray(G.sum(axis=1)).reshape(-1)\n",
    "\n",
    "    k=c.nonzero()[0]\n",
    "\n",
    "    D=sprs.csr_matrix((1/c[k],(k,k)),shape=(n,n))\n",
    "\n",
    "    if personalize is None:\n",
    "        e=sp.ones((n,1))\n",
    "    else:\n",
    "        e = personalize/sum(personalize);\n",
    "        \n",
    "    z = (((1-p)*(c!=0) + (c==0))/n)[sp.newaxis,:]\n",
    "    G = p*G.dot(D)\n",
    "    x = e/n\n",
    "    oldx = sp.zeros((n,1));\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    while sp.linalg.norm(x-oldx) > 0.001:\n",
    "        oldx = x\n",
    "        x = G.dot(x) + e.dot(z.dot(x))\n",
    "        iteration += 1\n",
    "        if iteration >= max_iter:\n",
    "            break;\n",
    "    x = x/sum(x)\n",
    "    \n",
    "    log('# of iterations: %s, normdiff: %s', iteration, sp.linalg.norm(x-oldx))\n",
    "    log('[pagerank_sparse_power]\\tfinished')\n",
    "    return x.reshape(-1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Semantic Relatedness\n",
    "The idea is get the neighborhood graph for each concept and calculating the similarity by embedding the graph into a vector and then perforiming cosine similarity. \n",
    "\n",
    "The process can be illustrated like this:\n",
    "    ![](../resrc/alg.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile calcsim.py \n",
    "\"\"\"Calculating Relatedness.\"\"\"\n",
    "# uncomment\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "import json\n",
    "import math\n",
    "\n",
    "from wikipedia import * # uncomment\n",
    "from pagerank import * # uncomment\n",
    "from utils import * # uncomment\n",
    "\n",
    "__author__ = \"Armin Sajadi\"\n",
    "__copyright__ = \"Copyright 215, The Wikisim Project\"\n",
    "__credits__ = [\"Armin Sajadi\", \"Evangelo Milios\", \"Armin Sajadi\"]\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.1\"\n",
    "__maintainer__ = \"Armin Sajadi\"\n",
    "__email__ = \"sajadi@cs.dal.ca\"\n",
    "__status__ = \"Development\"\n",
    "\n",
    "def _unify_ids_scores(*id_sc_tuple):\n",
    "    uids, id2in = e2i(*(ids for ids, _ in id_sc_tuple));\n",
    "    \n",
    "    uscs=tuple();            \n",
    "    for ids,scs in id_sc_tuple:\n",
    "        scs_u=sp.zeros(len(id2in))\n",
    "        scs_u[[id2in[wid] for wid in ids]] = scs;            \n",
    "        uscs += (scs_u,)                \n",
    "    return uids, uscs       \n",
    "\n",
    "\n",
    "def concept_embedding(wid, direction):\n",
    "    \"\"\" Calculates concept embedding to be used in relatedness\n",
    "    \n",
    "    Args:\n",
    "        wid: wikipedia id\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "        \n",
    "    Returns:\n",
    "        The neighbor ids, their scores and the whole neighorhood graph (for visualization purposes)\n",
    "        \n",
    "    \"\"\"\n",
    "    log('[concept_embedding started]\\twid = %s, direction = %s', wid, direction)\n",
    "\n",
    "    if direction == DIR_IN or direction==DIR_OUT:\n",
    "        em = _concept_embedding_io(wid, direction)\n",
    "    if direction == DIR_BOTH:\n",
    "        em = _concept_embedding_both(wid, direction)\n",
    "    log('[concept_embedding]\\tfinished')\n",
    "    return em\n",
    "    \n",
    "def _concept_embedding_io(wid, direction):\n",
    "    cached_em = checkcache(wid, direction);\n",
    "    if cached_em is not None:\n",
    "        log('found in cache, wid = %s, direction = %s', wid, direction)\n",
    "        return cached_em;\n",
    "\n",
    "    (ids, links) = getneighbors(wid, direction);\n",
    "    if not ids:\n",
    "        return None;\n",
    "    scores = pagerank_sparse_power(create_csr(links), reverse=True)\n",
    "     \n",
    "    em=defaultdict(int,zip(ids, scores));    \n",
    "    cachescores(wid, em, direction);\n",
    "    return em\n",
    "            \n",
    "\n",
    "def _concept_embedding_both(wid, direction):            \n",
    "        in_em = _concept_embedding_io(wid, DIR_IN);\n",
    "        out_em = _concept_embedding_io(wid, DIR_OUT )\n",
    "        if (in_em is None) or (out_em is None):\n",
    "            return None;\n",
    "        \n",
    "        ids=list(set(in_em.keys()).union(out_em.keys()))\n",
    "        in_sc=[in_em[wid] for wid in ids]\n",
    "        out_sc=[out_em[wid] for wid in ids]               \n",
    "        scores=([(x+y)/2 for x,y in zip(in_sc, out_sc)])\n",
    "\n",
    "        return defaultdict(int,zip(ids, scores))\n",
    "\n",
    "def getsim_wlm(id1, id2):\n",
    "    \"\"\" Calculates wlm (ngd) similarity between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    in1 = set(getlinkedpages(id1, DIR_IN))\n",
    "    in2 = set(getlinkedpages(id2, DIR_IN))\n",
    "    f1 = len(in1)\n",
    "    f2 = len(in2)\n",
    "    f12=len(in1.intersection(in2))\n",
    "    dist = (sp.log(max(f1,f2))-sp.log(f12))/(sp.log(WIKI_SIZE)-sp.log(min(f1,f2)));\n",
    "    if (f1==0) or (f2==0) or (f12==0):\n",
    "        return 0;\n",
    "    sim = 1-dist if dist <=1 else 0\n",
    "    return sim\n",
    "\n",
    "def getsim_cocit(id1, id2):\n",
    "    \"\"\" Calculates co-citation similarity between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    in1 = set(getlinkedpages(id1, DIR_IN))\n",
    "    in2 = set(getlinkedpages(id2, DIR_IN))\n",
    "    f1 = len(in1)\n",
    "    f2 = len(in2)\n",
    "    if (f1==0) or (f2==0):\n",
    "        return 0;\n",
    "    \n",
    "    f12=len(in1.intersection(in2))\n",
    "    sim = (f12)/(f1+f2-f12);\n",
    "    return sim\n",
    "\n",
    "\n",
    "def getsim_coup(id1, id2):\n",
    "    \"\"\" Calculates coupler similarity between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    in1 = set(getlinkedpages(id1, DIR_OUT))\n",
    "    in2 = set(getlinkedpages(id2, DIR_OUT))\n",
    "    f1 = len(in1)\n",
    "    f2 = len(in2)\n",
    "    if (f1==0) or (f2==0):\n",
    "        return 0;\n",
    "    \n",
    "    f12=len(in1.intersection(in2))\n",
    "    sim = (f12)/(f1+f2-f12);\n",
    "    return sim\n",
    "\n",
    "def getsim_ams(id1, id2):\n",
    "    \"\"\" Calculates amlser similarity between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    in1 = set(getlinkedpages(id1, DIR_IN))\n",
    "    out1 = set(getlinkedpages(id1, DIR_OUT))\n",
    "    link1 = in1.union(out1)\n",
    "    \n",
    "    in2 = set(getlinkedpages(id2, DIR_IN))\n",
    "    out2 = set(getlinkedpages(id2, DIR_OUT))\n",
    "    link2 = in2.union(out2)\n",
    "    \n",
    "    f1 = len(link1)\n",
    "    f2 = len(link2)\n",
    "    if (f1==0) or (f2==0):\n",
    "        return 0;\n",
    "    \n",
    "    f12=len(link1.intersection(link2))\n",
    "    sim = (f12)/(f1+f2-f12);\n",
    "    return sim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def getsim_emb(id1,id2, direction):\n",
    "    \"\"\" Calculates the similarity between two concepts\n",
    "    Arg:\n",
    "        id1, id2: the two concepts\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "        \n",
    "    Returns:\n",
    "        The similarity score\n",
    "    \"\"\"\n",
    "    em1 = concept_embedding(id1, direction);\n",
    "    em2 = concept_embedding(id2, direction);\n",
    "    if (em1 is None) or (em2 is None):\n",
    "        return 0;\n",
    "    \n",
    "    ids=list(set(em1.keys()).union(em2.keys()))\n",
    "    sc1=[em1[wid] for wid in ids]\n",
    "    sc2=[em2[wid] for wid in ids]               \n",
    "    \n",
    "    return 1-sp.spatial.distance.cosine(sp.array(sc1),sp.array(sc2));\n",
    "\n",
    "def getsim(id1,id2, method, direction=None):\n",
    "    \"\"\" Calculates well-known similarity metrics between two concepts \n",
    "    Arg:\n",
    "        id1, id2: the two concepts \n",
    "        method:\n",
    "            wlm: Wikipedia-Miner method\n",
    "            cocit: cocitation\n",
    "            coup: coupling\n",
    "            ams: amsler\n",
    "            rvspagerank: ebedding based similarity (in our case, \n",
    "                 reversed-page rank method)\n",
    "    Returns:\n",
    "        The similarity score        \n",
    "    \"\"\"\n",
    "    if method=='rvspagerank':\n",
    "        return getsim_emb(id1,id2, direction)\n",
    "    if method=='wlm':\n",
    "        return getsim_wlm(id1,id2)\n",
    "    if method=='cocit':\n",
    "        return getsim_cocit(id1,id2)\n",
    "    if method=='coup':\n",
    "        return getsim_coup(id1,id2)\n",
    "    if method=='ams':\n",
    "        return getsim_ams(id1,id2)\n",
    "\n",
    "    \n",
    "def getsim_file(infilename, outfilename, method='rvspagerank', direction=None):\n",
    "    \"\"\" Batched (file) similarity.\n",
    "    \n",
    "    Args: \n",
    "        infilename: tsv file in the format of pair1    pair2   [goldstandard]\n",
    "        outfilename: tsv file in the format of pair1    pair2   similarity\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "    Returns:\n",
    "        vector of scores, and Spearmans's correlation if goldstandard is given\n",
    "    \"\"\"\n",
    "    log('[getsim_file started]\\t%s -> %s', infilename, outfilename)\n",
    "    outfile = open(outfilename, 'w');\n",
    "    dsdata=readds(infilename);\n",
    "    gs=[];\n",
    "    scores=[];\n",
    "    #scores=[1-spatial.distance.cosine(vectors[row[0]],vectors[row[1]]) if (row[0] in vectors) and  (row[1] in vectors) else 0 for row in dsdata]\n",
    "    spcorr=None;\n",
    "    for row in dsdata:   \n",
    "        log('processing %s, %s', row[0], row[1])\n",
    "        if (row[0]=='null') or (row[1]=='null'):\n",
    "            continue;\n",
    "        if len(row)>2: \n",
    "            gs.append(row[2]);\n",
    "            \n",
    "        wid1 = title2id(row[0])\n",
    "        wid2 = title2id(row[1])\n",
    "        if (wid1 is None) or (wid2 is None):\n",
    "            sim=0;\n",
    "        else:\n",
    "            sim=getsim(wid1, wid2, method, direction);\n",
    "        outfile.write(\"\\t\".join([str(row[0]), str(row[1]), str(sim)])+'\\n')\n",
    "        scores.append(sim)\n",
    "    outfile.close();\n",
    "    if gs:\n",
    "        spcorr = sp.stats.spearmanr(scores, gs);\n",
    "    log('[getsim_file]\\tfinished')\n",
    "    return scores, spcorr\n",
    "\n",
    "def conceptrep(wid, direction, get_titles=True, cutoff=None):\n",
    "    \"\"\" Finds a representation for a concept\n",
    "    \n",
    "        Concept Representation is a vector of concepts with their score\n",
    "    Arg:\n",
    "        wid: Wikipedia id\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "        titles: include titles in the embedding (not needed for mere calculations)\n",
    "        cutoff: the first top cutoff dimensions (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        the vecotr of ids, their titles and theirs scores. It also returns the\n",
    "        graph for visualization purposes. \n",
    "    \"\"\"\n",
    "    \n",
    "    log('[conceptrep started]\\twid = %s, direction = %s', wid, direction)\n",
    "    \n",
    "    em=concept_embedding(wid, direction);    \n",
    "    if em is None:\n",
    "        return None;\n",
    "    ids = em.keys();\n",
    "    if cutoff is not None:\n",
    "        ids = sorted(em.keys(), key=lambda k: em[k], reverse=True)\n",
    "        ids=ids[:cutoff]\n",
    "        em=defaultdict(int, {wid:em[wid] for wid in ids})\n",
    "        \n",
    "    if get_titles:\n",
    "        em=defaultdict(int, {wid:(title, em[wid]) for wid,title in zip(ids,ids2title(ids))})\n",
    "    log ('[conceptrep]\\tfinished')\n",
    "    return em\n",
    "    \n",
    "\n",
    "def getembed_file(infilename, outfilename, direction, get_titles=False, cutoff=None):\n",
    "    \"\"\" Batched (file) concept representation.\n",
    "    \n",
    "    Args: \n",
    "        infilename: tsv file in the format of pair1    pair2   [goldstandard]\n",
    "        outfilename: tsv file in the format of pair1    pair2   similarity\n",
    "        direction: 0 for in, 1 for out, 2 for all\n",
    "        titles: include titles in the embedding (not needed for mere calculations)\n",
    "        cutoff: the first top cutoff dimensions (None for all)        \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    log('[getembed_file started]\\t%s -> %s', infilename, outfilename)\n",
    "    outfile = open(outfilename, 'w');\n",
    "    dsdata=readds(infilename);\n",
    "    scores=[];\n",
    "    for row in dsdata:        \n",
    "        wid = title2id(row[0])\n",
    "        if wid is None:\n",
    "            em='';\n",
    "        else:\n",
    "            em=conceptrep(wid, direction, get_titles, cutoff)\n",
    "        outfile.write(row[0]+\"\\t\"+json.dumps(em)+\"\\n\")\n",
    "    outfile.close();\n",
    "    log('[getembed_file]\\tfinished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%aimport calcsim\n",
    "\n",
    "%aimport wikipedia\n",
    "\n",
    "from wikipedia import * # uncomment\n",
    "from calcsim import *   # uncomment\n",
    "# Examples\n",
    "reopen()\n",
    "direction = DIR_IN\n",
    "\n",
    "page_title1 = 'Banana_bag' \n",
    "print ('page_title: ', page_title1)\n",
    "\n",
    "page_id1 = title2id(page_title1)\n",
    "print (\"id: \", page_id1)\n",
    "\n",
    "sr1 = synonymring_titles(page_id1)\n",
    "print (\"synonym ring: %s\\n \" % str(sr1[:5]))\n",
    "\n",
    "rep1=conceptrep(page_id1, direction,  get_titles=True, cutoff=5)\n",
    "print (\"Concept Representation:  %s\\n\" % json.dumps(rep1))\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "page_title2 = 'Miscarriage' \n",
    "print ('page_title: ', page_title2)\n",
    "\n",
    "page_id2 = title2id(page_title2)\n",
    "print (\"id: \", page_id2)\n",
    "\n",
    "sr2 = synonymring_titles(page_id2)\n",
    "print (\"synonym ring: %s\\n \" % str(sr2[:5]))\n",
    "\n",
    "rep2=conceptrep(page_id2, direction,  get_titles=True, cutoff=5)\n",
    "print (\"Concept Representation: %s\\n\" % json.dumps(rep2))\n",
    "\n",
    "\n",
    "\n",
    "sim = getsim(page_id1, page_id2,'rvspagerank',DIR_IN)\n",
    "print (\"similarity\", sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from calcsim import *\n",
    "\n",
    "import json\n",
    "from IPython.display import Javascript\n",
    "\n",
    "cre1 = conceptrep(title2id('Tehran'), DIR_OUT, get_titles=True, cutoff=200);\n",
    "cre2 = conceptrep(title2id('Sanandaj'), DIR_OUT, get_titles=True, cutoff=200);\n",
    "\n",
    "\n",
    "#runs arbitrary javascript, client-side\n",
    "Javascript(\"\"\"\n",
    "           window.vizObj1={};window.vizObj2={};\n",
    "           \"\"\".format(json.dumps(cre1), json.dumps(cre2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "require.config({\n",
    "    paths: {\n",
    "        d3:'//129.173.212.50/~sajadi/wikisim/js/d3',\n",
    "        d3_cloud:'//129.173.212.50/~sajadi/wikisim/js/d3.layout.cloud',\n",
    "        simple_draw:'//129.173.212.50/~sajadi/wikisim/js/simpledraw'\n",
    "\n",
    "    }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "function createWords(cp){\n",
    "\n",
    "    var titles=[];\n",
    "    var scores=[];\n",
    "\n",
    "    for (var key in cp){ \n",
    "        if (cp.hasOwnProperty(key)) {\n",
    "            titles.push(cp[key][0])\n",
    "            scores.push(cp[key][1])\n",
    "        }\n",
    "    }\n",
    "    var sum = scores.reduce(function(a, b) {return a + b;});\n",
    "    var min = Math.min.apply(null, scores)\n",
    "    var max = Math.max.apply(null, scores)\n",
    "    \n",
    "    scores=scores.map(function(a){return (a/sum)*90+20});\n",
    "    var words=[];\n",
    "    for (var i = 0; i<titles.length; i++) {\n",
    "        words.push({\"text\":titles[i], \"size\": scores[i]})\n",
    "    }\n",
    "    return words;\n",
    "}\n",
    "\n",
    "var words1=createWords(window.vizObj1);\n",
    "//element.text(JSON.stringify(words1));\n",
    "var words2=createWords(window.vizObj2);\n",
    "require(['d3','d3_cloud', 'simple_draw'], function(d3,d3_cloud, simple_draw){\n",
    "    $(\"#chart1\").remove();\n",
    "    element.append(\"<div id='chart1' style='width:49%; height:500px; float:left; border-style:solid'> </div>\");\n",
    "    simpledraw(words1, chart1);\n",
    "    \n",
    "    $(\"#chart2\").remove();\n",
    "    element.append(\"<div id='chart2' style='width:49%; margin-left:2%; height:500px; float:left; border-style:solid'> </div>\");\n",
    "    simpledraw(words2, '#chart2');    \n",
    "    \n",
    "});    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
