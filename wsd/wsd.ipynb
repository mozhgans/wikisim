{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "** Local and Global Algorithms ... **\n",
    "\n",
    "AQUAINT: Milne\n",
    "\n",
    "MSNBC dataset, taken from (Cucerzan, 2007),\n",
    "\n",
    "ACE: Mechanical Turkn\n",
    "\n",
    "Wiki: choose those paragraphts that p(t|m) makes atleast 10% error\n",
    "\n",
    "For evaluation, check BOT evaluation, mentioned in Milne \n",
    "\n",
    "Downloadable from :\n",
    "http://cogcomp.cs.illinois.edu/page/resource_view/4\n",
    "\n",
    "\n",
    "**Spotlight**\n",
    "two datasets, a wiki selection\n",
    "35 paragraphs from New York times\n",
    "There is a website, but couldn't find it\n",
    "\n",
    "http://oldwiki.dbpedia.org/Datasets/NLP\n",
    "\n",
    "Tag me:\n",
    "Wiki and tweet, \n",
    "available, but looks old!\n",
    "http://acube.di.unipi.it/tagme-dataset/\n",
    "\n",
    "**AIDA**\n",
    "\n",
    ": https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/aida/downloads/**\n",
    "AIDA CoNLL-YAGO Dataset: Hnad create from Conll\n",
    "AIDA-EE Dataset: Again hand done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wsd_util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wsd_util.py \n",
    "import sys\n",
    "from itertools import chain\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "sys.path.insert(0,'..')\n",
    "from wikisim.config import *\n",
    "\n",
    "from wikisim.calcsim import *\n",
    "\n",
    "def generate_candidates(S, M, max_t=10, enforce=False):\n",
    "    \"\"\" Given a sentence list (S) and  a mentions list (M), returns a list of candiates\n",
    "        Inputs:\n",
    "            S: segmented sentence [w1, ..., wn]\n",
    "            M: mensions [m1, ... , mj]\n",
    "            max_t: maximum candiate per mention\n",
    "            enforce: Makes sure the \"correct\" entity is among the candidates\n",
    "        Outputs:\n",
    "         Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "             where cij is the jth candidate for ith mention and pij is the relative frequency of cij\n",
    "    \n",
    "    \"\"\"\n",
    "    candslist=[]\n",
    "    for m in M:\n",
    "        \n",
    "        clist = anchor2concept(S[m[0]])\n",
    "        if not clist:\n",
    "            clist=((0L,1L),)\n",
    "        \n",
    "        clist = sorted(clist, key=lambda x: -x[1])\n",
    "        clist = clist[:max_t]\n",
    "        \n",
    "        smooth=0    \n",
    "        if enforce:          \n",
    "            wid = title2id(m[1])            \n",
    "    #         if wid is None:\n",
    "    #             raise Exception(m[1].encode('utf-8') + ' not found')\n",
    "            \n",
    "                        \n",
    "            trg = [(i,(c,f)) for i,(c,f) in enumerate(clist) if c==wid]\n",
    "            if not trg:\n",
    "                trg=[(len(clist), (wid,0))]\n",
    "                smooth=1\n",
    "\n",
    "                \n",
    "            if smooth==1 or trg[0][0]>=max_t: \n",
    "                if clist:\n",
    "                    clist.pop()\n",
    "                clist.append(trg[0][1])\n",
    "            \n",
    "        s = sum(c[1]+smooth for c in clist )        \n",
    "        clist = [(c,float(f+smooth)/s) for c,f in clist ]\n",
    "            \n",
    "        candslist.append(clist)\n",
    "    return  candslist \n",
    "\n",
    "def disambiguate_popular(C):\n",
    "    ids = [c[0][0] for c in C ]\n",
    "    titles= ids2title(ids)\n",
    "    return ids, titles\n",
    "\n",
    "def get_sim_matrix(candslist,method, direction):\n",
    "    concepts=  list(chain(*candslist))\n",
    "    concepts=  list(set(c[0] for c in concepts))\n",
    "    sims = pd.DataFrame(index=concepts, columns=concepts)\n",
    "    for cands1,cands2 in combinations(candslist,2):\n",
    "        for c1,c2 in product(cands1,cands2):\n",
    "            sims[c1[0]][c2[0]]= sims[c2[0]][c1[0]] = getsim(c1[0],c2[0] , method, direction)\n",
    "    return sims     \n",
    "\n",
    "def get_tp(gold_titles, ids):\n",
    "    tp=0\n",
    "    for m,id2 in zip(gold_titles, ids):\n",
    "        if title2id(m[1]) == id2:\n",
    "            tp += 1\n",
    "    return [tp, len(ids)]\n",
    "\n",
    "def get_prec(tp_list):\n",
    "    overall_tp = 0\n",
    "    simple_count=0\n",
    "    overall_count=0\n",
    "    macro_prec = 0;\n",
    "    for tp, count in tp_list:\n",
    "        if tp is None:\n",
    "            continue\n",
    "        simple_count +=1    \n",
    "        overall_tp += tp\n",
    "        overall_count += count\n",
    "        macro_prec += float(tp)/count\n",
    "        \n",
    "    macro_prec = macro_prec/simple_count\n",
    "    micro_prec = float(overall_tp)/overall_count\n",
    "    \n",
    "    return micro_prec, macro_prec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wsd.py \n",
    "\"\"\" Evaluating the method on Semantic Relatedness Datasets.\"\"\"\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time;\n",
    "import json \n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "        \n",
    "\n",
    "\n",
    "from wsdcoherence import *\n",
    "from wsdvsm import *\n",
    "\n",
    "#reopen()\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "def disambiguate(C, method, direction, op_method):\n",
    "    \"\"\" Disambiguate C list using a disambiguation method \n",
    "        Inputs:\n",
    "            C: Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            method: similarity method\n",
    "            direction: embedding type\n",
    "            op_method: disambiguation method \n",
    "                        most important ones: ilp (integer linear programming), \n",
    "                                             key: Key Entity based method\n",
    "        \n",
    "    \"\"\"\n",
    "    if op_method == 'popularity':\n",
    "        return disambiguate_popular(C)\n",
    "    if op_method == 'ilp':\n",
    "        return disambiguate_ilp(C, method, direction)\n",
    "    if op_method == 'ilp2':\n",
    "        return disambiguate_ilp_2(C, method, direction)\n",
    "    if op_method == 'keyq':\n",
    "        return key_quad(C, method, direction)\n",
    "    if op_method == 'pkeyq':\n",
    "        return Pkey_quad(C, method, direction)\n",
    "    if  op_method == 'simplecontext'  :\n",
    "        return simple_entity_context_disambiguate(C, direction, method)\n",
    "    if  op_method == 'context2'  :\n",
    "        return contextdisamb_2(C, direction)\n",
    "    if  op_method == 'context3'  :\n",
    "        return contextdisamb_3(C, direction)\n",
    "    if  op_method == 'entitycontext'  :\n",
    "        return entity_context_disambiguate(C, direction, method)\n",
    "\n",
    "        \n",
    "    if  op_method == 'context4_1'  :\n",
    "        return keyentity_disambiguate(C, direction, method, 1)\n",
    "    if  op_method == 'context4_2'  :\n",
    "        return keyentity_disambiguate(C, direction, method, 2)\n",
    "    if  op_method == 'context4_3'  :\n",
    "        return keyentity_disambiguate(C, direction, method, 3)    \n",
    "    if  op_method == 'keydisamb'  :\n",
    "        return keyentity_disambiguate(C, direction, method, 4)\n",
    "    \n",
    "    if  op_method == 'tagme'  :\n",
    "        return tagme(C, method, direction)\n",
    "    if  op_method == 'tagme2'  :\n",
    "        return tagme(C, method, direction, True)\n",
    "    \n",
    "\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def disambiguate_driver(C, ws, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\"):\n",
    "    \"\"\" Initiate the disambiguation by chunking the sentence \n",
    "        Disambiguate C list using a disambiguation method \n",
    "        Inputs:\n",
    "            C: Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            ws: Windows size for chunking\n",
    "            method: similarity method\n",
    "            direction: embedding type\n",
    "            op_method: disambiguation method \n",
    "                        most important ones: ilp (integer linear programming), \n",
    "                                             keyq: Key Entity based method\n",
    "        \n",
    "    \"\"\"\n",
    "    #TODO: modify this chunking to an overlapping version\n",
    "    if ws == 0: \n",
    "        return  disambiguate(C, method, direction, op_method)\n",
    "    \n",
    "    ids = []\n",
    "    titles = []\n",
    "    \n",
    "    windows = [[start, min(start+ws, len(C))] for start in range(0,len(C),ws) ]\n",
    "    last = len(windows)\n",
    "    if last > 1 and windows[last-1][1]-windows[last-1][0]<2:\n",
    "        windows[last-2][1] = len(C)\n",
    "        windows.pop()\n",
    "        \n",
    "    for w in windows:\n",
    "        chunk_c = C[w[0]:w[1]]\n",
    "        \n",
    "        chunk_ids, chunk_titles = disambiguate(chunk_c, method, direction, op_method)\n",
    "        ids += chunk_ids\n",
    "        titles += chunk_titles\n",
    "    return ids, titles     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Integer Programming\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence and Key-Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wsdcoherence.py \n",
    "\n",
    "from itertools import izip\n",
    "from pulp import *\n",
    "import random\n",
    "from itertools import chain\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "\n",
    "from wsd_util import *\n",
    "\n",
    "def getscore(x,y,method, direction):\n",
    "    \"\"\"Get similarity score for a method and a direction \"\"\"\n",
    "    x = encode_entity(x, method, get_id=False)\n",
    "    y = encode_entity(y, method, get_id=False)\n",
    "    return getsim(x,y ,method, direction)\n",
    "    #return random.random()\n",
    "\n",
    "def disambiguate_ilp(C, method, direction):\n",
    "    \"\"\" Disambiguate using ILP \n",
    "        Inputs: \n",
    "            C: Candidate List [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            method: Similarity method\n",
    "            direction: embedding direction\"\"\"\n",
    "    #C = [('a','b','c'), ('e', 'f', 'g'), ('h', 'i')]\n",
    "\n",
    "    R1 = [zip([i]*len(c), range(len(c))) for i,c in enumerate(C)]\n",
    "\n",
    "    #R1 = {r:str(r) for r in itertools.chain(*RI1)}\n",
    "    #R1 = [[str(rij) for rij in ri] for ri in RI1]\n",
    "\n",
    "    #RI1_flat = list(itertools.chain(*RI1))\n",
    "\n",
    "\n",
    "    R2=[]\n",
    "    for e in combination(R1,2):\n",
    "        R2 += [r for r in itertools.product(e[0], e[1]) ]        \n",
    "\n",
    "\n",
    "    #R2 = {r:str(r) for r in RI2}\n",
    "\n",
    "\n",
    "    \n",
    "    S = {((u0,u1),(v0,v1)):getscore(C[u0][u1][0],C[v0][v1][0], method, direction) for ((u0,u1),(v0,v1)) in R2}\n",
    "\n",
    "\n",
    "    prob = LpProblem(\"wsd\", LpMaximize)\n",
    "\n",
    "    R=list(itertools.chain(*R1)) + R2\n",
    "    R_vars = LpVariable.dicts(\"R\",R,\n",
    "                                lowBound = 0,\n",
    "                                upBound = 1,\n",
    "                                cat = pulp.LpInteger)\n",
    "    prob += lpSum([S[r]*R_vars[r] for r in R2])\n",
    "\n",
    "\n",
    "    i=0\n",
    "    for ri in R1:\n",
    "        prob += lpSum([R_vars[rij] for rij in ri])==1, (\"R1 %s constraint\")%i\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    for r in R2:\n",
    "        prob += lpSum([R_vars[r[0]],R_vars[r[1]],-2*R_vars[r]]) >=0, (\"R_%s_%s constraint\"%(r[0], r[1]))\n",
    "\n",
    "    prob.solve() \n",
    "    #print(\"Status:\", LpStatus[prob.status])\n",
    "    #print(\"Score:\", value(prob.objective))\n",
    "    ids    = [C[r[0]][r[1]][0] for r in list(itertools.chain(*R1)) if R_vars[r].value() == 1.0]\n",
    "    titles = ids2title(ids)\n",
    "    return ids, titles\n",
    "        \n",
    "def disambiguate_ilp_2(C, method, direction):\n",
    "    \n",
    "    #C = [('a','b','c'), ('e', 'f', 'g'), ('h', 'i')]\n",
    "\n",
    "    #R1 = [zip([\"R\"z]*len(c),zip([i]*len(c), range(len(c)))) for i,c in enumerate(C)]\n",
    "    R1 = [zip(['R']*len(c),[i]*len(c), range(len(c))) for i,c in enumerate(C)]\n",
    "    Q = [zip(['Q']*len(c),[i]*len(c), range(len(c))) for i,c in enumerate(C)]\n",
    "\n",
    "    #R1 = {r:str(r) for r in itertools.chain(*RI1)}\n",
    "    #R1 = [[str(rij) for rij in ri] for ri in RI1]\n",
    "\n",
    "    #RI1_flat = list(itertools.chain(*RI1))\n",
    "\n",
    "\n",
    "    R2=[]\n",
    "    for e in combination(R1,2):\n",
    "            R2 += [('R',(i,k),(j,l)) for (_,i,k),(_,j,l) in itertools.product(e[0], e[1]) ]        \n",
    "\n",
    "\n",
    "    #R2 = {r:str(r) for r in RI2}\n",
    "\n",
    "\n",
    "\n",
    "    S = {('R',(i,k),(j,l)):getscore(C[i][k][0],C[j][l][0], method, direction) for _,(i,k),(j,l) in R2}\n",
    "\n",
    "\n",
    "    prob = LpProblem(\"wsd\", LpMaximize)\n",
    "\n",
    "    R=list(itertools.chain(*R1)) + R2\n",
    "    Q=list(itertools.chain(*Q))\n",
    "    R_vars = LpVariable.dicts(\"R\",R,\n",
    "                                lowBound = 0,\n",
    "                                upBound = 1,\n",
    "                                cat = pulp.LpInteger)\n",
    "\n",
    "    Q_vars = LpVariable.dicts(\"Q\",Q,\n",
    "                                lowBound = 0,\n",
    "                                upBound = 1,\n",
    "                                cat = pulp.LpInteger)\n",
    "\n",
    "    prob += lpSum([S[r]*R_vars[r] for r in R2])\n",
    "\n",
    "\n",
    "    i=0\n",
    "    for ri in R1:\n",
    "        prob += lpSum([R_vars[rij] for rij in ri])==1, (\"R1 %s constraint\")%i\n",
    "        i += 1\n",
    "\n",
    "    prob += lpSum(Q_vars.values())==1, (\"Q constraint\")\n",
    "\n",
    "\n",
    "    for _,(i,k),(j,l) in R2:\n",
    "        prob += lpSum([R_vars[('R',i,k)],R_vars[('R',j,l)],-2*R_vars[('R',(i,k),(j,l))]]) >=0, (\"R_%s_%s constraint\"%((i,k),(j,l)))\n",
    "        prob += lpSum([Q_vars[('Q',i,k)],Q_vars[('Q',j,l)], -1*R_vars[('R',(i,k),(j,l))]]) >=0, (\"Q_%s_%s constraint\"%((i,k),(j,l)))\n",
    "\n",
    "    prob.solve() \n",
    "#     print(\"Status:\", LpStatus[prob.status])\n",
    "#     print(\"Score:\", value(prob.objective))\n",
    "\n",
    "#     for (_,i,k), q in Q_vars.items():\n",
    "#         if q.value()==1:\n",
    "#             print(\"central concept: \", id2title(C[i][k][0]))\n",
    "    ids    = [C[i][k][0] for _,i,k in list(itertools.chain(*R1)) if R_vars[('R',i,k)].value() == 1.0]\n",
    "    \n",
    "    \n",
    "    titles = ids2title(ids)\n",
    "    return ids, titles\n",
    "\n",
    "# key\n",
    "def evalkey(c, a, candslist, simmatrix):\n",
    "    resolved=[]\n",
    "    score=0;\n",
    "    for i in  range(len(candslist)):\n",
    "        if a==i:\n",
    "            resolved.append(c[0])\n",
    "            continue\n",
    "        cands = candslist[i]\n",
    "        vb = [(cj[0], simmatrix[c[0]][cj[0]])  for cj in cands]\n",
    "        max_concept, max_sc = max(vb, key=lambda x: x[1])\n",
    "        score += max_sc\n",
    "        resolved.append(max_concept)\n",
    "    return resolved,score\n",
    "\n",
    "def key_quad(candslist, method, direction):\n",
    "    res_all=[]\n",
    "    simmatrix = get_sim_matrix(candslist, method, direction)\n",
    "\n",
    "    for i in range(len(candslist)):\n",
    "        for j in range(len(candslist[i])):\n",
    "            res_ij =  evalkey(candslist[i][j], i, candslist, simmatrix)\n",
    "            res_all.append(res_ij)\n",
    "    res, score = max(res_all, key=lambda x: x[1])\n",
    "    #print(\"Score:\", score)\n",
    "    titles = ids2title(res)\n",
    "    return res, titles\n",
    "\n",
    "# Parallel Keyquad\n",
    "from functools import partial\n",
    "from multiprocessing import Pool as ThreadPool \n",
    "def Pevalkey((c, a), candslist, simmatrix):\n",
    "    resolved=[]\n",
    "    score=0;\n",
    "    for i in  range(len(candslist)):\n",
    "        if a==i:\n",
    "            resolved.append(c[0])\n",
    "            continue\n",
    "        cands = candslist[i]\n",
    "        vb = [(cj[0], simmatrix[c[0]][cj[0]])  for cj in cands]\n",
    "        max_concept, max_sc = max(vb, key=lambda x: x[1])\n",
    "        score += max_sc\n",
    "        resolved.append(max_concept)\n",
    "    return resolved,score\n",
    "def Pkey_quad(candslist, method, direction):\n",
    "    res_all=[]\n",
    "    simmatrix = get_sim_matrix(candslist, method, direction)\n",
    "    pool = ThreadPool(25) \n",
    "    \n",
    "    partial_evalkey = partial(Pevalkey, candslist=candslist, simmatrix=simmatrix)\n",
    "    I=[[j]*len(candslist[j]) for j in range(len(candslist))]\n",
    "    \n",
    "    res_all= pool.map(partial_evalkey, zip(itertools.chain(*candslist), itertools.chain(*I)))\n",
    "    pool.close() \n",
    "    pool.join() \n",
    "    \n",
    "    res, score = max(res_all, key=lambda x: x[1])\n",
    "    titles = ids2title(res)\n",
    "    return res, titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Model-Based Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wsdvsm.py \n",
    "\n",
    "\n",
    "from wsd_util import *\n",
    "\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from itertools import combinations\n",
    "from itertools import product\n",
    "\n",
    "def contextdisamb_2(candslist, direction=DIR_OUT):\n",
    "    cframelist=[]\n",
    "    cveclist_bdrs = []\n",
    "    for cands in candslist:\n",
    "        cands_rep = [conceptrep(c[0], direction=direction, get_titles=False) for c in cands]\n",
    "        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n",
    "        cframelist += cands_rep\n",
    "\n",
    "    #print \"ambig_count:\", ambig_count\n",
    "    cvec_fr = pd.concat(cframelist, join='outer', axis=1)\n",
    "    cvec_fr.fillna(0, inplace=True)\n",
    "    cvec_arr = cvec_fr.as_matrix().T\n",
    "    i=0\n",
    "    for cframe in cframelist:\n",
    "        if cframe.empty:\n",
    "            cvec_arr = np.insert(cvec_arr,i,0, axis=0)\n",
    "        i+=1    \n",
    "    \n",
    "    aggr_cveclist = np.zeros(shape=(len(candslist),cvec_arr.shape[1]))\n",
    "    for i in range(len(cveclist_bdrs)):\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        aggr_cveclist[i]=cvec_arr[b:e].sum(axis=0)\n",
    "    \n",
    "    res=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        convec=aggr_cveclist[:i].sum(axis=0) + aggr_cveclist[i+1:].sum(axis=0)\n",
    "\n",
    "        maxd=-1\n",
    "        index = -1\n",
    "        mi=0\n",
    "\n",
    "        for v in cvec:\n",
    "            d = 1-sp.spatial.distance.cosine(convec, v);\n",
    "            if d>maxd:\n",
    "                maxd=d\n",
    "                index=mi\n",
    "            mi +=1\n",
    "        if index==-1:\n",
    "            index=0\n",
    "        res.append(cands[index][0]) \n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cveclist_bdrs[i] = (b+index,b+index+1)\n",
    "        \n",
    "        aggr_cveclist[i] =  cvec_arr[b:e][index]\n",
    "        \n",
    "        candslist[i] = candslist[i][index][0]\n",
    "        \n",
    "        \n",
    "\n",
    "    titles = ids2title(res)\n",
    "\n",
    "    return res, titles\n",
    "\n",
    "def contextdisamb_3(candslist, direction=DIR_OUT):\n",
    "    cframelist=[]\n",
    "    cveclist_bdrs = []\n",
    "    ambig_count=0\n",
    "    for cands in candslist:\n",
    "        if len(candslist)>1:\n",
    "            ambig_count += 1\n",
    "        cands_rep = [conceptrep(c[0], direction=direction, get_titles=False) for c in cands]\n",
    "        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n",
    "        cframelist += cands_rep\n",
    "\n",
    "    #print \"ambig_count:\", ambig_count\n",
    "        \n",
    "    cvec_fr = pd.concat(cframelist, join='outer', axis=1)\n",
    "    cvec_fr.fillna(0, inplace=True)\n",
    "    cvec_arr = cvec_fr.as_matrix().T\n",
    "    i=0\n",
    "    for cframe in cframelist:\n",
    "        if cframe.empty:\n",
    "            cvec_arr = np.insert(cvec_arr,i,0, axis=0)\n",
    "        i+=1    \n",
    "    \n",
    "    aggr_cveclist = np.zeros(shape=(len(candslist),cvec_arr.shape[1]))\n",
    "    for i in range(len(cveclist_bdrs)):\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        aggr_cveclist[i]=cvec_arr[b:e].sum(axis=0)\n",
    "    from itertools import izip\n",
    "    resolved = 0\n",
    "    for resolved in range(ambig_count):\n",
    "        cands_score_list=[]        \n",
    "        for i in range(len(candslist)):\n",
    "            cands = candslist[i]\n",
    "            b,e = cveclist_bdrs[i]\n",
    "            cvec = cvec_arr[b:e]\n",
    "            convec=aggr_cveclist[:i].sum(axis=0) + aggr_cveclist[i+1:].sum(axis=0)\n",
    "            D=[]    \n",
    "            for v in cvec:\n",
    "                d = 1-sp.spatial.distance.cosine(convec, v);\n",
    "                if np.isnan(d):\n",
    "                    d=0\n",
    "                D.append(d)\n",
    "            D=sorted(enumerate(D), key=lambda x: -x[1])\n",
    "            cands_score_list.append(D)\n",
    "\n",
    "        max_concept, _ = max(enumerate(cands_score_list), key=lambda x: (x[1][0][1]-x[1][1][1]) if len(x[1])>1 else -1)\n",
    "        max_candidate = cands_score_list[max_concept][0][0]\n",
    "        \n",
    "        b,e = cveclist_bdrs[max_concept]\n",
    "        cveclist_bdrs[max_concept] = (b+max_concept,b+max_concept+1)\n",
    "        aggr_cveclist[max_concept] =  cvec_arr[b:e][max_candidate]\n",
    "        \n",
    "        candslist[max_concept] = [candslist[max_concept][max_candidate]]\n",
    "                                  \n",
    "        #cframelist[max_index] =  [cframelist[max_index][cands_score_list[max_index][0][0]]]\n",
    "        #break\n",
    "            #print index,\"\\n\"\n",
    "    res = [c[0][0] for c in candslist]\n",
    "    titles = ids2title(res)\n",
    "\n",
    "    return res, titles        \n",
    "    \n",
    "    \n",
    "#########################\n",
    "# KeyBased Method\n",
    "#########################\n",
    "# def simple_context_coherence(candslist, direction, method):\n",
    "#     cvec_arr, cveclist_bdrs =  get_candidate_representations(candslist, direction, method)        \n",
    "#     convec = cvec_arr.sum(axis=0)\n",
    "#     from itertools import izip\n",
    "#     res=[]\n",
    "#     for i in range(len(candslist)):\n",
    "#         cands = candslist[i]\n",
    "#         b,e = cveclist_bdrs[i]\n",
    "#         cvec = cvec_arr[b:e]\n",
    "        \n",
    "#         maxd=-1\n",
    "#         mi=0\n",
    "#         for v in cvec:\n",
    "#             d = 1-sp.spatial.distance.cosine(convec, v);\n",
    "#             if d>maxd:\n",
    "#                 maxd=d\n",
    "#                 index=mi\n",
    "#             mi +=1\n",
    "#         res.append(cands[index][0]) \n",
    "#         #print index,\"\\n\"\n",
    "#     titles = ids2title(res)\n",
    "#     return res, titles\n",
    "\n",
    "def coherence_scores_driver(C, ws, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\"):\n",
    "    \"\"\" Assigns a score to every candidate \n",
    "        Inputs:\n",
    "            C: Candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            ws: Windows size for chunking\n",
    "            method: similarity method\n",
    "            direction: embedding type\n",
    "            op_method: disambiguation method, either keyentity or entitycontext\n",
    "            \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    windows = [[start, min(start+ws, len(C))] for start in range(0,len(C),ws) ]\n",
    "    last = len(windows)\n",
    "    if last > 1 and windows[last-1][1]-windows[last-1][0]<2:\n",
    "        windows[last-2][1] = len(C)\n",
    "        windows.pop()\n",
    "    scores=[]    \n",
    "    for w in windows:\n",
    "        chunk_c = C[w[0]:w[1]]\n",
    "        if op_method == 'keydisamb':\n",
    "            scores += keyentity_candidate_scores(chunk_c, direction, method,4)\n",
    "            \n",
    "        if op_method == 'simplecontext':\n",
    "            _, _, candslist_scores = simple_entity_to_context_scores(chunk_c, direction, method);\n",
    "            scores += candslist_scores\n",
    "            \n",
    "        if op_method == 'entitycontext':\n",
    "            _, _, candslist_scores = entity_to_context_scores(chunk_c, direction, method);\n",
    "            scores += candslist_scores\n",
    "            \n",
    "    return scores\n",
    "\n",
    "def get_candidate_representations(candslist, direction, method):\n",
    "    '''returns an array of vector representations. \n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "      Outputs\n",
    "           cvec_arr: Candidate embeddings, a two dimensional array, each column \n",
    "                   is the representation of a candidate\n",
    "           cveclist_bdrs: a list of pairs (beginning, end), to indicate where \n",
    "                   the embeddings for a concepts indicates start and end. In other words\n",
    "                   The embedding of candidates [ci1...cik] in candslist is\n",
    "                   cvec_arr[cveclist_bdrs[i][0]:cveclist_bdrs[i][1]] \n",
    "    '''\n",
    "    \n",
    "    cframelist=[]\n",
    "    cveclist_bdrs = []\n",
    "    ambig_count=0\n",
    "    for cands in candslist:\n",
    "        if len(candslist)>1:\n",
    "            ambig_count += 1\n",
    "        cands_rep = [conceptrep(encode_entity(c[0], method, get_id=False), method=method, direction=direction, get_titles=False) for c in cands]\n",
    "        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n",
    "        cframelist += cands_rep\n",
    "\n",
    "    #print \"ambig_count:\", ambig_count\n",
    "        \n",
    "    cvec_fr = pd.concat(cframelist, join='outer', axis=1)\n",
    "    cvec_fr.fillna(0, inplace=True)\n",
    "    cvec_arr = cvec_fr.as_matrix().T\n",
    "    return cvec_arr, cveclist_bdrs\n",
    "\n",
    "\n",
    "def simple_entity_to_context_scores(candslist, direction, method):\n",
    "    ''' finds the similarity between each entity and its context representation\n",
    "        Inputs:\n",
    "            candslist: the list of candidates [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            cvec_arr: the array of all embeddings for the candidates\n",
    "            cveclist_bdrs: The embedding vector for each candidate: [[c11,...c1k],...[cn1,...c1m]]\n",
    "        Returns:\n",
    "           cvec_arr: Candidate embeddings, a two dimensional array, each column \n",
    "           cveclist_bdrs: a list of pairs (beginning, end), to indicate where the \n",
    "                   reperesentation of the candidates for cij reside        \n",
    "           cands_score_list: scroes in the form of [[s11,...s1k],...[sn1,...s1m]]\n",
    "                    where sij  is the similarity of c[i,j] to to ci-th context\n",
    "                    \n",
    "            '''\n",
    "    cvec_arr, cveclist_bdrs =  get_candidate_representations(candslist, direction, method)    \n",
    "    convec = cvec_arr.sum(axis=0)\n",
    "    \n",
    "    #aggr_cveclist = np.zeros(shape=(len(candslist),cvec_arr.shape[1]))\n",
    "    #for i in range(len(cveclist_bdrs)):\n",
    "        #b,e = cveclist_bdrs[i]\n",
    "        #aggr_cveclist[i]=cvec_arr[b:e].sum(axis=0)\n",
    "    \n",
    "    from itertools import izip\n",
    "    #resolved = 0\n",
    "    cands_score_list=[]        \n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        #convec=aggr_cveclist[:i].sum(axis=0) + aggr_cveclist[i+1:].sum(axis=0)\n",
    "        S=[]    \n",
    "        for v in cvec:\n",
    "            try:\n",
    "                s = 1-sp.spatial.distance.cosine(convec, v);\n",
    "            except:\n",
    "                s=0                \n",
    "            if np.isnan(s):\n",
    "                s=0\n",
    "            S.append(s)\n",
    "        cands_score_list.append(S)\n",
    "\n",
    "    return cvec_arr, cveclist_bdrs, cands_score_list\n",
    "\n",
    "def entity_to_context_scores(candslist, direction, method):\n",
    "    ''' finds the similarity between each entity and its context representation\n",
    "        Inputs:\n",
    "            candslist: the list of candidates [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            cvec_arr: the array of all embeddings for the candidates\n",
    "            cveclist_bdrs: The embedding vector for each candidate: [[c11,...c1k],...[cn1,...c1m]]\n",
    "        Returns:\n",
    "           cvec_arr: Candidate embeddings, a two dimensional array, each column \n",
    "           cveclist_bdrs: a list of pairs (beginning, end), to indicate where the \n",
    "                   reperesentation of the candidates for cij reside        \n",
    "           cands_score_list: scroes in the form of [[s11,...s1k],...[sn1,...s1m]]\n",
    "                    where sij  is the similarity of c[i,j] to to ci-th context\n",
    "                    \n",
    "            '''\n",
    "    cvec_arr, cveclist_bdrs =  get_candidate_representations(candslist, direction, method)    \n",
    "    \n",
    "    aggr_cveclist = np.zeros(shape=(len(candslist),cvec_arr.shape[1]))\n",
    "    for i in range(len(cveclist_bdrs)):\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        aggr_cveclist[i]=cvec_arr[b:e].sum(axis=0)\n",
    "    \n",
    "    from itertools import izip\n",
    "    resolved = 0\n",
    "    cands_score_list=[]        \n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        convec=aggr_cveclist[:i].sum(axis=0) + aggr_cveclist[i+1:].sum(axis=0)\n",
    "        S=[]    \n",
    "        for v in cvec:\n",
    "            try:\n",
    "                s = 1-sp.spatial.distance.cosine(convec, v);\n",
    "            except:\n",
    "                s=0                \n",
    "            if np.isnan(s):\n",
    "                s=0\n",
    "            S.append(s)\n",
    "        cands_score_list.append(S)\n",
    "\n",
    "    return cvec_arr, cveclist_bdrs, cands_score_list\n",
    "\n",
    "def simple_entity_context_disambiguate(candslist, direction=DIR_OUT, method='rvspagerank'):\n",
    "    '''Disambiguate a sentence using entity-context method\n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "       Returns: \n",
    "           a list of entity ids and a list of titles\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    _, _, candslist_scores = simple_entity_to_context_scores(candslist, direction, method);\n",
    "    # Iterate \n",
    "    true_entities = []\n",
    "    for cands, cands_scores in zip(candslist, candslist_scores):\n",
    "        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n",
    "        true_entities.append(cands[max_index][0])\n",
    "\n",
    "    titles = ids2title(true_entities)\n",
    "    return true_entities, titles        \n",
    "\n",
    "def entity_context_disambiguate(candslist, direction=DIR_OUT, method='rvspagerank'):\n",
    "    '''Disambiguate a sentence using entity-context method\n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "       Returns: \n",
    "           a list of entity ids and a list of titles\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    _, _, candslist_scores = entity_to_context_scores(candslist, direction, method);\n",
    "    # Iterate \n",
    "    true_entities = []\n",
    "    for cands, cands_scores in zip(candslist, candslist_scores):\n",
    "        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n",
    "        true_entities.append(cands[max_index][0])\n",
    "\n",
    "    titles = ids2title(true_entities)\n",
    "    return true_entities, titles        \n",
    "\n",
    "def key_criteria(cands_score):\n",
    "    ''' helper function for find_key_concept: returns a score indicating how good a key is x\n",
    "        Input:\n",
    "            scroes for candidates [ci1, ..., cik] in the form of (i, [(ri1, si1), ..., (rik, sik)] ) \n",
    "            where (rij,sij) indicates that sij is the similarity of c[i][rij] to to cith context\n",
    "            \n",
    "    '''\n",
    "    if len(cands_score[1])==0:\n",
    "        return -float(\"inf\")    \n",
    "    if len(cands_score[1])==1 or cands_score[1][1][1]==0:\n",
    "        return float(\"inf\")\n",
    "    \n",
    "    return (cands_score[1][0][1]-cands_score[1][1][1]) / cands_score[1][1][1]\n",
    "\n",
    "def find_key_concept(candslist, direction, method, ver=4):\n",
    "    ''' finds the key entity in the candidate list\n",
    "        Inputs:\n",
    "            candslist: the list of candidates [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            cvec_arr: the array of all embeddings for the candidates\n",
    "            cveclist_bdrs: The embedding vector for each candidate: [[c11,...c1k],...[cn1,...c1m]]\n",
    "        Returns:\n",
    "            cvec_arr: Candidate embeddings, a two dimensional array, each column \n",
    "            cveclist_bdrs: a list of pairs (beginning, end), to indicate where the \n",
    "            key_concept: the concept forwhich one of the candidates is the key entity\n",
    "            key_entity: candidate index for key_cancept that is detected to be key_entity\n",
    "            key_entity_vector: The embedding of key entity\n",
    "            '''\n",
    "    cvec_arr, cveclist_bdrs, cands_score_list = entity_to_context_scores(candslist, direction, method);\n",
    "    S=[sorted(enumerate(S), key=lambda x: -x[1]) for S in cands_score_list]\n",
    "        \n",
    "    if ver ==1: \n",
    "        key_concept, _ = max(enumerate(S), key=lambda x: x[1][0][1] if len(x[1])>1 else -1)\n",
    "    elif ver ==2: \n",
    "        key_concept, _ = max(enumerate(S), key=lambda x: (x[1][0][1]-x[1][1][1]) if len(x[1])>1 else -1)\n",
    "    elif ver ==3: \n",
    "        key_concept, _ = max(enumerate(S), key=lambda x: (x[1][0][1]-x[1][1][1])/(x[1][0][1]+x[1][1][1]) if len(x[1])>1 else -1)\n",
    "    elif ver ==4: \n",
    "        key_concept, _ = max(enumerate(S), key=key_criteria)\n",
    "    key_entity = S[key_concept][0][0]\n",
    "    \n",
    "    b,e = cveclist_bdrs[key_concept]\n",
    "    \n",
    "    key_entity_vector =  cvec_arr[b:e][key_entity]    \n",
    "    return cvec_arr, cveclist_bdrs, key_concept, key_entity, key_entity_vector\n",
    "\n",
    "\n",
    "def keyentity_candidate_scores(candslist, direction, method, ver):\n",
    "    '''returns entity scores using key-entity scoring \n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "           ver: 1 for the method explained in the paper\n",
    "           \n",
    "       Returns:\n",
    "           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    cvec_arr, cveclist_bdrs, key_concept, key_entity, key_entity_vector = find_key_concept(candslist, direction, method, ver)\n",
    "    \n",
    "    # Iterate \n",
    "    candslist_scores=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        cand_scores=[]\n",
    "\n",
    "        for v in cvec:\n",
    "            try:\n",
    "                d = 1-sp.spatial.distance.cosine(key_entity_vector, v);\n",
    "            except:\n",
    "                d=0                \n",
    "            if np.isnan(d):\n",
    "                d=0\n",
    "            \n",
    "            cand_scores.append(d)    \n",
    "        candslist_scores.append(cand_scores) \n",
    "    return candslist_scores\n",
    "\n",
    "def keyentity_disambiguate(candslist, direction=DIR_OUT, method='rvspagerank', ver=4):\n",
    "    '''Disambiguate a sentence using key-entity method\n",
    "       Inputs: \n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           direction: embedding direction\n",
    "           method: similarity method\n",
    "           ver: 1 for the method explained in the paper\n",
    "       Returns: \n",
    "           a list of entity ids and a list of titles\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    candslist_scores = keyentity_candidate_scores (candslist, direction, method, ver)\n",
    "    # Iterate \n",
    "    true_entities = []\n",
    "    for cands, cands_scores in zip(candslist, candslist_scores):\n",
    "        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n",
    "        true_entities.append(cands[max_index][0])\n",
    "\n",
    "    titles = ids2title(true_entities)\n",
    "    return true_entities, titles        \n",
    "\n",
    "## Plain context with word2vec\n",
    "\n",
    "def word_context_candidate_scores (S, M, candslist, ws):\n",
    "    '''returns entity scores using the similarity with their context\n",
    "       Inputs: \n",
    "           S: Sentence\n",
    "           M: Mentions\n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "            ws: word size\n",
    "       Returns:\n",
    "           Scores [[s11,...s1k],...[sn1,...s1m]] where sij is cij similarity to the key-entity\n",
    "    '''\n",
    "    \n",
    "    candslist_scores=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        pos = M[i][0]\n",
    "        #print \"At: \", M[i]\n",
    "        context = S[max(pos-ws,0):pos]+S[pos+1:pos+ws+1]\n",
    "        #print context\n",
    "        #print candslist[i], pos,context\n",
    "        context_vec = sp.zeros(getword2vec_model().vector_size)\n",
    "        for c in context:\n",
    "            #print \"getting vector for: \" , c\n",
    "            context_vec += getword2vector(c).as_matrix()\n",
    "        #print context_vec\n",
    "        cand_scores=[]\n",
    "\n",
    "        for c in cands:\n",
    "            try:\n",
    "                cand_vector = getentity2vector(encode_entity(c[0],'word2vec', get_id=False))\n",
    "                d = 1-sp.spatial.distance.cosine(context_vec, cand_vector);\n",
    "            except:\n",
    "                d=0                \n",
    "            if np.isnan(d):\n",
    "                d=0\n",
    "            \n",
    "            cand_scores.append(d)    \n",
    "        candslist_scores.append(cand_scores) \n",
    "\n",
    "    return candslist_scores\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "def word_context_disambiguate(S, M, candslist, ws ):\n",
    "    '''Disambiguate a sentence using word-context similarity\n",
    "       Inputs: \n",
    "           S: Sentence\n",
    "           M: Mentions\n",
    "           candslist: candidate list [[(c11, p11),...(c1k, p1k)],...[(cn1, pn1),...(c1m, p1m)]]\n",
    "           \n",
    "       Returns: \n",
    "           a list of entity ids and a list of titles\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    candslist_scores = word_context_candidate_scores (S, M, candslist, ws)\n",
    "                      \n",
    "    # Iterate \n",
    "    true_entities = []\n",
    "    for cands, cands_scores in zip(candslist, candslist_scores):\n",
    "        max_index, max_value = max(enumerate(cands_scores), key= lambda x:x[1])\n",
    "        true_entities.append(cands[max_index][0])\n",
    "\n",
    "    titles = ids2title(true_entities)\n",
    "    return true_entities, titles \n",
    "\n",
    "######\n",
    "\n",
    " \n",
    "\n",
    "def tagme_vote(c, a, candslist, simmatrix, pop):\n",
    "    v = 0\n",
    "    for b in  range(len(candslist)):\n",
    "        if a==b:\n",
    "            continue\n",
    "        cands = candslist[b]\n",
    "        if pop:\n",
    "            vb = [ci[1]*simmatrix[c[0]][ci[0]] for ci in cands]\n",
    "        else:\n",
    "            vb = [simmatrix[c[0]][ci[0]]  for ci in cands]\n",
    "        vb = sum(vb) / len(vb)\n",
    "        v += vb    \n",
    "    return v\n",
    "\n",
    "def tagme(candslist, method, direction, pop=False):\n",
    "    res=[]\n",
    "    simmatrix = get_sim_matrix(candslist, method, direction)\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        \n",
    "        maxd=-1\n",
    "        mi=0\n",
    "        #print len(cands)\n",
    "        for c in cands:\n",
    "            d = tagme_vote(c, i, candslist , simmatrix, pop);\n",
    "            if d>maxd:\n",
    "                maxd=d\n",
    "                index=mi\n",
    "            mi +=1\n",
    "        res.append(cands[index][0]) \n",
    "        #print index,\"\\n\"\n",
    "    titles = ids2title(res)\n",
    "    return res, titles\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wsd_eval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wsd_eval.py \n",
    "import sys\n",
    "from optparse import OptionParser\n",
    "\n",
    "#sys.path.insert(0,'..')\n",
    "#from wikisim.calcsim import *\n",
    "from wsd import *\n",
    "\n",
    "np.seterr(all='raise')\n",
    "\n",
    "parser = OptionParser()\n",
    "parser.add_option(\"-t\", \"--max_t\", action=\"store\", type=\"int\", dest=\"max_t\", default=5)\n",
    "parser.add_option(\"-c\", \"--max_count\", action=\"store\", type=\"int\", dest=\"max_count\", default=-1)\n",
    "parser.add_option(\"-w\", \"--win_size\", action=\"store\", type=\"int\", dest=\"win_size\", default=5)\n",
    "parser.add_option(\"-v\", action=\"store_true\", dest=\"verbose\", default=False)\n",
    "\n",
    "(options, args) = parser.parse_args()\n",
    "fresh_restart=True\n",
    "\n",
    "word2vec_path = os.path.join(home, 'backup/wikipedia/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "#word2vec_path = os.path.join(home, '/users/grad/sajadi/backup/wikipedia/20160305/embed/word2vec.enwiki-20160305-replace_surface.1.0.500.10.5.15.5.5/word2vec.enwiki-20160305-replace_surface.1.0.500.10.5.15.5.5')\n",
    "\n",
    "\n",
    "dsnames = [os.path.join(home,'backup/datasets/ner/kore.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/wiki-mentions.5000.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/aida.json'), \n",
    "          os.path.join(home,'backup/datasets/ner/msnbc.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/aquaint.json') \n",
    "          ]\n",
    "\n",
    "\n",
    "methods = (('ams', DIR_BOTH,'ilp'), ('wlm', DIR_IN,'ilp'),('rvspagerank', DIR_BOTH, 'ilp'))\n",
    "methods = (('word2vec.ehsan', DIR_BOTH,'ilp') ,('rvspagerank', DIR_BOTH, 'keyq'),('rvspagerank', DIR_BOTH, 'keydisamb'))\n",
    "methods = (('word2vec.ehsan', DIR_BOTH, 'keydisamb'), ('word2vec.ehsan', DIR_BOTH,'entitycontext') ,('rvspagerank', DIR_BOTH, 'entitycontext'))\n",
    "methods = (('rvspagerank', DIR_BOTH, 'simplecontext'),('word2vec.ehsan', DIR_BOTH, 'simplecontext'))\n",
    "#methods = (('rvspagerank', DIR_BOTH, 'ilp'),('word2vec.ehsan', DIR_BOTH, 'ilp'),)\n",
    "#methods = (('word2vec.ehsan', DIR_BOTH, 'simplecontext'),)\n",
    "\n",
    "#methods = (('word2vec.500', None,'context4_4'),)\n",
    "methods = (('rvspagerank', DIR_BOTH, 'keydisamb'), ('rvspagerank', DIR_BOTH, 'entitycontext') )\n",
    "\n",
    "\n",
    "max_t = options.max_t\n",
    "max_count = options.max_count\n",
    "verbose = options.verbose\n",
    "ws = options.win_size\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wsd')\n",
    "# if not os.path.exists(outdir): #Causes synchronization problem\n",
    "#     os.makedirs(outdir)\n",
    "\n",
    "tmpdir = os.path.join(outdir, 'tmp')\n",
    "# if not os.path.exists(tmpdir): #Causes synchronization problem\n",
    "#     os.makedirs(tmpdir)\n",
    "    \n",
    "resname =  os.path.join(outdir, 'reslog.csv')\n",
    "#clearlog(resname)\n",
    "\n",
    "detailedresname=  os.path.join(outdir, 'detailedreslog.txt')\n",
    "#clearlog(detailedresname)\n",
    "\n",
    "\n",
    "\n",
    "for method, direction, op_method in methods:\n",
    "    if 'word2vec' in method:\n",
    "        gensim_loadmodel(word2vec_path)\n",
    "        print \"loaded\"\n",
    "        sys.stdout.flush()\n",
    "    for dsname in dsnames:\n",
    "        start = time.time()\n",
    "        \n",
    "        print \"dsname: %s, method: %s, op_method: %s, direction: %s, max_t: %s, ws: %s ...\"  % (dsname,\n",
    "                method, op_method, direction, max_t, ws)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        tmpfilename = os.path.join(tmpdir, \n",
    "                                   '-'.join([method, str(direction), op_method, str(max_t), str(ws), os.path.basename(dsname)]))\n",
    "        overall=[]\n",
    "        start_count=-1\n",
    "        if os.path.isfile(tmpfilename) and not fresh_restart:\n",
    "            with open(tmpfilename,'r') as tmpf:\n",
    "                for line in tmpf:\n",
    "                    js = json.loads(line.strip())\n",
    "                    start_count = js['no']\n",
    "                    if js['tp'] is not None:\n",
    "                        overall.append(js['tp'])\n",
    "        \n",
    "        if start_count !=-1:\n",
    "            print \"Continuing from\\t\", start_count\n",
    "            \n",
    "        count=0\n",
    "        with open(dsname,'r') as ds, open(tmpfilename,'a') as tmpf:\n",
    "            for line in ds:\n",
    "                js = json.loads(line.decode('utf-8').strip());\n",
    "                S = js[\"text\"]\n",
    "                M = js[\"mentions\"]\n",
    "                count +=1\n",
    "                if count <= start_count:\n",
    "                    continue\n",
    "                if verbose:\n",
    "                    print \"%s:\\tS=%s\\n\\tM=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                C = generate_candidates(S, M, max_t=max_t, enforce=True)\n",
    "                \n",
    "                try:\n",
    "                    ids, titles = disambiguate_driver(C, ws, method=method, direction=direction, op_method=op_method)\n",
    "                    tp = get_tp(M, ids) \n",
    "                except Exception as ex:\n",
    "                    tp = (None, None)\n",
    "                    print \"[Error]:\\t\", type(ex), ex\n",
    "                    raise\n",
    "                    continue\n",
    "                \n",
    "                overall.append(tp)\n",
    "                tmpf.write(json.dumps({\"no\":count, \"tp\":tp})+\"\\n\")\n",
    "                if (max_count !=-1) and (count >= max_count):\n",
    "                    break\n",
    "                    \n",
    "\n",
    "        elapsed = str(timeformat(int(time.time()-start)));\n",
    "        print \"done\"\n",
    "        detailedres ={\"dsname\":dsname, \"method\": method, \"op_method\": op_method, \"driection\": direction,\n",
    "                      \"max_t\": max_t, \"tp\":overall, \"elapsed\": elapsed, \"ws\": ws}\n",
    "        \n",
    "        \n",
    "        logres(detailedresname, '%s',  json.dumps(detailedres))\n",
    "        \n",
    "        micro_prec, macro_prec = get_prec(overall)        \n",
    "        logres(resname, '%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s', method, op_method, graphtype(direction), max_t , ws, \n",
    "               dsname, micro_prec, macro_prec, elapsed)\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example to show  how to get scores only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates:  [[(8551L, 0.8145922746781116), (36684L, 0.06866952789699571), (9072L, 0.06437768240343347), (8618L, 0.026609442060085836), (670599L, 0.02575107296137339)], [(4689460L, 0.8232923354874574), (32388L, 0.08519213397262178), (1147963L, 0.0801890070182753), (47923L, 0.01125703564727955), (45979L, 6.948787436592315e-05)], [(41188263L, 0.9722714965922883), (69802L, 0.020632994118196246), (14256525L, 0.003641116609093455), (44636014L, 0.0017738773223788628), (100383L, 0.0016805153580431332)]] \n",
      "\n",
      "Key Scores_method_1:  [[0.0002226677322854087, 2.0723799248778541e-05, 8.4090270149217439e-05, 1.0, 0.0], [0.0001735395941440121, 0.00091676079085445394, 2.1336085988088449e-05, 0.0027893462212920106, 0.085002685209083939], [0.01068388071283688, 0.0018273400886253954, 7.8498187545683606e-05, 0.0014733524810309762, 0.00076126583321334262]] \n",
      "\n",
      "Key Scores_method_2:  [[0.00066010531003890538, 0.0016635181804792731, 0.0021782449484554212, 0.034048225409881616, 0.0010287486972100357], [0.00059483481694666551, 0.0013233024165593132, 1.4416077105949832e-05, 0.003465176316260532, 0.015577435277724261], [0.0094972254631763287, 0.001630071323747595, 0.00018957356543036763, 0.00043561559528770832, 0.00079549102081577505]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload\n",
    "\n",
    "# %aimport wsd\n",
    "# import sys\n",
    "from wsd import *\n",
    "import time\n",
    "ws=5\n",
    "S=[\"Carlos\", \"met\", \"David\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "M=[[0, \"Roberto_Carlos\"], [2, \"David_Beckham\"], [4, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "\n",
    "S=[\"Carlos\", \"met\", \"David\", \"and\" , \"Victoria\", \"in\", \"Madrid\"]\n",
    "M=[[2, \"David_Beckham\"], [4, \"Victoria_Beckham\"], [6, \"Madrid\"]]\n",
    "\n",
    "start = time.time()\n",
    "C = generate_candidates(S, M, max_t=5, enforce=True)\n",
    "print \"Candidates: \", C, \"\\n\"\n",
    "\n",
    "candslist_scores = coherence_scores_driver(C, ws, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\")\n",
    "print \"Key Scores_method_1: \", candslist_scores, \"\\n\"\n",
    "\n",
    "candslist_scores = coherence_scores_driver(C, ws, method='rvspagerank', direction=DIR_BOTH, op_method=\"entitycontext\")\n",
    "print \"Key Scores_method_2: \", candslist_scores, \"\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example: Popularity Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from optparse import OptionParser\n",
    "\n",
    "from wsd import *\n",
    "\n",
    "# np.seterr(all='raise')\n",
    "\n",
    "# parser = OptionParser()\n",
    "# parser.add_option(\"-t\", \"--max_t\", action=\"store\", type=\"int\", dest=\"max_t\", default=5)\n",
    "# parser.add_option(\"-c\", \"--max_count\", action=\"store\", type=\"int\", dest=\"max_count\", default=-1)\n",
    "# parser.add_option(\"-w\", \"--win_size\", action=\"store\", type=\"int\", dest=\"win_size\", default=5)\n",
    "# parser.add_option(\"-v\", action=\"store_true\", dest=\"verbose\", default=False)\n",
    "\n",
    "# (options, args) = parser.parse_args()\n",
    "\n",
    "#word2vec_path = os.path.join(home, 'backup/wikipedia/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "\n",
    "dsnames = [os.path.join(home,'backup/datasets/ner/kore.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/wiki-mentions.5000.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/aida.json'), \n",
    "          os.path.join(home,'backup/datasets/ner/msnbc.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/aquaint.json') \n",
    "          ]\n",
    "\n",
    "dsnames = [os.path.join(home,'backup/datasets/ner/kore.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/msnbc.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/aquaint.json') \n",
    "          ]\n",
    "\n",
    "\n",
    "max_t = 50\n",
    "max_count = -1\n",
    "verbose = False\n",
    "ws = 5\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wsd')\n",
    "# if not os.path.exists(outdir): #Causes synchronization problem\n",
    "#     os.makedirs(outdir)\n",
    "\n",
    "tmpdir = os.path.join(outdir, 'tmp')\n",
    "# if not os.path.exists(tmpdir): #Causes synchronization problem\n",
    "#     os.makedirs(tmpdir)\n",
    "    \n",
    "resname =  os.path.join(outdir, 'reslog.csv')\n",
    "#clearlog(resname)\n",
    "\n",
    "detailedresname=  os.path.join(outdir, 'detailedreslog.txt')\n",
    "#clearlog(detailedresname)\n",
    "\n",
    "\n",
    "\n",
    "for dsname in dsnames:\n",
    "    start = time.time()\n",
    "\n",
    "    overall=[]\n",
    "    with open(dsname,'r') as ds:\n",
    "        for line in ds:\n",
    "            js = json.loads(line.decode('utf-8').strip());\n",
    "            S = js[\"text\"]\n",
    "            M = js[\"mentions\"]\n",
    "            #print \"S=%s\\n\\tM=%s\" % (json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))\n",
    "\n",
    "            C = generate_candidates(S, M, max_t=max_t, enforce=False)\n",
    "\n",
    "            try:\n",
    "                ids, titles = disambiguate_driver(C, ws, op_method='popularity')\n",
    "                tp = get_tp(M, ids) \n",
    "                #print M,\" , \", tp\n",
    "            except Exception as ex:\n",
    "                tp = (None, None)\n",
    "                print \"[Error]:\\t\", type(ex), ex\n",
    "                #raise\n",
    "                continue\n",
    "\n",
    "            overall.append(tp)\n",
    "\n",
    "\n",
    "    elapsed = str(timeformat(int(time.time()-start)));\n",
    "    print \"done\"\n",
    "    detailedres ={\"dsname\":dsname, \"method\": \"NA\", \"op_method\": \"popularity\", \"driection\": \"NA\",\n",
    "                   \"tp\":overall, \"elapsed\": elapsed, \"ws\": ws}\n",
    "\n",
    "\n",
    "    logres(detailedresname, '%s',  json.dumps(detailedres))\n",
    "    #print overall\n",
    "    micro_prec, macro_prec = get_prec(overall)        \n",
    "    print(resname, '%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s', \"NA\", \"popularity\", \"NA\", max_t , ws, \n",
    "           dsname, micro_prec, macro_prec, elapsed)\n",
    "    \n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results for the Walk Through Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload\n",
    "\n",
    "# %aimport wsd\n",
    "# import sys\n",
    "from wsd import *\n",
    "import time\n",
    "ws=5\n",
    "word2vec_path = os.path.join(home, 'backup/wikipedia/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "#gensim_loadmodel(word2vec_path)\n",
    "#print \"loaded\"\n",
    "#sys.stdout.flush()\n",
    "\n",
    "S=[\"David\", \"started\", \"dating\", \"Victoria\" , \"after\", \"she\", \"attended\", \"a\", \"Man United\", \"match\"]\n",
    "M=[[0, \"David_Beckham\"], [3, \"Victoria_Beckham\"], [8, \"Manchester United F.C.\"]]\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "C = generate_candidates(S, M, max_t=5, enforce=True)\n",
    "print \"Candidates: \", C, \"\\n\"\n",
    "candslist=C\n",
    "res_all=[]\n",
    "simmatrix = get_sim_matrix(C, 'rvspagerank', DIR_BOTH)\n",
    "print simmatrix\n",
    "for i in range(len(candslist)):\n",
    "    for j in range(len(candslist[i])):\n",
    "        res_ij =  evalkey(candslist[i][j], i, candslist, simmatrix)\n",
    "        res_all.append(res_ij)\n",
    "res, score = max(res_all, key=lambda x: x[1])\n",
    "#print(\"Score:\", score)\n",
    "titles = ids2title(res)\n",
    "\n",
    "# candslist_scores = coherence_scores_driver(C, ws, method='rvspagerank', direction=DIR_BOTH, op_method=\"simplecontext\")\n",
    "# print \"Key Scores_method_2: \", candslist_scores, \"\\n\"\n",
    "\n",
    "\n",
    "# ids, titles = disambiguate_driver(S, M, C, ws, method='word2vec', op_method='simplecontext')\n",
    "# tp = get_tp(M, ids) \n",
    "\n",
    "# candslist_scores = coherence_scores_driver(C, ws, method='rvspagerank', direction=DIR_BOTH, op_method=\"keydisamb\")\n",
    "# print \"Key Scores_method_1: \", candslist_scores, \"\\n\"\n",
    "\n",
    "# candslist_scores = coherence_scores_driver(C, ws, method='rvspagerank', direction=DIR_BOTH, op_method=\"entitycontext\")\n",
    "# print \"Key Scores_method_2: \", candslist_scores, \"\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
