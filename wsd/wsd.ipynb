{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "** Local and Global Algorithms ... **\n",
    "\n",
    "AQUAINT: Milne\n",
    "\n",
    "MSNBC dataset, taken from (Cucerzan, 2007),\n",
    "\n",
    "ACE: Mechanical Turkn\n",
    "\n",
    "Wiki: choose those paragraphts that p(t|m) makes atleast 10% error\n",
    "\n",
    "For evaluation, check BOT evaluation, mentioned in Milne \n",
    "\n",
    "Downloadable from :\n",
    "http://cogcomp.cs.illinois.edu/page/resource_view/4\n",
    "\n",
    "\n",
    "**Spotlight**\n",
    "two datasets, a wiki selection\n",
    "35 paragraphs from New York times\n",
    "There is a website, but couldn't find it\n",
    "\n",
    "http://oldwiki.dbpedia.org/Datasets/NLP\n",
    "\n",
    "Tag me:\n",
    "Wiki and tweet, \n",
    "available, but looks old!\n",
    "http://acube.di.unipi.it/tagme-dataset/\n",
    "\n",
    "**AIDA**\n",
    "\n",
    ": https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/aida/downloads/**\n",
    "AIDA CoNLL-YAGO Dataset: Hnad create from Conll\n",
    "AIDA-EE Dataset: Again hand done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile wsd.py \n",
    "\"\"\" Evaluating the method on Semantic Relatedness Datasets.\"\"\"\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time;\n",
    "import json \n",
    "import requests\n",
    "from itertools import chain\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "        \n",
    "\n",
    "\n",
    "sys.path.insert(0,'..')\n",
    "from wikisim.config import *\n",
    "\n",
    "from wikisim.calcsim import *\n",
    "#reopen()\n",
    "\n",
    "\n",
    "def generate_candidates(S, M, max_t=10, enforce=True):\n",
    "    candslist=[]\n",
    "    for m in M:\n",
    "        wid = title2id(m[1])\n",
    "        if wid is None:\n",
    "            raise Exception(m[1].encode('utf-8') + ' not found')\n",
    "        \n",
    "        clist = anchor2concept(S[m[0]])\n",
    "        clist = sorted(clist, key=lambda x: -x[1])\n",
    "\n",
    "        smooth=0    \n",
    "        trg = [(i,(c,f)) for i,(c,f) in enumerate(clist) if c==wid]\n",
    "        if not trg:\n",
    "            trg=[(len(clist), (wid,0))]\n",
    "            smooth=1\n",
    "\n",
    "            \n",
    "        clist = clist[:max_t]\n",
    "        if smooth==1 or trg[0][0]>=max_t: \n",
    "            if clist:\n",
    "                clist.pop()\n",
    "            clist.append(trg[0][1])\n",
    "        s = sum(c[1]+smooth for c in clist )        \n",
    "        clist = [(c,float(f+smooth)/s) for c,f in clist ]\n",
    "            \n",
    "        candslist.append(clist)\n",
    "    return  candslist \n",
    "def disambiguate(C, method, direction, op_method):\n",
    "    if op_method == 'ilp':\n",
    "        return disambiguate_ilp(C, method, direction)\n",
    "    if op_method == 'ilp2':\n",
    "        return disambiguate_ilp_2(C, method, direction)\n",
    "    if op_method == 'keyq':\n",
    "        return key_quad(C, method, direction)\n",
    "    if op_method == 'pkeyq':\n",
    "        return Pkey_quad(C, method, direction)\n",
    "    if  op_method == 'context1'  :\n",
    "        return contextdisamb_1(C, direction)\n",
    "    if  op_method == 'context2'  :\n",
    "        return contextdisamb_2(C, direction)\n",
    "    if  op_method == 'context3'  :\n",
    "        return contextdisamb_3(C, direction)\n",
    "    \n",
    "    if  op_method == 'context4_1'  :\n",
    "        return contextdisamb_4(C, direction, method, 1)\n",
    "    if  op_method == 'context4_2'  :\n",
    "        return contextdisamb_4(C, direction, method, 2)\n",
    "    if  op_method == 'context4_3'  :\n",
    "        return contextdisamb_4(C, direction, method, 3)    \n",
    "    if  op_method == 'context4_4'  :\n",
    "        return contextdisamb_4(C, direction, method, 4)\n",
    "    \n",
    "    if  op_method == 'tagme'  :\n",
    "        return tagme(C, method, direction)\n",
    "    if  op_method == 'tagme2'  :\n",
    "        return tagme(C, method, direction, True)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def disambiguate_driver(C, ws, method, direction, op_method):\n",
    "    ids = []\n",
    "    titles = []\n",
    "    \n",
    "    windows = [[start, min(start+ws, len(C))] for start in range(0,len(C),ws) ]\n",
    "    last = len(windows)\n",
    "    if last > 1 and windows[last-1][1]-windows[last-1][0]<2:\n",
    "        windows[last-2][1] = len(C)\n",
    "        windows.pop()\n",
    "        \n",
    "    for w in windows:\n",
    "        chunk_c = C[w[0]:w[1]]\n",
    "        chunk_ids, chunk_titles = disambiguate(chunk_c, method, direction, op_method)\n",
    "        ids += chunk_ids\n",
    "        titles += chunk_titles\n",
    "    return ids, titles     \n",
    "\n",
    "def get_tp(gold_titles, ids):\n",
    "    tp=0\n",
    "    for m,id2 in zip(gold_titles, ids):\n",
    "        if title2id(m[1]) == id2:\n",
    "            tp += 1\n",
    "    return [tp, len(ids)]\n",
    "\n",
    "def get_prec(tp_list):\n",
    "    overall_tp = 0\n",
    "    simple_count=0\n",
    "    overall_count=0\n",
    "    macro_prec = 0;\n",
    "    for tp, count in tp_list:\n",
    "        if tp is None:\n",
    "            continue\n",
    "        simple_count +=1    \n",
    "        overall_tp += tp\n",
    "        overall_count += count\n",
    "        macro_prec += float(tp)/count\n",
    "        \n",
    "    macro_prec = macro_prec/simple_count\n",
    "    micro_prec = float(overall_tp)/overall_count\n",
    "    \n",
    "    return micro_prec, macro_prec\n",
    "\n",
    "\n",
    "# Integer Programming\n",
    "\n",
    "from itertools import izip\n",
    "from itertools import product\n",
    "from pulp import *\n",
    "import random\n",
    "\n",
    "def getscore(x,y,method, direction):\n",
    "    return getsim(x,y ,method, direction)\n",
    "    #return random.random()\n",
    "\n",
    "def disambiguate_ilp(C, method, direction):\n",
    "    #C = [('a','b','c'), ('e', 'f', 'g'), ('h', 'i')]\n",
    "\n",
    "    R1 = [zip([i]*len(c), range(len(c))) for i,c in enumerate(C)]\n",
    "\n",
    "    #R1 = {r:str(r) for r in itertools.chain(*RI1)}\n",
    "    #R1 = [[str(rij) for rij in ri] for ri in RI1]\n",
    "\n",
    "    #RI1_flat = list(itertools.chain(*RI1))\n",
    "\n",
    "\n",
    "    R2=[]\n",
    "    for e in combination(R1,2):\n",
    "        R2 += [r for r in itertools.product(e[0], e[1]) ]        \n",
    "\n",
    "\n",
    "    #R2 = {r:str(r) for r in RI2}\n",
    "\n",
    "\n",
    "    \n",
    "    S = {((u0,u1),(v0,v1)):getscore(C[u0][u1][0],C[v0][v1][0], method, direction) for ((u0,u1),(v0,v1)) in R2}\n",
    "\n",
    "\n",
    "    prob = LpProblem(\"wsd\", LpMaximize)\n",
    "\n",
    "    R=list(itertools.chain(*R1)) + R2\n",
    "    R_vars = LpVariable.dicts(\"R\",R,\n",
    "                                lowBound = 0,\n",
    "                                upBound = 1,\n",
    "                                cat = pulp.LpInteger)\n",
    "    prob += lpSum([S[r]*R_vars[r] for r in R2])\n",
    "\n",
    "\n",
    "    i=0\n",
    "    for ri in R1:\n",
    "        prob += lpSum([R_vars[rij] for rij in ri])==1, (\"R1 %s constraint\")%i\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    for r in R2:\n",
    "        prob += lpSum([R_vars[r[0]],R_vars[r[1]],-2*R_vars[r]]) >=0, (\"R_%s_%s constraint\"%(r[0], r[1]))\n",
    "\n",
    "    prob.solve() \n",
    "    #print(\"Status:\", LpStatus[prob.status])\n",
    "    #print(\"Score:\", value(prob.objective))\n",
    "    ids    = [C[r[0]][r[1]][0] for r in list(itertools.chain(*R1)) if R_vars[r].value() == 1.0]\n",
    "    titles = ids2title(ids)\n",
    "    return ids, titles\n",
    "        \n",
    "def disambiguate_ilp_2(C, method, direction):\n",
    "    \n",
    "    #C = [('a','b','c'), ('e', 'f', 'g'), ('h', 'i')]\n",
    "\n",
    "    #R1 = [zip([\"R\"z]*len(c),zip([i]*len(c), range(len(c)))) for i,c in enumerate(C)]\n",
    "    R1 = [zip(['R']*len(c),[i]*len(c), range(len(c))) for i,c in enumerate(C)]\n",
    "    Q = [zip(['Q']*len(c),[i]*len(c), range(len(c))) for i,c in enumerate(C)]\n",
    "\n",
    "    #R1 = {r:str(r) for r in itertools.chain(*RI1)}\n",
    "    #R1 = [[str(rij) for rij in ri] for ri in RI1]\n",
    "\n",
    "    #RI1_flat = list(itertools.chain(*RI1))\n",
    "\n",
    "\n",
    "    R2=[]\n",
    "    for e in combination(R1,2):\n",
    "            R2 += [('R',(i,k),(j,l)) for (_,i,k),(_,j,l) in itertools.product(e[0], e[1]) ]        \n",
    "\n",
    "\n",
    "    #R2 = {r:str(r) for r in RI2}\n",
    "\n",
    "\n",
    "\n",
    "    S = {('R',(i,k),(j,l)):getscore(C[i][k][0],C[j][l][0], method, direction) for _,(i,k),(j,l) in R2}\n",
    "\n",
    "\n",
    "    prob = LpProblem(\"wsd\", LpMaximize)\n",
    "\n",
    "    R=list(itertools.chain(*R1)) + R2\n",
    "    Q=list(itertools.chain(*Q))\n",
    "    R_vars = LpVariable.dicts(\"R\",R,\n",
    "                                lowBound = 0,\n",
    "                                upBound = 1,\n",
    "                                cat = pulp.LpInteger)\n",
    "\n",
    "    Q_vars = LpVariable.dicts(\"Q\",Q,\n",
    "                                lowBound = 0,\n",
    "                                upBound = 1,\n",
    "                                cat = pulp.LpInteger)\n",
    "\n",
    "    prob += lpSum([S[r]*R_vars[r] for r in R2])\n",
    "\n",
    "\n",
    "    i=0\n",
    "    for ri in R1:\n",
    "        prob += lpSum([R_vars[rij] for rij in ri])==1, (\"R1 %s constraint\")%i\n",
    "        i += 1\n",
    "\n",
    "    prob += lpSum(Q_vars.values())==1, (\"Q constraint\")\n",
    "\n",
    "\n",
    "    for _,(i,k),(j,l) in R2:\n",
    "        prob += lpSum([R_vars[('R',i,k)],R_vars[('R',j,l)],-2*R_vars[('R',(i,k),(j,l))]]) >=0, (\"R_%s_%s constraint\"%((i,k),(j,l)))\n",
    "        prob += lpSum([Q_vars[('Q',i,k)],Q_vars[('Q',j,l)], -1*R_vars[('R',(i,k),(j,l))]]) >=0, (\"Q_%s_%s constraint\"%((i,k),(j,l)))\n",
    "\n",
    "    prob.solve() \n",
    "#     print(\"Status:\", LpStatus[prob.status])\n",
    "#     print(\"Score:\", value(prob.objective))\n",
    "\n",
    "#     for (_,i,k), q in Q_vars.items():\n",
    "#         if q.value()==1:\n",
    "#             print(\"central concept: \", id2title(C[i][k][0]))\n",
    "    ids    = [C[i][k][0] for _,i,k in list(itertools.chain(*R1)) if R_vars[('R',i,k)].value() == 1.0]\n",
    "    \n",
    "    \n",
    "    titles = ids2title(ids)\n",
    "    return ids, titles\n",
    "\n",
    "# key\n",
    "def evalkey(c, a, candslist, simmatrix):\n",
    "    resolved=[]\n",
    "    score=0;\n",
    "    for i in  range(len(candslist)):\n",
    "        if a==i:\n",
    "            resolved.append(c[0])\n",
    "            continue\n",
    "        cands = candslist[i]\n",
    "        vb = [(cj[0], simmatrix[c[0]][cj[0]])  for cj in cands]\n",
    "        max_concept, max_sc = max(vb, key=lambda x: x[1])\n",
    "        score += max_sc\n",
    "        resolved.append(max_concept)\n",
    "    return resolved,score\n",
    "\n",
    "def key_quad(candslist, method, direction):\n",
    "    res_all=[]\n",
    "    simmatrix = get_sim_matrix(candslist, method, direction)\n",
    "\n",
    "    for i in range(len(candslist)):\n",
    "        for j in range(len(candslist[i])):\n",
    "            res_ij =  evalkey(candslist[i][j], i, candslist, simmatrix)\n",
    "            res_all.append(res_ij)\n",
    "    res, score = max(res_all, key=lambda x: x[1])\n",
    "    #print(\"Score:\", score)\n",
    "    titles = ids2title(res)\n",
    "    return res, titles\n",
    "\n",
    "# Parallel Keyquad\n",
    "from functools import partial\n",
    "from multiprocessing import Pool as ThreadPool \n",
    "def Pevalkey((c, a), candslist, simmatrix):\n",
    "    resolved=[]\n",
    "    score=0;\n",
    "    for i in  range(len(candslist)):\n",
    "        if a==i:\n",
    "            resolved.append(c[0])\n",
    "            continue\n",
    "        cands = candslist[i]\n",
    "        vb = [(cj[0], simmatrix[c[0]][cj[0]])  for cj in cands]\n",
    "        max_concept, max_sc = max(vb, key=lambda x: x[1])\n",
    "        score += max_sc\n",
    "        resolved.append(max_concept)\n",
    "    return resolved,score\n",
    "def Pkey_quad(candslist, method, direction):\n",
    "    res_all=[]\n",
    "    simmatrix = get_sim_matrix(candslist, method, direction)\n",
    "    pool = ThreadPool(25) \n",
    "    \n",
    "    partial_evalkey = partial(Pevalkey, candslist=candslist, simmatrix=simmatrix)\n",
    "    I=[[j]*len(candslist[j]) for j in range(len(candslist))]\n",
    "    \n",
    "    res_all= pool.map(partial_evalkey, zip(itertools.chain(*candslist), itertools.chain(*I)))\n",
    "    pool.close() \n",
    "    pool.join() \n",
    "    \n",
    "    res, score = max(res_all, key=lambda x: x[1])\n",
    "    titles = ids2title(res)\n",
    "    return res, titles\n",
    "\n",
    "\n",
    "# Context Vector\n",
    "\n",
    "        \n",
    "\n",
    "def contextdisamb_1(candslist, direction=DIR_OUT):\n",
    "    cframelist=[]\n",
    "    cveclist_bdrs = []\n",
    "    for cands in candslist:\n",
    "        cands_rep = [conceptrep(c[0], direction=direction, get_titles=False) for c in cands]\n",
    "        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n",
    "        cframelist += cands_rep\n",
    "\n",
    "    # for cands in candslist:\n",
    "    #     cveclisttitles.append([conceptrep(c, direction, get_titles=False) for c in cands])\n",
    "\n",
    "    cvec_fr = pd.concat(cframelist, join='outer', axis=1)\n",
    "    cvec_fr.fillna(0, inplace=True)\n",
    "    cvec_arr = cvec_fr.as_matrix().T\n",
    "\n",
    "    i=0\n",
    "    for cframe in cframelist:\n",
    "        if cframe.empty:\n",
    "            cvec_arr = np.insert(cvec_arr,i,0, axis=0)\n",
    "        i+=1    \n",
    "    \n",
    "    convec = cvec_arr.sum(axis=0)\n",
    "    from itertools import izip\n",
    "    res=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        \n",
    "        maxd=-1\n",
    "        mi=0\n",
    "        for v in cvec:\n",
    "            d = 1-sp.spatial.distance.cosine(convec, v);\n",
    "            if d>maxd:\n",
    "                maxd=d\n",
    "                index=mi\n",
    "            mi +=1\n",
    "        res.append(cands[index][0]) \n",
    "        #print index,\"\\n\"\n",
    "    titles = ids2title(res)\n",
    "    return res, titles\n",
    "\n",
    "                                   \n",
    "def contextdisamb_2(candslist, direction=DIR_OUT):\n",
    "    cframelist=[]\n",
    "    cveclist_bdrs = []\n",
    "    for cands in candslist:\n",
    "        cands_rep = [conceptrep(c[0], direction=direction, get_titles=False) for c in cands]\n",
    "        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n",
    "        cframelist += cands_rep\n",
    "\n",
    "    #print \"ambig_count:\", ambig_count\n",
    "    cvec_fr = pd.concat(cframelist, join='outer', axis=1)\n",
    "    cvec_fr.fillna(0, inplace=True)\n",
    "    cvec_arr = cvec_fr.as_matrix().T\n",
    "    i=0\n",
    "    for cframe in cframelist:\n",
    "        if cframe.empty:\n",
    "            cvec_arr = np.insert(cvec_arr,i,0, axis=0)\n",
    "        i+=1    \n",
    "    \n",
    "    aggr_cveclist = np.zeros(shape=(len(candslist),cvec_arr.shape[1]))\n",
    "    for i in range(len(cveclist_bdrs)):\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        aggr_cveclist[i]=cvec_arr[b:e].sum(axis=0)\n",
    "    \n",
    "    res=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        convec=aggr_cveclist[:i].sum(axis=0) + aggr_cveclist[i+1:].sum(axis=0)\n",
    "\n",
    "        maxd=-1\n",
    "        index = -1\n",
    "        mi=0\n",
    "\n",
    "        for v in cvec:\n",
    "            d = 1-sp.spatial.distance.cosine(convec, v);\n",
    "            if d>maxd:\n",
    "                maxd=d\n",
    "                index=mi\n",
    "            mi +=1\n",
    "        if index==-1:\n",
    "            index=0\n",
    "        res.append(cands[index][0]) \n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cveclist_bdrs[i] = (b+index,b+index+1)\n",
    "        \n",
    "        aggr_cveclist[i] =  cvec_arr[b:e][index]\n",
    "        \n",
    "        candslist[i] = candslist[i][index][0]\n",
    "        \n",
    "        \n",
    "\n",
    "    titles = ids2title(res)\n",
    "\n",
    "    return res, titles\n",
    "\n",
    "def contextdisamb_3(candslist, direction=DIR_OUT):\n",
    "    cframelist=[]\n",
    "    cveclist_bdrs = []\n",
    "    ambig_count=0\n",
    "    for cands in candslist:\n",
    "        if len(candslist)>1:\n",
    "            ambig_count += 1\n",
    "        cands_rep = [conceptrep(c[0], direction=direction, get_titles=False) for c in cands]\n",
    "        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n",
    "        cframelist += cands_rep\n",
    "\n",
    "    #print \"ambig_count:\", ambig_count\n",
    "        \n",
    "    cvec_fr = pd.concat(cframelist, join='outer', axis=1)\n",
    "    cvec_fr.fillna(0, inplace=True)\n",
    "    cvec_arr = cvec_fr.as_matrix().T\n",
    "    i=0\n",
    "    for cframe in cframelist:\n",
    "        if cframe.empty:\n",
    "            cvec_arr = np.insert(cvec_arr,i,0, axis=0)\n",
    "        i+=1    \n",
    "    \n",
    "    aggr_cveclist = np.zeros(shape=(len(candslist),cvec_arr.shape[1]))\n",
    "    for i in range(len(cveclist_bdrs)):\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        aggr_cveclist[i]=cvec_arr[b:e].sum(axis=0)\n",
    "    from itertools import izip\n",
    "    resolved = 0\n",
    "    for resolved in range(ambig_count):\n",
    "        Dlist=[]        \n",
    "        for i in range(len(candslist)):\n",
    "            cands = candslist[i]\n",
    "            b,e = cveclist_bdrs[i]\n",
    "            cvec = cvec_arr[b:e]\n",
    "            convec=aggr_cveclist[:i].sum(axis=0) + aggr_cveclist[i+1:].sum(axis=0)\n",
    "            D=[]    \n",
    "            for v in cvec:\n",
    "                d = 1-sp.spatial.distance.cosine(convec, v);\n",
    "                if np.isnan(d):\n",
    "                    d=0\n",
    "                D.append(d)\n",
    "            D=sorted(enumerate(D), key=lambda x: -x[1])\n",
    "            Dlist.append(D)\n",
    "\n",
    "        max_concept, _ = max(enumerate(Dlist), key=lambda x: (x[1][0][1]-x[1][1][1]) if len(x[1])>1 else -1)\n",
    "        max_candidate = Dlist[max_concept][0][0]\n",
    "        \n",
    "        b,e = cveclist_bdrs[max_concept]\n",
    "        cveclist_bdrs[max_concept] = (b+max_concept,b+max_concept+1)\n",
    "        aggr_cveclist[max_concept] =  cvec_arr[b:e][max_candidate]\n",
    "        \n",
    "        candslist[max_concept] = [candslist[max_concept][max_candidate]]\n",
    "                                  \n",
    "        #cframelist[max_index] =  [cframelist[max_index][Dlist[max_index][0][0]]]\n",
    "        #break\n",
    "            #print index,\"\\n\"\n",
    "    res = [c[0][0] for c in candslist]\n",
    "    titles = ids2title(res)\n",
    "\n",
    "    return res, titles        \n",
    "    \n",
    "    \n",
    "######\n",
    "def key_criteria(x):\n",
    "    if len(x[1])==1 or x[1][1][1]==0:\n",
    "        return float(\"inf\")\n",
    "    \n",
    "    return (x[1][0][1]-x[1][1][1]) / x[1][1][1]\n",
    "\n",
    "def find_key_concept(candslist, cveclist_bdrs, cvec_arr, ver):\n",
    "    \n",
    "    aggr_cveclist = np.zeros(shape=(len(candslist),cvec_arr.shape[1]))\n",
    "    for i in range(len(cveclist_bdrs)):\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        aggr_cveclist[i]=cvec_arr[b:e].sum(axis=0)\n",
    "    \n",
    "    from itertools import izip\n",
    "    resolved = 0\n",
    "    Dlist=[]        \n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "        convec=aggr_cveclist[:i].sum(axis=0) + aggr_cveclist[i+1:].sum(axis=0)\n",
    "        D=[]    \n",
    "        for v in cvec:\n",
    "            try:\n",
    "                d = 1-sp.spatial.distance.cosine(convec, v);\n",
    "            except:\n",
    "                d=0                \n",
    "            if np.isnan(d):\n",
    "                d=0\n",
    "            D.append(d)\n",
    "        D=sorted(enumerate(D), key=lambda x: -x[1])\n",
    "        Dlist.append(D)\n",
    "\n",
    "    if ver ==1: \n",
    "        max_concept, _ = max(enumerate(Dlist), key=lambda x: x[1][0][1] if len(x[1])>1 else -1)\n",
    "    elif ver ==2: \n",
    "        max_concept, _ = max(enumerate(Dlist), key=lambda x: (x[1][0][1]-x[1][1][1]) if len(x[1])>1 else -1)\n",
    "    elif ver ==3: \n",
    "        max_concept, _ = max(enumerate(Dlist), key=lambda x: (x[1][0][1]-x[1][1][1])/(x[1][0][1]+x[1][1][1]) if len(x[1])>1 else -1)\n",
    "    elif ver ==4: \n",
    "        max_concept, _ = max(enumerate(Dlist), key=key_criteria)\n",
    "    max_candidate = Dlist[max_concept][0][0]\n",
    "    return max_concept, max_candidate\n",
    "\n",
    "\n",
    "def contextdisamb_4(candslist, direction=DIR_OUT, method='rvspagerank', ver=1):\n",
    "    cframelist=[]\n",
    "    cveclist_bdrs = []\n",
    "    ambig_count=0\n",
    "    for cands in candslist:\n",
    "        if len(candslist)>1:\n",
    "            ambig_count += 1\n",
    "        cands_rep = [conceptrep(c[0], method=method, direction=direction, get_titles=False) for c in cands]\n",
    "        cveclist_bdrs += [(len(cframelist), len(cframelist)+len(cands_rep))]\n",
    "        cframelist += cands_rep\n",
    "\n",
    "    #print \"ambig_count:\", ambig_count\n",
    "        \n",
    "    cvec_fr = pd.concat(cframelist, join='outer', axis=1)\n",
    "    cvec_fr.fillna(0, inplace=True)\n",
    "    cvec_arr = cvec_fr.as_matrix().T\n",
    "    i=0\n",
    "    for cframe in cframelist:\n",
    "        if cframe.empty:\n",
    "            cvec_arr = np.insert(cvec_arr,i,0, axis=0)\n",
    "        i+=1    \n",
    "    \n",
    "        \n",
    "    # find maximum ... \n",
    "        \n",
    "    max_concept, max_candidate = find_key_concept(candslist, cveclist_bdrs, cvec_arr, ver)\n",
    "    \n",
    "    b,e = cveclist_bdrs[max_concept]\n",
    "    \n",
    "    convec =  cvec_arr[b:e][max_candidate]\n",
    "        \n",
    "    \n",
    "    # Iterate \n",
    "    res=[]\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cvec = cvec_arr[b:e]\n",
    "\n",
    "        maxd=-1\n",
    "        index = -1\n",
    "        mi=0\n",
    "\n",
    "        for v in cvec:\n",
    "\n",
    "            try:\n",
    "                d = 1-sp.spatial.distance.cosine(convec, v);\n",
    "            except:\n",
    "                d=0                \n",
    "            if np.isnan(d):\n",
    "                d=0\n",
    "            \n",
    "            if d>maxd:\n",
    "                maxd=d\n",
    "                index=mi\n",
    "            mi +=1\n",
    "        if index==-1:\n",
    "            index=0\n",
    "        #print i, index, maxd    \n",
    "        res.append(cands[index][0]) \n",
    "        b,e = cveclist_bdrs[i]\n",
    "        cveclist_bdrs[i] = (b+index,b+index+1)\n",
    "        \n",
    "        #aggr_cveclist[i] =  cvec_arr[b:e][index]\n",
    "        \n",
    "        candslist[i] = candslist[i][index][0]\n",
    "        \n",
    "        \n",
    "\n",
    "    titles = ids2title(res)\n",
    "\n",
    "    return res, titles        \n",
    "\n",
    "######\n",
    "def get_sim_matrix(candslist,method, direction):\n",
    "    concepts=  list(chain(*candslist))\n",
    "    concepts=  list(set(c[0] for c in concepts))\n",
    "    sims = pd.DataFrame(index=concepts, columns=concepts)\n",
    "    for cands1,cands2 in combinations(candslist,2):\n",
    "        for c1,c2 in product(cands1,cands2):\n",
    "            sims[c1[0]][c2[0]]= sims[c2[0]][c1[0]] = getsim(c1[0],c2[0] , method, direction)\n",
    "    return sims        \n",
    "\n",
    "def tagme_vote(c, a, candslist, simmatrix, pop):\n",
    "    v = 0\n",
    "    for b in  range(len(candslist)):\n",
    "        if a==b:\n",
    "            continue\n",
    "        cands = candslist[b]\n",
    "        if pop:\n",
    "            vb = [ci[1]*simmatrix[c[0]][ci[0]] for ci in cands]\n",
    "        else:\n",
    "            vb = [simmatrix[c[0]][ci[0]]  for ci in cands]\n",
    "        vb = sum(vb) / len(vb)\n",
    "        v += vb    \n",
    "    return v\n",
    "\n",
    "def tagme(candslist, method, direction, pop=False):\n",
    "    res=[]\n",
    "    simmatrix = get_sim_matrix(candslist, method, direction)\n",
    "    for i in range(len(candslist)):\n",
    "        cands = candslist[i]\n",
    "        \n",
    "        maxd=-1\n",
    "        mi=0\n",
    "        #print len(cands)\n",
    "        for c in cands:\n",
    "            d = tagme_vote(c, i, candslist , simmatrix, pop);\n",
    "            if d>maxd:\n",
    "                maxd=d\n",
    "                index=mi\n",
    "            mi +=1\n",
    "        res.append(cands[index][0]) \n",
    "        #print index,\"\\n\"\n",
    "    titles = ids2title(res)\n",
    "    return res, titles\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wsd_eval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wsd_eval.py \n",
    "import sys\n",
    "from optparse import OptionParser\n",
    "\n",
    "from wsd import *\n",
    "\n",
    "np.seterr(all='raise')\n",
    "\n",
    "parser = OptionParser()\n",
    "parser.add_option(\"-t\", \"--max_t\", action=\"store\", type=\"int\", dest=\"max_t\", default=5)\n",
    "parser.add_option(\"-c\", \"--max_count\", action=\"store\", type=\"int\", dest=\"max_count\", default=-1)\n",
    "parser.add_option(\"-w\", \"--win_size\", action=\"store\", type=\"int\", dest=\"win_size\", default=5)\n",
    "parser.add_option(\"-v\", action=\"store_true\", dest=\"verbose\", default=False)\n",
    "\n",
    "(options, args) = parser.parse_args()\n",
    "\n",
    "#word2vec_path = os.path.join(home, 'backup/wikipedia/WikipediaClean5Negative300Skip10.Ehsan/WikipediaClean5Negative300Skip10')\n",
    "word2vec_path = os.path.join(home, '/users/grad/sajadi/backup/wikipedia/20160305/embed/word2vec.enwiki-20160305-replace_surface.1.0.500.10.5.28.5.5/word2vec.enwiki-20160305-replace_surface.1.0.500.10.5.28.5.5')\n",
    "\n",
    "\n",
    "dsnames = [os.path.join(home,'backup/datasets/ner/kore.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/wiki-mentions.5000.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/aida.json'), \n",
    "          os.path.join(home,'backup/datasets/ner/msnbc.json'),\n",
    "          os.path.join(home,'backup/datasets/ner/aquaint.json') \n",
    "          ]\n",
    "\n",
    "methods = (('ams', DIR_BOTH,'ilp'), ('wlm', DIR_IN,'ilp'),('rvspagerank', DIR_BOTH, 'ilp'),\n",
    "           ('wlm', DIR_IN, 'tagme'), ('rvspagerank', DIR_BOTH, 'tagme'),\n",
    "           ('rvspagerank', DIR_BOTH, 'context4_4'), \n",
    "          )\n",
    "\n",
    "methods = (('word2vec.500', None,'context4_4'),)\n",
    "#methods = (('rvspagerank', DIR_BOTH,'context4_4'),)\n",
    "\n",
    "max_t = options.max_t\n",
    "max_count = options.max_count\n",
    "verbose = options.verbose\n",
    "ws = options.win_size\n",
    "\n",
    "outdir = os.path.join(baseresdir, 'wsd')\n",
    "# if not os.path.exists(outdir): #Causes synchronization problem\n",
    "#     os.makedirs(outdir)\n",
    "\n",
    "tmpdir = os.path.join(outdir, 'tmp')\n",
    "# if not os.path.exists(tmpdir): #Causes synchronization problem\n",
    "#     os.makedirs(tmpdir)\n",
    "    \n",
    "resname =  os.path.join(outdir, 'reslog.csv')\n",
    "#clearlog(resname)\n",
    "\n",
    "detailedresname=  os.path.join(outdir, 'detailedreslog.txt')\n",
    "#clearlog(detailedresname)\n",
    "\n",
    "\n",
    "\n",
    "for method, direction, op_method in methods:\n",
    "    if 'word2vec' in method:\n",
    "        gensim_loadmodel(word2vec_path)\n",
    "        print \"loaded\"\n",
    "        sys.stdout.flush()\n",
    "    for dsname in dsnames:\n",
    "        start = time.time()\n",
    "        \n",
    "        print \"dsname: %s, method: %s, op_method: %s, direction: %s, max_t: %s, ws: %s ...\"  % (dsname,\n",
    "                method, op_method, direction, max_t, ws)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        tmpfilename = os.path.join(tmpdir, \n",
    "                                   '-'.join([method, str(direction), op_method, str(max_t), str(ws), os.path.basename(dsname)]))\n",
    "        overall=[]\n",
    "        start_count=-1\n",
    "        if os.path.isfile(tmpfilename):\n",
    "            with open(tmpfilename,'r') as tmpf:\n",
    "                for line in tmpf:\n",
    "                    js = json.loads(line.strip())\n",
    "                    start_count = js['no']\n",
    "                    if js['tp'] is not None:\n",
    "                        overall.append(js['tp'])\n",
    "        \n",
    "        if start_count !=-1:\n",
    "            print \"Continuing from\\t\", start_count\n",
    "            \n",
    "        count=0\n",
    "        with open(dsname,'r') as ds, open(tmpfilename,'a') as tmpf:\n",
    "            for line in ds:\n",
    "                js = json.loads(line.decode('utf-8').strip());\n",
    "                S = js[\"text\"]\n",
    "                M = js[\"mentions\"]\n",
    "                count +=1\n",
    "                if count <= start_count:\n",
    "                    continue\n",
    "                if verbose:\n",
    "                    print \"%s:\\tS=%s\\n\\tM=%s\" % (count, json.dumps(S, ensure_ascii=False).encode('utf-8'),json.dumps(M, ensure_ascii=False).encode('utf-8'))\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                C = generate_candidates(S, M, max_t=max_t, enforce=True)\n",
    "                \n",
    "                try:\n",
    "                    ids, titles = disambiguate_driver(C, ws, method, direction, op_method)\n",
    "                    tp = get_tp(M, ids) \n",
    "                except Exception as ex:\n",
    "                    tp = (None, None)\n",
    "                    print \"[Error]:\\t\", type(ex), ex\n",
    "                    #raise\n",
    "                    continue\n",
    "                \n",
    "                overall.append(tp)\n",
    "                tmpf.write(json.dumps({\"no\":count, \"tp\":tp})+\"\\n\")\n",
    "                if (max_count !=-1) and (count >= max_count):\n",
    "                    break\n",
    "                    \n",
    "\n",
    "        elapsed = str(timeformat(int(time.time()-start)));\n",
    "        print \"done\"\n",
    "        detailedres ={\"dsname\":dsname, \"method\": method, \"op_method\": op_method, \"driection\": direction,\n",
    "                      \"max_t\": max_t, \"tp\":overall, \"elapsed\": elapsed, \"ws\": ws}\n",
    "        \n",
    "        \n",
    "        logres(detailedresname, '%s',  json.dumps(detailedres))\n",
    "        \n",
    "        micro_prec, macro_prec = get_prec(overall)        \n",
    "        logres(resname, '%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s', method, op_method, graphtype(direction), max_t , ws, \n",
    "               dsname, micro_prec, macro_prec, elapsed)\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/grad/sajadi/backup/anaconda2/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from optparse import OptionParser\n",
    "\n",
    "from wsd import *\n",
    "\n",
    "word2vec_path = os.path.join(home, '/users/grad/sajadi/backup/wikipedia/20160305/embed/word2vec.enwiki-20160305-replace_surface.1.0.400.5.5.20.5.5/word2vec.enwiki-20160305-replace_surface.1.0.400.5.5.20.5.5')\n",
    "\n",
    "mmodel = gensim_loadmodel(word2vec_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel.window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mmodel.min_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
